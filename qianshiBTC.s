	.text
	.file	"qianshiBTC.c"
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function secp256k1_context_create
.LCPI0_0:
	.quad	1528782576908142                # 0x56e6b6e6f776e
	.quad	571204991136263                 # 0x2078206973207
.LCPI0_1:
	.quad	4347608368179571                # 0xf722074686973
	.quad	1713889907705446                # 0x616c617220666
	.text
	.globl	secp256k1_context_create
	.p2align	4, 0x90
	.type	secp256k1_context_create,@function
secp256k1_context_create:               # @secp256k1_context_create
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$221720, %rsp                   # imm = 0x36218
	.cfi_def_cfa_offset 221776
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	%edi, %ebp
	movl	$176, %edi
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB0_23
# %bb.1:                                # %checked_malloc.exit
	movq	%rax, %rbx
	xorps	%xmm0, %xmm0
	movups	%xmm0, (%rax)
	testb	$2, %bpl
	je	.LBB0_12
# %bb.2:
	movl	%ebp, 36(%rsp)                  # 4-byte Spill
	movl	$65536, %edi                    # imm = 0x10000
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB0_23
# %bb.3:                                # %checked_malloc.exit.i
	movq	%rax, %r12
	movq	%rbx, 16(%rsp)                  # 8-byte Spill
	addq	$8, %rbx
	movabsq	$4503599627370495, %r13         # imm = 0xFFFFFFFFFFFFF
	movabsq	$281474976710655, %rbp          # imm = 0xFFFFFFFFFFFF
	movq	%rbx, 40(%rsp)                  # 8-byte Spill
	movq	%rax, (%rbx)
	movq	secp256k1_ge_const_g+32(%rip), %rax
	movq	%rax, 352(%rsp)
	movups	secp256k1_ge_const_g+16(%rip), %xmm0
	movaps	%xmm0, 336(%rsp)
	movups	secp256k1_ge_const_g(%rip), %xmm0
	movaps	%xmm0, 320(%rsp)
	leaq	secp256k1_ge_const_g(%rip), %rbx
	movups	secp256k1_ge_const_g+40(%rip), %xmm0
	movups	%xmm0, 360(%rsp)
	movups	secp256k1_ge_const_g+56(%rip), %xmm0
	movups	%xmm0, 376(%rsp)
	movq	secp256k1_ge_const_g+72(%rip), %rax
	movq	%rax, 392(%rsp)
	movaps	.LCPI0_0(%rip), %xmm0           # xmm0 = [1528782576908142,571204991136263]
	movaps	%xmm0, 528(%rsp)
	movaps	.LCPI0_1(%rip), %xmm0           # xmm0 = [4347608368179571,1713889907705446]
	movaps	%xmm0, 544(%rsp)
	movabsq	$92807349957475, %rax           # imm = 0x546865207363
	movq	%rax, 560(%rsp)
	leaq	90640(%rsp), %rdi
	leaq	528(%rsp), %rsi
	xorl	%r14d, %r14d
	xorl	%edx, %edx
	callq	secp256k1_ge_set_xo_var
	movl	90720(%rsp), %eax
	movl	%eax, 312(%rsp)
	movups	90640(%rsp), %xmm0
	movups	90656(%rsp), %xmm1
	movaps	%xmm0, 192(%rsp)
	movaps	%xmm1, 208(%rsp)
	movq	90672(%rsp), %rax
	movq	%rax, 224(%rsp)
	movups	90680(%rsp), %xmm0
	movups	%xmm0, 232(%rsp)
	movups	90696(%rsp), %xmm0
	movups	%xmm0, 248(%rsp)
	movq	90712(%rsp), %rax
	movq	%rax, 264(%rsp)
	movq	$1, 272(%rsp)
	xorps	%xmm0, %xmm0
	movups	%xmm0, 280(%rsp)
	movups	%xmm0, 296(%rsp)
	leaq	192(%rsp), %rdi
	movq	%rdi, %rsi
	movq	%rbx, %rdx
	callq	secp256k1_gej_add_ge_var
	movaps	320(%rsp), %xmm0
	movaps	336(%rsp), %xmm1
	movaps	352(%rsp), %xmm2
	movaps	368(%rsp), %xmm3
	movaps	%xmm0, 400(%rsp)
	movaps	%xmm1, 416(%rsp)
	movaps	%xmm2, 432(%rsp)
	movaps	%xmm3, 448(%rsp)
	movaps	384(%rsp), %xmm0
	movaps	%xmm0, 464(%rsp)
	movq	$1, 480(%rsp)
	xorps	%xmm0, %xmm0
	movups	%xmm0, 488(%rsp)
	movups	%xmm0, 504(%rsp)
	movl	$0, 520(%rsp)
	movaps	192(%rsp), %xmm0
	movaps	208(%rsp), %xmm1
	movaps	224(%rsp), %xmm2
	movaps	240(%rsp), %xmm3
	movaps	%xmm0, 64(%rsp)
	movaps	%xmm1, 80(%rsp)
	movaps	%xmm2, 96(%rsp)
	movaps	%xmm3, 112(%rsp)
	movaps	256(%rsp), %xmm0
	movaps	%xmm0, 128(%rsp)
	movaps	272(%rsp), %xmm0
	movaps	%xmm0, 144(%rsp)
	movaps	288(%rsp), %xmm0
	movaps	%xmm0, 160(%rsp)
	movaps	304(%rsp), %xmm0
	movaps	%xmm0, 176(%rsp)
	jmp	.LBB0_4
	.p2align	4, 0x90
.LBB0_6:                                #   in Loop: Header=BB0_4 Depth=1
	addq	$2048, %r14                     # imm = 0x800
	cmpq	$131072, %r14                   # imm = 0x20000
	je	.LBB0_7
.LBB0_4:                                # %.preheader45.preheader.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%rsp,%r14), %rsi
	addq	$90640, %rsi                    # imm = 0x16210
	movaps	176(%rsp), %xmm0
	movups	%xmm0, 90752(%rsp,%r14)
	movaps	160(%rsp), %xmm0
	movups	%xmm0, 90736(%rsp,%r14)
	movaps	144(%rsp), %xmm0
	movups	%xmm0, 90720(%rsp,%r14)
	movaps	128(%rsp), %xmm0
	movups	%xmm0, 90704(%rsp,%r14)
	movaps	64(%rsp), %xmm0
	movaps	80(%rsp), %xmm1
	movaps	96(%rsp), %xmm2
	movaps	112(%rsp), %xmm3
	movups	%xmm3, 90688(%rsp,%r14)
	movups	%xmm2, 90672(%rsp,%r14)
	movups	%xmm1, 90656(%rsp,%r14)
	movups	%xmm0, 90640(%rsp,%r14)
	leaq	90768(%rsp,%r14), %rdi
	movq	%rdi, (%rsp)                    # 8-byte Spill
	leaq	400(%rsp), %r15
	movq	%r15, %rdx
	callq	secp256k1_gej_add_var
	leaq	(%rsp,%r14), %rbx
	addq	$90896, %rbx                    # imm = 0x16310
	movq	%rbx, %rdi
	movq	(%rsp), %rsi                    # 8-byte Reload
	movq	%r15, %rdx
	callq	secp256k1_gej_add_var
	leaq	(%rsp,%r14), %rdi
	addq	$91024, %rdi                    # imm = 0x16390
	movq	%rdi, (%rsp)                    # 8-byte Spill
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	secp256k1_gej_add_var
	leaq	(%rsp,%r14), %rdi
	addq	$91152, %rdi                    # imm = 0x16410
	movq	%rdi, 8(%rsp)                   # 8-byte Spill
	movq	(%rsp), %rsi                    # 8-byte Reload
	movq	%r15, %rdx
	callq	secp256k1_gej_add_var
	leaq	(%rsp,%r14), %rbx
	addq	$91280, %rbx                    # imm = 0x16490
	movq	%rbx, %rdi
	movq	8(%rsp), %rsi                   # 8-byte Reload
	movq	%r15, %rdx
	callq	secp256k1_gej_add_var
	leaq	(%rsp,%r14), %rdi
	addq	$91408, %rdi                    # imm = 0x16510
	movq	%rdi, (%rsp)                    # 8-byte Spill
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	secp256k1_gej_add_var
	leaq	(%rsp,%r14), %rbx
	addq	$91536, %rbx                    # imm = 0x16590
	movq	%rbx, %rdi
	movq	(%rsp), %rsi                    # 8-byte Reload
	movq	%r15, %rdx
	callq	secp256k1_gej_add_var
	leaq	(%rsp,%r14), %rdi
	addq	$91664, %rdi                    # imm = 0x16610
	movq	%rdi, (%rsp)                    # 8-byte Spill
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	secp256k1_gej_add_var
	leaq	(%rsp,%r14), %rbx
	addq	$91792, %rbx                    # imm = 0x16690
	movq	%rbx, %rdi
	movq	(%rsp), %rsi                    # 8-byte Reload
	movq	%r15, %rdx
	callq	secp256k1_gej_add_var
	leaq	(%rsp,%r14), %rdi
	addq	$91920, %rdi                    # imm = 0x16710
	movq	%rdi, (%rsp)                    # 8-byte Spill
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	secp256k1_gej_add_var
	leaq	(%rsp,%r14), %rbx
	addq	$92048, %rbx                    # imm = 0x16790
	movq	%rbx, %rdi
	movq	(%rsp), %rsi                    # 8-byte Reload
	movq	%r15, %rdx
	callq	secp256k1_gej_add_var
	leaq	(%rsp,%r14), %rdi
	addq	$92176, %rdi                    # imm = 0x16810
	movq	%rdi, (%rsp)                    # 8-byte Spill
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	secp256k1_gej_add_var
	leaq	(%rsp,%r14), %rbx
	addq	$92304, %rbx                    # imm = 0x16890
	movq	%rbx, %rdi
	movq	(%rsp), %rsi                    # 8-byte Reload
	movq	%r15, %rdx
	callq	secp256k1_gej_add_var
	leaq	(%rsp,%r14), %rdi
	addq	$92432, %rdi                    # imm = 0x16910
	movq	%rdi, (%rsp)                    # 8-byte Spill
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	secp256k1_gej_add_var
	leaq	(%rsp,%r14), %rdi
	addq	$92560, %rdi                    # imm = 0x16990
	movq	(%rsp), %rsi                    # 8-byte Reload
	movq	%r15, %rdx
	callq	secp256k1_gej_add_var
	movq	%r15, %rdi
	movq	%r15, %rsi
	callq	secp256k1_gej_double_var
	movq	%r15, %rdi
	movq	%r15, %rsi
	callq	secp256k1_gej_double_var
	movq	%r15, %rdi
	movq	%r15, %rsi
	callq	secp256k1_gej_double_var
	movq	%r15, %rdi
	movq	%r15, %rsi
	callq	secp256k1_gej_double_var
	leaq	64(%rsp), %rdi
	movq	%rdi, %rsi
	callq	secp256k1_gej_double_var
	cmpq	$126976, %r14                   # imm = 0x1F000
	jne	.LBB0_6
# %bb.5:                                #   in Loop: Header=BB0_4 Depth=1
	movq	136(%rsp), %rax
	movq	%rax, %rcx
	shrq	$48, %rcx
	andq	%rbp, %rax
	movabsq	$4294968273, %rdx               # imm = 0x1000003D1
	imulq	%rdx, %rcx
	addq	104(%rsp), %rcx
	movq	%rcx, %rbp
	shrq	$52, %rbp
	addq	112(%rsp), %rbp
	andq	%r13, %rcx
	movabsq	$18014381329608892, %rsi        # imm = 0x3FFFFBFFFFF0BC
	subq	%rcx, %rsi
	movq	%rbp, %rdx
	shrq	$52, %rdx
	addq	120(%rsp), %rdx
	andq	%r13, %rbp
	movq	%rdx, %rcx
	shrq	$52, %rcx
	addq	128(%rsp), %rcx
	andq	%r13, %rdx
	movq	%rcx, %rdi
	shrq	$52, %rdi
	addq	%rax, %rdi
	movabsq	$1125899906842620, %rax         # imm = 0x3FFFFFFFFFFFC
	subq	%rdi, %rax
	andq	%r13, %rcx
	movq	%rsi, 104(%rsp)
	movabsq	$18014398509481980, %rdi        # imm = 0x3FFFFFFFFFFFFC
	movq	%rdi, %rsi
	subq	%rbp, %rsi
	movabsq	$281474976710655, %rbp          # imm = 0xFFFFFFFFFFFF
	movq	%rsi, 112(%rsp)
	movq	%rdi, %rsi
	subq	%rdx, %rsi
	movq	%rsi, 120(%rsp)
	movq	%rdi, %rdx
	subq	%rcx, %rdx
	movq	%rdx, 128(%rsp)
	movq	%rax, 136(%rsp)
	leaq	64(%rsp), %rdi
	movq	%rdi, %rsi
	leaq	192(%rsp), %rdx
	callq	secp256k1_gej_add_var
	jmp	.LBB0_6
.LBB0_7:
	movabsq	$4503595332402222, %r14         # imm = 0xFFFFEFFFFFC2E
	leaq	528(%rsp), %rsi
	leaq	90640(%rsp), %rdx
	movl	$1024, %edi                     # imm = 0x400
	callq	secp256k1_ge_set_all_gej_var
	addq	$56, %r12
	leaq	600(%rsp), %rax
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB0_8:                                # %.preheader.i
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_9 Depth 2
	movq	%rcx, 48(%rsp)                  # 8-byte Spill
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	movq	%rax, %rcx
	xorl	%r15d, %r15d
	movq	%r12, 24(%rsp)                  # 8-byte Spill
	.p2align	4, 0x90
.LBB0_9:                                #   Parent Loop BB0_8 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%r15, 8(%rsp)                   # 8-byte Spill
	movq	%rcx, (%rsp)                    # 8-byte Spill
	movq	-40(%rcx), %rdi
	movq	%rdi, %rax
	shrq	$48, %rax
	movabsq	$4294968273, %rdx               # imm = 0x1000003D1
	imulq	%rdx, %rax
	movq	%rdx, %r11
	addq	-72(%rcx), %rax
	movq	(%rcx), %r10
	movq	%rax, %rcx
	shrq	$52, %rcx
	movq	(%rsp), %rdx                    # 8-byte Reload
	addq	-64(%rdx), %rcx
	andq	%rbp, %rdi
	movq	%rcx, %rdx
	shrq	$52, %rdx
	movq	(%rsp), %rsi                    # 8-byte Reload
	addq	-56(%rsi), %rdx
	andq	%r13, %rax
	movq	%rdx, %rsi
	shrq	$52, %rsi
	movq	(%rsp), %rbp                    # 8-byte Reload
	addq	-48(%rbp), %rsi
	andq	%r13, %rdx
	movq	%rdx, %rbx
	andq	%rcx, %rbx
	andq	%r13, %rcx
	movq	%rsi, %r8
	shrq	$52, %r8
	addq	%rdi, %r8
	andq	%rsi, %rbx
	andq	%r13, %rsi
	movq	%r8, %r9
	shrq	$48, %r9
	movq	%r8, %rbp
	movabsq	$281474976710655, %rdi          # imm = 0xFFFFFFFFFFFF
	xorq	%rdi, %rbp
	xorq	%r13, %rbx
	orq	%rbp, %rbx
	sete	%bpl
	cmpq	%r14, %rax
	seta	%bl
	andb	%bpl, %bl
	movzbl	%bl, %ebp
	orq	%r9, %rbp
	movq	%r11, %rbx
	imulq	%r11, %rbp
	addq	%rax, %rbp
	movq	%rbp, %rdi
	shrq	$52, %rdi
	addq	%rcx, %rdi
	andq	%r13, %rbp
	movq	%rdi, %rcx
	shrq	$52, %rcx
	addq	%rdx, %rcx
	movq	%rcx, %rax
	shrq	$52, %rax
	addq	%rsi, %rax
	movq	%rax, %r11
	shrq	$52, %r11
	movq	%r10, %rdx
	shrq	$48, %rdx
	imulq	%rbx, %rdx
	movq	(%rsp), %rsi                    # 8-byte Reload
	addq	-32(%rsi), %rdx
	addq	%r8, %r11
	movq	%rdx, %r8
	shrq	$52, %r8
	movq	(%rsp), %rsi                    # 8-byte Reload
	addq	-24(%rsi), %r8
	movabsq	$281474976710655, %rsi          # imm = 0xFFFFFFFFFFFF
	andq	%rsi, %r10
	movq	%r13, %r9
	movq	%r8, %r13
	shrq	$52, %r13
	movq	(%rsp), %rsi                    # 8-byte Reload
	addq	-16(%rsi), %r13
	andq	%r9, %rdx
	movq	%r13, %r15
	shrq	$52, %r15
	movq	(%rsp), %rsi                    # 8-byte Reload
	addq	-8(%rsi), %r15
	andq	%r9, %r13
	movq	%r13, %rsi
	andq	%r8, %rsi
	andq	%r9, %r8
	movq	%r15, %r12
	shrq	$52, %r12
	addq	%r10, %r12
	andq	%r15, %rsi
	andq	%r9, %r15
	movq	%r12, %r10
	shrq	$48, %r10
	movq	%r12, %r14
	movabsq	$281474976710655, %rbx          # imm = 0xFFFFFFFFFFFF
	xorq	%rbx, %r14
	xorq	%r9, %rsi
	orq	%r14, %rsi
	sete	%sil
	movabsq	$4503595332402222, %rbx         # imm = 0xFFFFEFFFFFC2E
	cmpq	%rbx, %rdx
	seta	%bl
	andb	%sil, %bl
	movzbl	%bl, %r14d
	orq	%r10, %r14
	movabsq	$4294968273, %rsi               # imm = 0x1000003D1
	imulq	%rsi, %r14
	addq	%rdx, %r14
	movq	%r14, %rsi
	shrq	$52, %rsi
	addq	%r8, %rsi
	movq	%rsi, %rdx
	shrq	$52, %rdx
	addq	%r13, %rdx
	movq	%r9, %r13
	movq	%rdx, %r10
	shrq	$52, %r10
	addq	%r15, %r10
	movq	8(%rsp), %r15                   # 8-byte Reload
	movabsq	$1099511627775, %r9             # imm = 0xFFFFFFFFFF
	movq	%r10, %r8
	shrq	$52, %r8
	addq	%r12, %r8
	movq	24(%rsp), %r12                  # 8-byte Reload
	movq	%rdi, %rbx
	shlq	$52, %rbx
	orq	%rbp, %rbx
	movabsq	$281474976710655, %rbp          # imm = 0xFFFFFFFFFFFF
	movq	%rbx, -56(%r12,%r15)
	shrq	$12, %rdi
	andq	%r9, %rdi
	movq	%rcx, %rbx
	shlq	$40, %rbx
	orq	%rdi, %rbx
	movq	%rbx, -48(%r12,%r15)
	shrq	$24, %rcx
	andl	$268435455, %ecx                # imm = 0xFFFFFFF
	movq	%rax, %rdi
	shlq	$28, %rdi
	orq	%rcx, %rdi
	movq	(%rsp), %rcx                    # 8-byte Reload
	movq	%rdi, -40(%r12,%r15)
	shrq	$36, %rax
	movzwl	%ax, %eax
	shlq	$16, %r11
	orq	%rax, %r11
	andq	%r13, %r14
	movq	%r11, -32(%r12,%r15)
	movq	%rsi, %rax
	shlq	$52, %rax
	orq	%r14, %rax
	movabsq	$4503595332402222, %r14         # imm = 0xFFFFEFFFFFC2E
	movq	%rax, -24(%r12,%r15)
	shrq	$12, %rsi
	andq	%r9, %rsi
	movq	%rdx, %rax
	shlq	$40, %rax
	orq	%rsi, %rax
	movq	%rax, -16(%r12,%r15)
	shrq	$24, %rdx
	andl	$268435455, %edx                # imm = 0xFFFFFFF
	movq	%r10, %rax
	shlq	$28, %rax
	orq	%rdx, %rax
	movq	%rax, -8(%r12,%r15)
	shrq	$36, %r10
	movzwl	%r10w, %eax
	shlq	$16, %r8
	orq	%rax, %r8
	movq	%r8, (%r12,%r15)
	addq	$64, %r15
	addq	$88, %rcx
	cmpq	$1024, %r15                     # imm = 0x400
	jne	.LBB0_9
# %bb.10:                               #   in Loop: Header=BB0_8 Depth=1
	movq	48(%rsp), %rcx                  # 8-byte Reload
	addq	$1, %rcx
	addq	$1024, %r12                     # imm = 0x400
	movq	56(%rsp), %rax                  # 8-byte Reload
	addq	$1408, %rax                     # imm = 0x580
	cmpq	$64, %rcx
	jne	.LBB0_8
# %bb.11:                               # %secp256k1_ecmult_gen_context_build.exit
	movq	40(%rsp), %rdi                  # 8-byte Reload
	xorl	%esi, %esi
	callq	secp256k1_ecmult_gen_blind
	movq	16(%rsp), %rbx                  # 8-byte Reload
	movl	36(%rsp), %ebp                  # 4-byte Reload
.LBB0_12:
	testb	$1, %bpl
	je	.LBB0_22
# %bb.13:
	cmpq	$0, (%rbx)
	jne	.LBB0_22
# %bb.14:
	movl	$0, 648(%rsp)
	movups	secp256k1_ge_const_g(%rip), %xmm0
	movaps	%xmm0, 528(%rsp)
	movups	secp256k1_ge_const_g+16(%rip), %xmm0
	movaps	%xmm0, 544(%rsp)
	movq	secp256k1_ge_const_g+32(%rip), %rax
	movq	%rax, 560(%rsp)
	movups	secp256k1_ge_const_g+40(%rip), %xmm0
	movups	%xmm0, 568(%rsp)
	movups	secp256k1_ge_const_g+56(%rip), %xmm0
	movups	%xmm0, 584(%rsp)
	movq	secp256k1_ge_const_g+72(%rip), %rax
	movq	%rax, 600(%rsp)
	movq	$1, 608(%rsp)
	xorps	%xmm0, %xmm0
	movups	%xmm0, 616(%rsp)
	movups	%xmm0, 632(%rsp)
	movl	$1048576, %edi                  # imm = 0x100000
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB0_23
# %bb.15:                               # %checked_malloc.exit.i11
	movq	%rax, %r14
	movq	%rax, (%rbx)
	movl	$2097152, %edi                  # imm = 0x200000
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB0_23
# %bb.16:                               # %checked_malloc.exit.i.i
	movq	%rax, %r12
	movl	$1441792, %edi                  # imm = 0x160000
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB0_23
# %bb.17:                               # %checked_malloc.exit2.i.i
	movq	%rax, %r13
	movq	%rbx, 16(%rsp)                  # 8-byte Spill
	movaps	640(%rsp), %xmm0
	movups	%xmm0, 112(%r12)
	movaps	624(%rsp), %xmm0
	movups	%xmm0, 96(%r12)
	movaps	608(%rsp), %xmm0
	movups	%xmm0, 80(%r12)
	movaps	592(%rsp), %xmm0
	movups	%xmm0, 64(%r12)
	movaps	528(%rsp), %xmm0
	movaps	544(%rsp), %xmm1
	movaps	560(%rsp), %xmm2
	movaps	576(%rsp), %xmm3
	movups	%xmm3, 48(%r12)
	movups	%xmm2, 32(%r12)
	movups	%xmm1, 16(%r12)
	movups	%xmm0, (%r12)
	leaq	90640(%rsp), %r15
	leaq	528(%rsp), %rsi
	movq	%r15, %rdi
	callq	secp256k1_gej_double_var
	movl	$16383, %ebp                    # imm = 0x3FFF
	movq	%r12, %rdx
	.p2align	4, 0x90
.LBB0_18:                               # =>This Inner Loop Header: Depth=1
	leaq	128(%rdx), %rbx
	movq	%rbx, %rdi
	movq	%r15, %rsi
	callq	secp256k1_gej_add_var
	movq	%rbx, %rdx
	addq	$-1, %rbp
	jne	.LBB0_18
# %bb.19:
	movl	$16384, %edi                    # imm = 0x4000
	movq	%r13, %rsi
	movq	%r12, 24(%rsp)                  # 8-byte Spill
	movq	%r12, %rdx
	callq	secp256k1_ge_set_all_gej_var
	addq	$56, %r14
	movl	$72, %r9d
	movq	%r13, (%rsp)                    # 8-byte Spill
	.p2align	4, 0x90
.LBB0_20:                               # =>This Inner Loop Header: Depth=1
	movq	%r14, 8(%rsp)                   # 8-byte Spill
	movq	-40(%r13,%r9), %rsi
	movq	%rsi, %rax
	shrq	$48, %rax
	movabsq	$4294968273, %rcx               # imm = 0x1000003D1
	imulq	%rcx, %rax
	movq	%rcx, %r11
	addq	-72(%r13,%r9), %rax
	movq	(%r13,%r9), %r8
	movq	%rax, %rdx
	shrq	$52, %rdx
	addq	-64(%r13,%r9), %rdx
	movabsq	$281474976710655, %rcx          # imm = 0xFFFFFFFFFFFF
	andq	%rcx, %rsi
	movq	%rcx, %r15
	movq	%rdx, %rbp
	shrq	$52, %rbp
	addq	-56(%r13,%r9), %rbp
	movabsq	$4503599627370495, %rbx         # imm = 0xFFFFFFFFFFFFF
	andq	%rbx, %rax
	movq	%rbp, %r10
	shrq	$52, %r10
	addq	-48(%r13,%r9), %r10
	andq	%rbx, %rbp
	movq	%rbp, %rdi
	andq	%rdx, %rdi
	andq	%rbx, %rdx
	movq	%r10, %r14
	shrq	$52, %r14
	addq	%rsi, %r14
	andq	%r10, %rdi
	andq	%rbx, %r10
	movq	%r14, %rsi
	shrq	$48, %rsi
	movq	%r14, %rcx
	xorq	%r15, %rcx
	xorq	%rbx, %rdi
	movq	%rbx, %r12
	orq	%rcx, %rdi
	sete	%cl
	movabsq	$4503595332402222, %rdi         # imm = 0xFFFFEFFFFFC2E
	cmpq	%rdi, %rax
	seta	%bl
	andb	%cl, %bl
	movzbl	%bl, %r13d
	orq	%rsi, %r13
	movq	%r11, %rcx
	imulq	%r11, %r13
	addq	%rax, %r13
	movq	%r13, %rbx
	shrq	$52, %rbx
	addq	%rdx, %rbx
	andq	%r12, %r13
	movq	%rbx, %rsi
	shrq	$52, %rsi
	addq	%rbp, %rsi
	movq	%rsi, %r11
	shrq	$52, %r11
	addq	%r10, %r11
	movq	%r11, %r10
	shrq	$52, %r10
	movq	%r8, %rax
	shrq	$48, %rax
	imulq	%rcx, %rax
	movq	(%rsp), %rcx                    # 8-byte Reload
	addq	-32(%rcx,%r9), %rax
	addq	%r14, %r10
	movq	%rax, %rcx
	shrq	$52, %rcx
	movq	(%rsp), %rdx                    # 8-byte Reload
	addq	-24(%rdx,%r9), %rcx
	movabsq	$281474976710655, %rdx          # imm = 0xFFFFFFFFFFFF
	andq	%rdx, %r8
	movq	%rcx, %r15
	shrq	$52, %r15
	movq	(%rsp), %rdx                    # 8-byte Reload
	addq	-16(%rdx,%r9), %r15
	movq	%r12, %rdi
	andq	%r12, %rax
	movq	%r15, %r14
	shrq	$52, %r14
	movq	(%rsp), %rdx                    # 8-byte Reload
	addq	-8(%rdx,%r9), %r14
	andq	%r12, %r15
	movq	%r15, %rdx
	andq	%rcx, %rdx
	andq	%r12, %rcx
	movq	%r14, %r12
	shrq	$52, %r12
	addq	%r8, %r12
	andq	%r14, %rdx
	andq	%rdi, %r14
	movq	%r12, %r8
	shrq	$48, %r8
	movq	%r12, %rbp
	movabsq	$281474976710655, %rdi          # imm = 0xFFFFFFFFFFFF
	xorq	%rdi, %rbp
	movabsq	$4503599627370495, %rdi         # imm = 0xFFFFFFFFFFFFF
	xorq	%rdi, %rdx
	orq	%rbp, %rdx
	sete	%bpl
	movabsq	$4503595332402222, %rdx         # imm = 0xFFFFEFFFFFC2E
	cmpq	%rdx, %rax
	seta	%dl
	andb	%bpl, %dl
	movzbl	%dl, %edx
	orq	%r8, %rdx
	movabsq	$4294968273, %rdi               # imm = 0x1000003D1
	imulq	%rdi, %rdx
	addq	%rax, %rdx
	movq	%rdx, %rbp
	shrq	$52, %rbp
	addq	%rcx, %rbp
	movq	%rbp, %rax
	shrq	$52, %rax
	addq	%r15, %rax
	movq	%rax, %rcx
	shrq	$52, %rcx
	addq	%r14, %rcx
	movq	8(%rsp), %r14                   # 8-byte Reload
	movq	%rcx, %r8
	shrq	$52, %r8
	addq	%r12, %r8
	movq	%rbx, %rdi
	shlq	$52, %rdi
	orq	%r13, %rdi
	movq	(%rsp), %r13                    # 8-byte Reload
	movq	%rdi, -56(%r14)
	shrq	$12, %rbx
	movabsq	$1099511627775, %r15            # imm = 0xFFFFFFFFFF
	andq	%r15, %rbx
	movq	%rsi, %rdi
	shlq	$40, %rdi
	orq	%rbx, %rdi
	movq	%rdi, -48(%r14)
	shrq	$24, %rsi
	andl	$268435455, %esi                # imm = 0xFFFFFFF
	movq	%r11, %rdi
	shlq	$28, %rdi
	orq	%rsi, %rdi
	movq	%rdi, -40(%r14)
	shrq	$36, %r11
	movzwl	%r11w, %esi
	shlq	$16, %r10
	orq	%rsi, %r10
	movabsq	$4503599627370495, %rsi         # imm = 0xFFFFFFFFFFFFF
	andq	%rsi, %rdx
	movq	%r10, -32(%r14)
	movq	%rbp, %rsi
	shlq	$52, %rsi
	orq	%rdx, %rsi
	movq	%rsi, -24(%r14)
	shrq	$12, %rbp
	andq	%r15, %rbp
	movq	%rax, %rdx
	shlq	$40, %rdx
	orq	%rbp, %rdx
	movq	%rdx, -16(%r14)
	shrq	$24, %rax
	andl	$268435455, %eax                # imm = 0xFFFFFFF
	movq	%rcx, %rdx
	shlq	$28, %rdx
	orq	%rax, %rdx
	movq	%rdx, -8(%r14)
	shrq	$36, %rcx
	movzwl	%cx, %eax
	shlq	$16, %r8
	orq	%rax, %r8
	movq	%r8, (%r14)
	addq	$88, %r9
	addq	$64, %r14
	cmpq	$1441864, %r9                   # imm = 0x160048
	jne	.LBB0_20
# %bb.21:                               # %secp256k1_ecmult_table_precomp_ge_storage_var.exit.i
	movq	24(%rsp), %rdi                  # 8-byte Reload
	callq	free@PLT
	movq	%r13, %rdi
	callq	free@PLT
	movq	16(%rsp), %rbx                  # 8-byte Reload
.LBB0_22:
	movq	%rbx, %rax
	addq	$221720, %rsp                   # imm = 0x36218
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB0_23:
	.cfi_def_cfa_offset 221776
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.76(%rip), %rdx
	leaq	.L.str.77(%rip), %r8
	movl	$66, %ecx
	xorl	%eax, %eax
	callq	fprintf@PLT
	callq	abort@PLT
.Lfunc_end0:
	.size	secp256k1_context_create, .Lfunc_end0-secp256k1_context_create
	.cfi_endproc
                                        # -- End function
	.globl	secp256k1_context_clone         # -- Begin function secp256k1_context_clone
	.p2align	4, 0x90
	.type	secp256k1_context_clone,@function
secp256k1_context_clone:                # @secp256k1_context_clone
	.cfi_startproc
# %bb.0:
	pushq	%r15
	.cfi_def_cfa_offset 16
	pushq	%r14
	.cfi_def_cfa_offset 24
	pushq	%rbx
	.cfi_def_cfa_offset 32
	.cfi_offset %rbx, -32
	.cfi_offset %r14, -24
	.cfi_offset %r15, -16
	movq	%rdi, %r14
	movl	$176, %edi
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB1_10
# %bb.1:                                # %checked_malloc.exit
	movq	%rax, %rbx
	movq	(%r14), %r15
	testq	%r15, %r15
	je	.LBB1_2
# %bb.3:
	movl	$1048576, %edi                  # imm = 0x100000
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB1_10
# %bb.4:                                # %checked_malloc.exit.i
	movq	%rax, (%rbx)
	movl	$1048576, %edx                  # imm = 0x100000
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	memcpy@PLT
	movq	8(%r14), %r15
	testq	%r15, %r15
	je	.LBB1_6
.LBB1_7:
	movl	$65536, %edi                    # imm = 0x10000
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB1_10
# %bb.8:                                # %checked_malloc.exit.i7
	movq	%rax, 8(%rbx)
	movl	$65536, %edx                    # imm = 0x10000
	movq	%rax, %rdi
	movq	%r15, %rsi
	callq	memcpy@PLT
	movups	48(%r14), %xmm0
	movups	64(%r14), %xmm1
	movups	80(%r14), %xmm2
	movups	96(%r14), %xmm3
	movups	%xmm0, 48(%rbx)
	movups	%xmm1, 64(%rbx)
	movups	%xmm2, 80(%rbx)
	movups	%xmm3, 96(%rbx)
	movups	112(%r14), %xmm0
	movups	%xmm0, 112(%rbx)
	movups	128(%r14), %xmm0
	movups	%xmm0, 128(%rbx)
	movups	144(%r14), %xmm0
	movups	%xmm0, 144(%rbx)
	movups	160(%r14), %xmm0
	movups	%xmm0, 160(%rbx)
	movups	16(%r14), %xmm0
	movups	32(%r14), %xmm1
	movups	%xmm1, 32(%rbx)
	movups	%xmm0, 16(%rbx)
	jmp	.LBB1_9
.LBB1_2:
	movq	$0, (%rbx)
	movq	8(%r14), %r15
	testq	%r15, %r15
	jne	.LBB1_7
.LBB1_6:
	movq	$0, 8(%rbx)
.LBB1_9:                                # %secp256k1_ecmult_gen_context_clone.exit
	movq	%rbx, %rax
	popq	%rbx
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	retq
.LBB1_10:
	.cfi_def_cfa_offset 32
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.76(%rip), %rdx
	leaq	.L.str.77(%rip), %r8
	movl	$66, %ecx
	xorl	%eax, %eax
	callq	fprintf@PLT
	callq	abort@PLT
.Lfunc_end1:
	.size	secp256k1_context_clone, .Lfunc_end1-secp256k1_context_clone
	.cfi_endproc
                                        # -- End function
	.globl	secp256k1_context_destroy       # -- Begin function secp256k1_context_destroy
	.p2align	4, 0x90
	.type	secp256k1_context_destroy,@function
secp256k1_context_destroy:              # @secp256k1_context_destroy
	.cfi_startproc
# %bb.0:
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset %rbx, -16
	movq	%rdi, %rbx
	movq	(%rdi), %rdi
	callq	free@PLT
	movq	$0, (%rbx)
	movq	8(%rbx), %rdi
	callq	free@PLT
	movq	%rbx, %rdi
	popq	%rbx
	.cfi_def_cfa_offset 8
	jmp	free@PLT                        # TAILCALL
.Lfunc_end2:
	.size	secp256k1_context_destroy, .Lfunc_end2-secp256k1_context_destroy
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function secp256k1_ecdsa_verify
.LCPI3_0:
	.quad	4503599627370495                # 0xfffffffffffff
	.quad	281474976710655                 # 0xffffffffffff
	.text
	.globl	secp256k1_ecdsa_verify
	.p2align	4, 0x90
	.type	secp256k1_ecdsa_verify,@function
secp256k1_ecdsa_verify:                 # @secp256k1_ecdsa_verify
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$664, %rsp                      # imm = 0x298
	.cfi_def_cfa_offset 720
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	testq	%rdi, %rdi
	je	.LBB3_1
# %bb.3:
	movq	%rdi, %r13
	cmpq	$0, (%rdi)
	je	.LBB3_4
# %bb.5:
	testq	%rsi, %rsi
	je	.LBB3_6
# %bb.7:
	movq	%rdx, %r12
	testq	%rdx, %rdx
	je	.LBB3_8
# %bb.9:
	movq	%r8, %rbx
	testq	%r8, %r8
	je	.LBB3_10
# %bb.11:
	movl	%r9d, %ebp
	movl	%ecx, %r15d
	leaq	600(%rsp), %r14
	movq	%r14, %rdi
	xorl	%edx, %edx
	callq	secp256k1_scalar_set_b32
	leaq	312(%rsp), %rdi
	movq	%rbx, %rsi
	movl	%ebp, %edx
	callq	secp256k1_eckey_pubkey_parse
	testl	%eax, %eax
	je	.LBB3_12
# %bb.13:
	pxor	%xmm0, %xmm0
	movdqa	%xmm0, 128(%rsp)
	movdqa	%xmm0, 112(%rsp)
	movdqa	%xmm0, 416(%rsp)
	movdqa	%xmm0, 400(%rsp)
	cmpb	$48, (%r12)
	jne	.LBB3_32
# %bb.14:
	movzbl	3(%r12), %eax
	leaq	5(%rax), %rcx
	cmpl	%r15d, %ecx
	jge	.LBB3_32
# %bb.15:
	movzbl	(%r12,%rcx), %ebx
	movzbl	1(%r12), %ecx
	leal	(%rbx,%rax), %edx
	addl	$4, %edx
	cmpl	%ecx, %edx
	jne	.LBB3_32
# %bb.16:
	leal	(%rbx,%rax), %ecx
	addl	$6, %ecx
	cmpl	%r15d, %ecx
	jg	.LBB3_32
# %bb.17:
	cmpb	$2, 2(%r12)
	jne	.LBB3_32
# %bb.18:
	testb	%al, %al
	je	.LBB3_32
# %bb.19:
	cmpb	$2, 4(%rax,%r12)
	jne	.LBB3_32
# %bb.20:
	testb	%bl, %bl
	je	.LBB3_32
# %bb.21:                               # %.lr.ph.preheader.i
	movl	%eax, %ecx
	leaq	(%r12,%rcx), %rbp
	addq	$6, %rbp
	leal	-1(%rbx), %edx
	addq	%rcx, %rdx
	leaq	(%r12,%rdx), %rcx
	addq	$7, %rcx
	.p2align	4, 0x90
.LBB3_22:                               # %.lr.ph.i
                                        # =>This Inner Loop Header: Depth=1
	cmpb	$0, (%rbp)
	jne	.LBB3_25
# %bb.23:                               #   in Loop: Header=BB3_22 Depth=1
	addq	$1, %rbp
	addl	$-1, %ebx
	jne	.LBB3_22
# %bb.24:
	xorl	%ebx, %ebx
	movq	%rcx, %rbp
	jmp	.LBB3_26
.LBB3_12:
	movl	$-1, %eax
	jmp	.LBB3_47
.LBB3_25:                               # %.critedge.i
	cmpl	$32, %ebx
	ja	.LBB3_32
.LBB3_26:                               # %.lr.ph78.preheader.i
	leal	-1(%rax), %ecx
	addq	%r12, %rcx
	addq	$5, %rcx
	addq	$4, %r12
	.p2align	4, 0x90
.LBB3_27:                               # %.lr.ph78.i
                                        # =>This Inner Loop Header: Depth=1
	cmpb	$0, (%r12)
	jne	.LBB3_30
# %bb.28:                               #   in Loop: Header=BB3_27 Depth=1
	leal	-1(%rax), %edx
	addq	$1, %r12
	cmpl	$1, %eax
	movl	%edx, %eax
	jg	.LBB3_27
# %bb.29:
	xorl	%eax, %eax
	movq	%rcx, %r12
	jmp	.LBB3_31
.LBB3_30:                               # %.critedge1.i
	cmpl	$32, %eax
	ja	.LBB3_32
.LBB3_31:                               # %.critedge1.thread.i
	movl	%eax, %edx
	leaq	112(%rsp), %r15
	movq	%r15, %rdi
	subq	%rdx, %rdi
	addq	$32, %rdi
	movq	%r12, %rsi
	callq	memcpy@PLT
	movslq	%ebx, %rdx
	leaq	400(%rsp), %rdi
	subq	%rdx, %rdi
	addq	$32, %rdi
	movq	%rbp, %rsi
	callq	memcpy@PLT
	movl	$0, 240(%rsp)
	leaq	48(%rsp), %rbp
	leaq	240(%rsp), %rdx
	movq	%rbp, %rdi
	movq	%r15, %rsi
	callq	secp256k1_scalar_set_b32
	cmpl	$0, 240(%rsp)
	je	.LBB3_33
.LBB3_32:                               # %secp256k1_ecdsa_sig_parse.exit.thread
	movl	$-2, %eax
.LBB3_47:
	addq	$664, %rsp                      # imm = 0x298
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB3_33:                               # %secp256k1_ecdsa_sig_parse.exit
	.cfi_def_cfa_offset 720
	leaq	80(%rsp), %rbx
	leaq	400(%rsp), %rsi
	leaq	240(%rsp), %rdx
	movq	%rbx, %rdi
	callq	secp256k1_scalar_set_b32
	cmpl	$0, 240(%rsp)
	movl	$-2, %eax
	jne	.LBB3_47
# %bb.34:
	movdqu	48(%rsp), %xmm0
	movdqu	64(%rsp), %xmm1
	por	%xmm0, %xmm1
	pxor	%xmm0, %xmm0
	pcmpeqb	%xmm0, %xmm1
	pmovmskb	%xmm1, %eax
	cmpl	$65535, %eax                    # imm = 0xFFFF
	je	.LBB3_46
# %bb.35:
	movdqu	80(%rsp), %xmm1
	movdqu	96(%rsp), %xmm2
	por	%xmm1, %xmm2
	pcmpeqb	%xmm0, %xmm2
	pmovmskb	%xmm2, %eax
	cmpl	$65535, %eax                    # imm = 0xFFFF
	je	.LBB3_46
# %bb.36:
	leaq	568(%rsp), %r15
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	secp256k1_scalar_inverse
	leaq	240(%rsp), %rsi
	movq	%r15, %rdi
	movq	%r14, %rdx
	movq	%r13, 40(%rsp)                  # 8-byte Spill
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	536(%rsp), %rdi
	callq	secp256k1_scalar_reduce_512
	leaq	240(%rsp), %rsi
	leaq	568(%rsp), %rdi
	movq	%rbp, %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	632(%rsp), %rbx
	movq	%rbx, %rdi
	callq	secp256k1_scalar_reduce_512
	movl	392(%rsp), %eax
	movl	%eax, 232(%rsp)
	movups	312(%rsp), %xmm0
	movdqu	328(%rsp), %xmm1
	movaps	%xmm0, 112(%rsp)
	movdqa	%xmm1, 128(%rsp)
	movq	344(%rsp), %rax
	movq	%rax, 144(%rsp)
	movups	352(%rsp), %xmm0
	movups	%xmm0, 152(%rsp)
	movdqu	368(%rsp), %xmm0
	movdqu	%xmm0, 168(%rsp)
	movq	384(%rsp), %rax
	movq	%rax, 184(%rsp)
	movq	$1, 192(%rsp)
	pxor	%xmm0, %xmm0
	movdqu	%xmm0, 200(%rsp)
	movdqu	%xmm0, 216(%rsp)
	leaq	400(%rsp), %rsi
	leaq	112(%rsp), %rdx
	movq	40(%rsp), %rdi                  # 8-byte Reload
	movq	%rbx, %rcx
	leaq	536(%rsp), %r8
	callq	secp256k1_ecmult
	cmpl	$0, 520(%rsp)
	je	.LBB3_37
.LBB3_46:
	xorl	%eax, %eax
	jmp	.LBB3_47
.LBB3_37:                               # %secp256k1_fe_set_b32.exit.i
	movabsq	$4503599606050524, %rbx         # imm = 0xFFFFFFEBAAEDC
	movq	72(%rsp), %rax
	movq	64(%rsp), %rcx
	movq	48(%rsp), %rdx
	movq	56(%rsp), %rsi
	leaq	21319971(%rbx), %rdi
	andq	%rdx, %rdi
	movq	%rdi, (%rsp)
	shrq	$52, %rdx
	movq	%rsi, %rdi
	shlq	$12, %rdi
	leaq	21315876(%rbx), %rbp
	andq	%rdi, %rbp
	orq	%rdx, %rbp
	movq	%rbp, 8(%rsp)
	shrq	$40, %rsi
	movq	%rax, %rdx
	shldq	$36, %rcx, %rdx
	shlq	$24, %rcx
	leaq	4542756(%rbx), %rdi
	andq	%rcx, %rdi
	orq	%rsi, %rdi
	movq	%rdi, 16(%rsp)
	movabsq	$4503599627370495, %rcx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rcx
	movq	%rcx, 24(%rsp)
	shrq	$16, %rax
	movq	%rax, 32(%rsp)
	movq	%rsp, %rdi
	leaq	400(%rsp), %rsi
	callq	secp256k1_gej_eq_x_var
	testl	%eax, %eax
	je	.LBB3_39
# %bb.38:                               # %secp256k1_ecdsa_sig_verify.exit.thread30
	movl	$1, %eax
	jmp	.LBB3_47
.LBB3_39:
	movq	32(%rsp), %rax
	orq	24(%rsp), %rax
	jne	.LBB3_46
# %bb.40:
	movq	16(%rsp), %rax
	cmpq	$21319971, %rax                 # imm = 0x1455123
	ja	.LBB3_46
# %bb.41:
	movq	8(%rsp), %rcx
	jne	.LBB3_45
# %bb.42:
	movabsq	$445351433356290, %rdx          # imm = 0x1950B75FC4402
	cmpq	%rdx, %rcx
	ja	.LBB3_46
# %bb.43:
	jne	.LBB3_45
# %bb.44:
	movabsq	$3836686497331950, %rsi         # imm = 0xDA1722FC9BAEE
	movq	%rdx, %rcx
	cmpq	%rsi, (%rsp)
	jae	.LBB3_46
.LBB3_45:                               # %secp256k1_ecdsa_sig_verify.exit
	movabsq	$666908835070273, %rdx          # imm = 0x25E8CD0364141
	addq	%rdx, (%rsp)
	movabsq	$4058248194014205, %rdx         # imm = 0xE6AF48A03BBFD
	addq	%rcx, %rdx
	movq	%rdx, 8(%rsp)
	addq	%rbx, %rax
	movq	%rax, 16(%rsp)
	movdqa	.LCPI3_0(%rip), %xmm0           # xmm0 = [4503599627370495,281474976710655]
	movdqu	%xmm0, 24(%rsp)
	movq	%rsp, %rdi
	leaq	400(%rsp), %rsi
	callq	secp256k1_gej_eq_x_var
	movl	%eax, %ecx
	movl	$1, %eax
	testl	%ecx, %ecx
	jne	.LBB3_47
	jmp	.LBB3_46
.LBB3_1:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.2(%rip), %r8
	movl	$62, %ecx
	jmp	.LBB3_2
.LBB3_4:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.3(%rip), %r8
	movl	$63, %ecx
	jmp	.LBB3_2
.LBB3_6:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.4(%rip), %r8
	movl	$64, %ecx
	jmp	.LBB3_2
.LBB3_8:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.5(%rip), %r8
	movl	$65, %ecx
	jmp	.LBB3_2
.LBB3_10:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.6(%rip), %r8
	movl	$66, %ecx
.LBB3_2:
	xorl	%eax, %eax
	callq	fprintf@PLT
	callq	abort@PLT
.Lfunc_end3:
	.size	secp256k1_ecdsa_verify, .Lfunc_end3-secp256k1_ecdsa_verify
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function secp256k1_scalar_set_b32
	.type	secp256k1_scalar_set_b32,@function
secp256k1_scalar_set_b32:               # @secp256k1_scalar_set_b32
	.cfi_startproc
# %bb.0:
	pushq	%rbx
	.cfi_def_cfa_offset 16
	.cfi_offset %rbx, -16
	movq	24(%rsi), %r8
	bswapq	%r8
	movq	%r8, (%rdi)
	movq	16(%rsi), %r9
	bswapq	%r9
	movq	%r9, 8(%rdi)
	movq	8(%rsi), %rbx
	bswapq	%rbx
	movq	%rbx, 16(%rdi)
	movq	(%rsi), %r11
	bswapq	%r11
	cmpq	$-1, %r11
	setne	%r10b
	cmpq	$-2, %rbx
	setb	%cl
	orb	%r10b, %cl
	movzbl	%cl, %eax
	xorl	%esi, %esi
	cmpq	$-1, %rbx
	sete	%sil
	movl	%eax, %ecx
	notl	%ecx
	andl	%esi, %ecx
	movabsq	$-4994812053365940165, %r10     # imm = 0xBAAEDCE6AF48A03B
	xorl	%esi, %esi
	cmpq	%r10, %r9
	setb	%r10b
	seta	%sil
	orb	%r10b, %al
	movzbl	%al, %eax
	notl	%eax
	andl	%eax, %esi
	orl	%ecx, %esi
	movabsq	$-4624529908474429120, %r10     # imm = 0xBFD25E8CD0364140
	xorl	%ecx, %ecx
	cmpq	%r10, %r8
	seta	%cl
	andl	%eax, %ecx
	orl	%esi, %ecx
	movabsq	$4624529908474429119, %rax      # imm = 0x402DA1732FC9BEBF
	imulq	%rcx, %rax
	movabsq	$4994812053365940164, %rsi      # imm = 0x4551231950B75FC4
	imulq	%rcx, %rsi
	addq	%r8, %rax
	movq	%rax, (%rdi)
	adcq	%r9, %rsi
	movq	%rsi, 8(%rdi)
	adcq	%rcx, %rbx
	movq	%rbx, 16(%rdi)
	adcq	$0, %r11
	movq	%r11, 24(%rdi)
	testq	%rdx, %rdx
	je	.LBB4_2
# %bb.1:
	movl	%ecx, (%rdx)
.LBB4_2:
	popq	%rbx
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end4:
	.size	secp256k1_scalar_set_b32, .Lfunc_end4-secp256k1_scalar_set_b32
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function secp256k1_eckey_pubkey_parse
	.type	secp256k1_eckey_pubkey_parse,@function
secp256k1_eckey_pubkey_parse:           # @secp256k1_eckey_pubkey_parse
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$120, %rsp
	.cfi_def_cfa_offset 176
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rdi, %r12
	xorl	%eax, %eax
	cmpl	$65, %edx
	je	.LBB5_7
# %bb.1:
	cmpl	$33, %edx
	jne	.LBB5_20
# %bb.2:
	movb	(%rsi), %r9b
	movl	%r9d, %edx
	andb	$-2, %dl
	cmpb	$2, %dl
	jne	.LBB5_20
# %bb.3:
	movq	1(%rsi), %r11
	movq	9(%rsi), %rbp
	bswapq	%r11
	bswapq	%rbp
	movabsq	$4503599627370495, %r8          # imm = 0xFFFFFFFFFFFFF
	movq	17(%rsi), %rcx
	bswapq	%rcx
	movq	25(%rsi), %rdx
	bswapq	%rdx
	movq	%rdx, %r10
	andq	%r8, %r10
	movq	%r10, 16(%rsp)
	shrq	$52, %rdx
	movq	%rcx, %rax
	shlq	$12, %rax
	leaq	-4095(%r8), %rsi
	andq	%rax, %rsi
	orq	%rdx, %rsi
	movq	%rsi, 24(%rsp)
	shrq	$40, %rcx
	movq	%r11, %rax
	shldq	$36, %rbp, %rax
	shlq	$24, %rbp
	leaq	-16777215(%r8), %rbx
	andq	%rbp, %rbx
	orq	%rcx, %rbx
	movq	%rbx, 32(%rsp)
	movabsq	$68719476735, %rcx              # imm = 0xFFFFFFFFF
	movabsq	$4503530907893760, %rbp         # imm = 0xFFFF000000000
	orq	%rcx, %rbp
	andq	%rax, %rbp
	movq	%rbp, 40(%rsp)
	shrq	$16, %r11
	movq	%r11, 48(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	cmpq	%rax, %r11
	jne	.LBB5_6
# %bb.4:
	andq	%rbp, %rbx
	andq	%rsi, %rbx
	cmpq	%r8, %rbx
	jne	.LBB5_6
# %bb.5:
	xorl	%eax, %eax
	movabsq	$4503595332402222, %rcx         # imm = 0xFFFFEFFFFFC2E
	cmpq	%rcx, %r10
	ja	.LBB5_20
.LBB5_6:                                # %secp256k1_fe_set_b32.exit
	xorl	%edx, %edx
	cmpb	$3, %r9b
	sete	%dl
	leaq	16(%rsp), %rsi
	movq	%r12, %rdi
	callq	secp256k1_ge_set_xo_var
	movl	%eax, %ecx
	xorl	%eax, %eax
	testl	%ecx, %ecx
	setne	%al
	jmp	.LBB5_20
.LBB5_7:
	movb	(%rsi), %cl
	cmpb	$7, %cl
	ja	.LBB5_20
# %bb.8:
	movzbl	%cl, %ecx
	movl	$208, %edx
	btl	%ecx, %edx
	jae	.LBB5_20
# %bb.9:
	movabsq	$4503599627370495, %r10         # imm = 0xFFFFFFFFFFFFF
	movabsq	$4503530907893760, %r9          # imm = 0xFFFF000000000
	movabsq	$281474976710655, %r8           # imm = 0xFFFFFFFFFFFF
	movq	1(%rsi), %r11
	movq	9(%rsi), %r15
	bswapq	%r11
	bswapq	%r15
	movq	17(%rsi), %rcx
	bswapq	%rcx
	movq	25(%rsi), %rbp
	bswapq	%rbp
	movq	%rbp, %rbx
	andq	%r10, %rbx
	shrq	$52, %rbp
	movq	%rcx, %r14
	shlq	$12, %r14
	leaq	-4095(%r10), %rdi
	andq	%rdi, %r14
	orq	%rbp, %r14
	shrq	$40, %rcx
	movq	%r11, %rbp
	shldq	$36, %r15, %rbp
	shlq	$24, %r15
	leaq	-16777215(%r10), %rdx
	andq	%rdx, %r15
	orq	%rcx, %r15
	movabsq	$68719476735, %r13              # imm = 0xFFFFFFFFF
	orq	%r9, %r13
	andq	%rbp, %r13
	shrq	$16, %r11
	cmpq	%r8, %r11
	jne	.LBB5_12
# %bb.10:
	movq	%r15, %rcx
	andq	%r13, %rcx
	andq	%r14, %rcx
	cmpq	%r10, %rcx
	jne	.LBB5_12
# %bb.11:
	movabsq	$4503595332402222, %rcx         # imm = 0xFFFFEFFFFFC2E
	cmpq	%rcx, %rbx
	ja	.LBB5_20
.LBB5_12:                               # %secp256k1_fe_set_b32.exit27
	movq	%rbx, 56(%rsp)                  # 8-byte Spill
	movq	%r12, 64(%rsp)                  # 8-byte Spill
	movq	33(%rsi), %r8
	movq	41(%rsi), %r9
	bswapq	%r8
	bswapq	%r9
	movq	49(%rsi), %rcx
	bswapq	%rcx
	movq	%r10, %rbx
	movq	57(%rsi), %r10
	bswapq	%r10
	movq	%r10, %r12
	andq	%rbx, %r12
	movq	%r10, %rbp
	shrq	$52, %rbp
	movq	%rcx, %rbx
	shlq	$12, %rbx
	andq	%rdi, %rbx
	orq	%rbp, %rbx
	shrq	$40, %rcx
	movq	%r9, %rdi
	shlq	$24, %rdi
	andq	%rdx, %rdi
	orq	%rcx, %rdi
	shrq	$28, %r9
	movq	%r8, %rcx
	shlq	$36, %rcx
	movabsq	$4503530907893760, %rdx         # imm = 0xFFFF000000000
	andq	%rdx, %rcx
	orq	%r9, %rcx
	shrq	$16, %r8
	movabsq	$281474976710655, %rdx          # imm = 0xFFFFFFFFFFFF
	cmpq	%rdx, %r8
	jne	.LBB5_15
# %bb.13:
	movq	%rdi, %rbp
	andq	%rcx, %rbp
	andq	%rbx, %rbp
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	cmpq	%rdx, %rbp
	jne	.LBB5_15
# %bb.14:
	movabsq	$4503595332402222, %rbp         # imm = 0xFFFFEFFFFFC2E
	cmpq	%rbp, %r12
	ja	.LBB5_20
.LBB5_15:                               # %secp256k1_fe_set_b32.exit30
	movq	64(%rsp), %rbp                  # 8-byte Reload
	movl	$0, 80(%rbp)
	movq	56(%rsp), %rdx                  # 8-byte Reload
	movq	%rdx, (%rbp)
	movq	%r14, 8(%rbp)
	movq	%r15, 16(%rbp)
	movq	%r13, 24(%rbp)
	movq	%r11, 32(%rbp)
	movq	%r12, 40(%rbp)
	movq	%rbx, 48(%rbp)
	movq	%rdi, 56(%rbp)
	movq	%rcx, 64(%rbp)
	movq	%r8, 72(%rbp)
	movb	(%rsi), %cl
	movl	%ecx, %edx
	andb	$-2, %dl
	cmpb	$6, %dl
	jne	.LBB5_17
# %bb.16:
	cmpb	$7, %cl
	setne	%cl
	andb	$1, %r10b
	cmpb	%cl, %r10b
	je	.LBB5_20
.LBB5_17:
	leaq	40(%rbp), %rsi
	leaq	16(%rsp), %rdi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 80(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, (%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 8(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rsp), %rsi
	movq	80(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	80(%rsp), %rdi
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 8(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 72(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, (%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	72(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	(%rsp), %rsi
	movq	8(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 8(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 72(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, (%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	72(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	(%rsp), %rsi
	movq	8(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	80(%rsp), %rax
	movq	112(%rsp), %rbp
	movq	%rbp, %rcx
	shrq	$48, %rcx
	movabsq	$281474976710655, %rdx          # imm = 0xFFFFFFFFFFFF
	andq	%rdx, %rbp
	movabsq	$4294968273, %r9                # imm = 0x1000003D1
	imulq	%r9, %rcx
	addq	%rcx, %rax
	addq	$7, %rax
	movq	%rax, %rsi
	shrq	$52, %rsi
	addq	88(%rsp), %rsi
	movabsq	$4503599627370495, %r10         # imm = 0xFFFFFFFFFFFFF
	andq	%r10, %rax
	movq	%rsi, %rdi
	shrq	$52, %rdi
	addq	96(%rsp), %rdi
	movq	%rdi, %rdx
	shrq	$52, %rdx
	addq	104(%rsp), %rdx
	movq	%rdx, %rcx
	shrq	$52, %rcx
	subq	48(%rsp), %rbp
	addq	%rcx, %rbp
	movabsq	$1125899906842620, %r8          # imm = 0x3FFFFFFFFFFFC
	addq	%rbp, %r8
	movq	%r8, %rcx
	shrq	$48, %rcx
	subq	16(%rsp), %rax
	imulq	%r9, %rcx
	addq	%rcx, %rax
	movabsq	$18014381329608892, %rbp        # imm = 0x3FFFFBFFFFF0BC
	addq	%rax, %rbp
	movq	%rbp, %rbx
	andq	%r10, %rbx
	addq	$-1, %r9
	xorq	%rbx, %r9
	testq	%rbx, %rbx
	je	.LBB5_19
# %bb.18:
	xorl	%eax, %eax
	cmpq	%r10, %r9
	jne	.LBB5_20
.LBB5_19:
	andq	%r10, %rdx
	andq	%r10, %rdi
	andq	%r10, %rsi
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %r8
	shrq	$52, %rbp
	addq	%rsi, %rbp
	movabsq	$18014398509481980, %rax        # imm = 0x3FFFFFFFFFFFFC
	addq	%rax, %rbp
	subq	24(%rsp), %rbp
	movq	%rbp, %rcx
	shrq	$52, %rcx
	addq	%rax, %rdi
	subq	32(%rsp), %rdi
	addq	%rcx, %rdi
	movq	%rbp, %rcx
	andq	%r10, %rcx
	orq	%rbx, %rcx
	andq	%r9, %rbp
	movq	%rdi, %rsi
	shrq	$52, %rsi
	addq	%rax, %rdx
	subq	40(%rsp), %rdx
	addq	%rsi, %rdx
	andq	%rdi, %rbp
	andq	%r10, %rdi
	orq	%rcx, %rdi
	movq	%rdx, %rax
	shrq	$52, %rax
	addq	%r8, %rax
	andq	%rdx, %rbp
	andq	%r10, %rdx
	orq	%rdi, %rdx
	movabsq	$4222124650659840, %rcx         # imm = 0xF000000000000
	xorq	%rax, %rcx
	andq	%rbp, %rcx
	orq	%rax, %rdx
	sete	%al
	cmpq	%r10, %rcx
	sete	%cl
	orb	%al, %cl
	movzbl	%cl, %eax
.LBB5_20:                               # %secp256k1_fe_set_b32.exit27.thread
	addq	$120, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end5:
	.size	secp256k1_eckey_pubkey_parse, .Lfunc_end5-secp256k1_eckey_pubkey_parse
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function nonce_function_rfc6979
	.type	nonce_function_rfc6979,@function
nonce_function_rfc6979:                 # @nonce_function_rfc6979
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	subq	$72, %rsp
	.cfi_def_cfa_offset 112
	.cfi_offset %rbx, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r8, %rax
	movl	%ecx, %r15d
	movq	%rsi, %rcx
	movq	%rdi, %rbx
	xorl	%r8d, %r8d
	testq	%rax, %rax
	setne	%r8b
	shlq	$5, %r8
	movq	%rsp, %r14
	movq	%r14, %rdi
	movq	%rdx, %rsi
	movq	%rcx, %rdx
	movq	%rax, %rcx
	callq	secp256k1_rfc6979_hmac_sha256_initialize
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB6_1:                                # =>This Inner Loop Header: Depth=1
	movq	%r14, %rdi
	movq	%rbx, %rsi
	callq	secp256k1_rfc6979_hmac_sha256_generate
	addl	$1, %ebp
	cmpl	%r15d, %ebp
	jbe	.LBB6_1
# %bb.2:
	movl	$1, %eax
	addq	$72, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end6:
	.size	nonce_function_rfc6979, .Lfunc_end6-nonce_function_rfc6979
	.cfi_endproc
                                        # -- End function
	.globl	secp256k1_ecdsa_sign            # -- Begin function secp256k1_ecdsa_sign
	.p2align	4, 0x90
	.type	secp256k1_ecdsa_sign,@function
secp256k1_ecdsa_sign:                   # @secp256k1_ecdsa_sign
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$456, %rsp                      # imm = 0x1C8
	.cfi_def_cfa_offset 512
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	$0, 60(%rsp)
	testq	%rdi, %rdi
	je	.LBB7_1
# %bb.3:
	movq	%rdi, %rbp
	cmpq	$0, 8(%rdi)
	je	.LBB7_4
# %bb.5:
	movq	%rsi, %r12
	testq	%rsi, %rsi
	je	.LBB7_6
# %bb.7:
	movq	%rdx, %r13
	testq	%rdx, %rdx
	je	.LBB7_8
# %bb.9:
	movq	%rcx, %r15
	testq	%rcx, %rcx
	je	.LBB7_10
# %bb.11:
	movq	%r8, %rbx
	testq	%r8, %r8
	je	.LBB7_12
# %bb.13:
	testq	%r9, %r9
	leaq	nonce_function_rfc6979(%rip), %r14
	cmovneq	%r9, %r14
	leaq	384(%rsp), %rdi
	leaq	60(%rsp), %rdx
	movq	%rbx, %rsi
	callq	secp256k1_scalar_set_b32
	cmpl	$0, 60(%rsp)
	je	.LBB7_14
.LBB7_172:                              # %.thread51
	movl	$0, (%r15)
	xorl	%eax, %eax
.LBB7_173:
	addq	$456, %rsp                      # imm = 0x1C8
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB7_14:
	.cfi_def_cfa_offset 512
	movdqa	384(%rsp), %xmm0
	por	400(%rsp), %xmm0
	pxor	%xmm1, %xmm1
	pcmpeqb	%xmm0, %xmm1
	pmovmskb	%xmm1, %eax
	cmpl	$65535, %eax                    # imm = 0xFFFF
	je	.LBB7_172
# %bb.15:
	movq	%r13, 280(%rsp)                 # 8-byte Spill
	movq	%r15, 272(%rsp)                 # 8-byte Spill
	movq	512(%rsp), %r15
	leaq	424(%rsp), %rdi
	movq	%r12, %rsi
	xorl	%edx, %edx
	callq	secp256k1_scalar_set_b32
	leaq	16(%rsp), %rdi
	movq	%r12, %rsi
	movq	%rbx, %rdx
	xorl	%ecx, %ecx
	movq	%r15, %r8
	callq	*%r14
	testl	%eax, %eax
	je	.LBB7_21
# %bb.16:                               # %.lr.ph.preheader
	addq	$8, %rbp
	movl	$1, %r15d
	leaq	16(%rsp), %r13
	movq	%r13, 8(%rsp)                   # 8-byte Spill
	jmp	.LBB7_17
	.p2align	4, 0x90
.LBB7_20:                               #   in Loop: Header=BB7_17 Depth=1
	leaq	16(%rsp), %r13
	movq	%r13, %rdi
	movq	%r12, %rsi
	movq	%rbx, %rdx
	movl	%r15d, %ecx
	movq	512(%rsp), %r8
	callq	*%r14
	addl	$1, %r15d
	testl	%eax, %eax
	je	.LBB7_21
.LBB7_17:                               # %.lr.ph
                                        # =>This Inner Loop Header: Depth=1
	leaq	352(%rsp), %rdi
	movq	%r13, %rsi
	leaq	60(%rsp), %rdx
	callq	secp256k1_scalar_set_b32
	pxor	%xmm1, %xmm1
	movdqa	%xmm1, 32(%rsp)
	movdqa	352(%rsp), %xmm0
	por	368(%rsp), %xmm0
	movdqa	%xmm1, 16(%rsp)
	pcmpeqb	%xmm1, %xmm0
	pmovmskb	%xmm0, %eax
	cmpl	$65535, %eax                    # imm = 0xFFFF
	je	.LBB7_20
# %bb.18:                               # %.lr.ph
                                        #   in Loop: Header=BB7_17 Depth=1
	cmpl	$0, 60(%rsp)
	jne	.LBB7_20
# %bb.19:                               #   in Loop: Header=BB7_17 Depth=1
	movq	%rbp, %rdi
	leaq	288(%rsp), %rsi
	leaq	384(%rsp), %rdx
	leaq	424(%rsp), %rcx
	leaq	352(%rsp), %r8
	xorl	%r9d, %r9d
	callq	secp256k1_ecdsa_sig_sign
	testl	%eax, %eax
	je	.LBB7_20
# %bb.22:
	pxor	%xmm0, %xmm0
	movdqa	%xmm0, 16(%rsp)
	movdqa	%xmm0, 64(%rsp)
	movq	312(%rsp), %rbp
	movq	%rbp, %rax
	bswapq	%rax
	movq	%rax, 17(%rsp)
	movq	304(%rsp), %r9
	movq	%r9, %rax
	bswapq	%rax
	movq	%rax, 25(%rsp)
	movq	296(%rsp), %rax
	movq	%rax, %rcx
	bswapq	%rcx
	movq	288(%rsp), %r15
	movq	%rcx, 33(%rsp)
	movq	%r15, %rcx
	bswapq	%rcx
	movq	%rcx, 41(%rsp)
	movq	344(%rsp), %rdx
	movq	%rdx, %rcx
	bswapq	%rcx
	movq	%rcx, 65(%rsp)
	movq	336(%rsp), %rcx
	movq	%rcx, %rsi
	movq	%rcx, 256(%rsp)                 # 8-byte Spill
	bswapq	%rcx
	movq	%rcx, 73(%rsp)
	movq	328(%rsp), %rcx
	movq	%rcx, %rsi
	movq	%rcx, 248(%rsp)                 # 8-byte Spill
	bswapq	%rcx
	movq	%rcx, 81(%rsp)
	movq	320(%rsp), %rcx
	movq	%rcx, %rsi
	movq	%rcx, 240(%rsp)                 # 8-byte Spill
	bswapq	%rcx
	movq	%rcx, 89(%rsp)
	cmpb	$0, 16(%rsp)
	movl	$33, %ecx
	movq	%rcx, (%rsp)                    # 8-byte Spill
	movl	$33, %esi
	jne	.LBB7_106
# %bb.23:
	testq	%rbp, %rbp
	js	.LBB7_106
# %bb.24:
	leaq	17(%rsp), %rcx
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	movq	%rbp, %rcx
	shrq	$56, %rcx
	movl	$32, %esi
	testb	%cl, %cl
	jne	.LBB7_106
# %bb.25:
	movq	%rbp, %r11
	shrq	$48, %r11
	movq	%rbp, %r13
	shrq	$40, %r13
	movq	%rbp, %r10
	shrq	$32, %r10
	movq	%rbp, %r8
	shrq	$24, %r8
	movq	%rbp, %rdi
	shrq	$16, %rdi
	movq	%rbp, %r12
	shrq	$8, %r12
	movq	%r9, %rcx
	shrq	$56, %rcx
	movq	%r9, %r14
	shrq	$48, %r14
	movq	%r9, %rbx
	shrq	$40, %rbx
	movq	%rbx, 264(%rsp)                 # 8-byte Spill
	movq	%r9, %rbx
	shrq	$32, %rbx
	movq	%rbx, 232(%rsp)                 # 8-byte Spill
	movq	%r9, %rbx
	shrq	$24, %rbx
	movq	%rbx, 224(%rsp)                 # 8-byte Spill
	movq	%r9, %rbx
	shrq	$16, %rbx
	movq	%rbx, 216(%rsp)                 # 8-byte Spill
	movq	%r9, %rbx
	shrq	$8, %rbx
	movq	%rbx, 208(%rsp)                 # 8-byte Spill
	movq	%rax, %rbx
	shrq	$56, %rbx
	movq	%rbx, 200(%rsp)                 # 8-byte Spill
	movq	%rax, %rbx
	shrq	$48, %rbx
	movq	%rbx, 192(%rsp)                 # 8-byte Spill
	movq	%rax, %rbx
	shrq	$40, %rbx
	movq	%rbx, 184(%rsp)                 # 8-byte Spill
	movq	%rax, %rbx
	shrq	$32, %rbx
	movq	%rbx, 176(%rsp)                 # 8-byte Spill
	movq	%rax, %rbx
	shrq	$24, %rbx
	movq	%rbx, 168(%rsp)                 # 8-byte Spill
	movq	%rax, %rbx
	shrq	$16, %rbx
	movq	%rbx, 160(%rsp)                 # 8-byte Spill
	movq	%rax, %rbx
	shrq	$8, %rbx
	movq	%rbx, 152(%rsp)                 # 8-byte Spill
	movq	%r15, %rbx
	shrq	$56, %rbx
	movq	%rbx, 136(%rsp)                 # 8-byte Spill
	movq	%r15, %rbx
	shrq	$48, %rbx
	movq	%rbx, 144(%rsp)                 # 8-byte Spill
	movq	%r15, %rbx
	shrq	$40, %rbx
	movq	%rbx, 104(%rsp)                 # 8-byte Spill
	movq	%r15, %rbx
	shrq	$32, %rbx
	movq	%rbx, 112(%rsp)                 # 8-byte Spill
	movq	%r15, %rbx
	shrq	$24, %rbx
	movq	%rbx, 120(%rsp)                 # 8-byte Spill
	movq	%r15, %rbx
	shrq	$16, %rbx
	movq	%rbx, 128(%rsp)                 # 8-byte Spill
	movq	%r15, %rbx
	shrq	$8, %rbx
	testb	%r11b, %r11b
	js	.LBB7_106
# %bb.26:
	leaq	18(%rsp), %rsi
	movq	%rsi, 8(%rsp)                   # 8-byte Spill
	movl	$31, %esi
	jne	.LBB7_106
# %bb.27:
	testb	%r13b, %r13b
	js	.LBB7_106
# %bb.28:
	leaq	19(%rsp), %rsi
	movq	%rsi, 8(%rsp)                   # 8-byte Spill
	movl	$30, %esi
	jne	.LBB7_106
# %bb.29:
	testb	%r10b, %r10b
	js	.LBB7_106
# %bb.30:
	leaq	20(%rsp), %rsi
	movq	%rsi, 8(%rsp)                   # 8-byte Spill
	movl	$29, %esi
	jne	.LBB7_106
# %bb.31:
	testb	%r8b, %r8b
	js	.LBB7_106
# %bb.32:
	leaq	21(%rsp), %rsi
	movq	%rsi, 8(%rsp)                   # 8-byte Spill
	movl	$28, %esi
	jne	.LBB7_106
# %bb.33:
	testb	%dil, %dil
	js	.LBB7_106
# %bb.34:
	leaq	22(%rsp), %rsi
	movq	%rsi, 8(%rsp)                   # 8-byte Spill
	movl	$27, %esi
	jne	.LBB7_106
# %bb.35:
	testb	%r12b, %r12b
	js	.LBB7_106
# %bb.36:
	leaq	23(%rsp), %rsi
	movq	%rsi, 8(%rsp)                   # 8-byte Spill
	movl	$26, %esi
	jne	.LBB7_106
# %bb.37:
	testb	%bpl, %bpl
	js	.LBB7_106
# %bb.38:
	leaq	24(%rsp), %rsi
	movq	%rsi, 8(%rsp)                   # 8-byte Spill
	movl	$25, %esi
	jne	.LBB7_106
# %bb.39:
	testq	%r9, %r9
	js	.LBB7_106
# %bb.40:
	leaq	25(%rsp), %rsi
	movq	%rsi, 8(%rsp)                   # 8-byte Spill
	movl	$24, %esi
	testb	%cl, %cl
	jne	.LBB7_106
# %bb.41:
	testb	%r14b, %r14b
	js	.LBB7_106
# %bb.42:
	leaq	26(%rsp), %rcx
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_43
# %bb.44:
	cmpb	$0, 264(%rsp)                   # 1-byte Folded Reload
	movl	$23, %esi
	js	.LBB7_106
# %bb.45:
	leaq	27(%rsp), %rcx
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_46
# %bb.47:
	cmpb	$0, 232(%rsp)                   # 1-byte Folded Reload
	movl	$22, %esi
	js	.LBB7_106
# %bb.48:
	leaq	28(%rsp), %rcx
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_49
# %bb.50:
	cmpb	$0, 224(%rsp)                   # 1-byte Folded Reload
	movl	$21, %esi
	js	.LBB7_106
# %bb.51:
	leaq	29(%rsp), %rcx
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_52
# %bb.53:
	cmpb	$0, 216(%rsp)                   # 1-byte Folded Reload
	movl	$20, %esi
	js	.LBB7_106
# %bb.54:
	leaq	30(%rsp), %rcx
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_55
# %bb.56:
	cmpb	$0, 208(%rsp)                   # 1-byte Folded Reload
	movl	$19, %esi
	js	.LBB7_106
# %bb.57:
	leaq	31(%rsp), %rcx
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_58
# %bb.59:
	testb	%r9b, %r9b
	movl	$18, %esi
	js	.LBB7_106
# %bb.60:
	leaq	32(%rsp), %rcx
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_61
# %bb.62:
	testq	%rax, %rax
	movl	$17, %esi
	js	.LBB7_106
# %bb.63:
	leaq	33(%rsp), %rcx
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	cmpb	$0, 200(%rsp)                   # 1-byte Folded Reload
	jne	.LBB7_64
# %bb.65:
	cmpb	$0, 192(%rsp)                   # 1-byte Folded Reload
	movl	$16, %esi
	js	.LBB7_106
# %bb.66:
	leaq	34(%rsp), %rcx
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_67
# %bb.68:
	cmpb	$0, 184(%rsp)                   # 1-byte Folded Reload
	movl	$15, %esi
	js	.LBB7_106
# %bb.69:
	leaq	35(%rsp), %rcx
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_70
# %bb.71:
	cmpb	$0, 176(%rsp)                   # 1-byte Folded Reload
	movl	$14, %esi
	js	.LBB7_106
# %bb.72:
	leaq	36(%rsp), %rcx
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_73
# %bb.74:
	cmpb	$0, 168(%rsp)                   # 1-byte Folded Reload
	movl	$13, %esi
	js	.LBB7_106
# %bb.75:
	leaq	37(%rsp), %rcx
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_76
# %bb.77:
	cmpb	$0, 160(%rsp)                   # 1-byte Folded Reload
	movl	$12, %esi
	js	.LBB7_106
# %bb.78:
	leaq	38(%rsp), %rcx
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_79
# %bb.80:
	cmpb	$0, 152(%rsp)                   # 1-byte Folded Reload
	movl	$11, %esi
	js	.LBB7_106
# %bb.81:
	leaq	39(%rsp), %rcx
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_82
# %bb.83:
	testb	%al, %al
	movl	$10, %esi
	js	.LBB7_106
# %bb.84:
	leaq	40(%rsp), %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_85
# %bb.86:
	testq	%r15, %r15
	movl	$9, %esi
	js	.LBB7_106
# %bb.87:
	leaq	41(%rsp), %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	cmpb	$0, 136(%rsp)                   # 1-byte Folded Reload
	jne	.LBB7_88
# %bb.89:
	cmpb	$0, 144(%rsp)                   # 1-byte Folded Reload
	movl	$8, %esi
	js	.LBB7_106
# %bb.90:
	leaq	42(%rsp), %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_91
# %bb.92:
	cmpb	$0, 104(%rsp)                   # 1-byte Folded Reload
	movl	$7, %esi
	js	.LBB7_106
# %bb.93:
	leaq	43(%rsp), %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_94
# %bb.95:
	cmpb	$0, 112(%rsp)                   # 1-byte Folded Reload
	movl	$6, %esi
	js	.LBB7_106
# %bb.96:
	leaq	44(%rsp), %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_97
# %bb.98:
	cmpb	$0, 120(%rsp)                   # 1-byte Folded Reload
	movl	$5, %esi
	js	.LBB7_106
# %bb.99:
	leaq	45(%rsp), %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_100
# %bb.101:
	cmpb	$0, 128(%rsp)                   # 1-byte Folded Reload
	movl	$4, %esi
	js	.LBB7_106
# %bb.102:
	leaq	46(%rsp), %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	jne	.LBB7_103
# %bb.104:
	testb	%bl, %bl
	movl	$3, %esi
	js	.LBB7_106
# %bb.105:
	leaq	47(%rsp), %rax
	leaq	48(%rsp), %rsi
	sete	%cl
	testb	%r15b, %r15b
	setns	%bl
	andb	%cl, %bl
	cmoveq	%rax, %rsi
	movq	%rsi, 8(%rsp)                   # 8-byte Spill
	movzbl	%bl, %eax
	movl	$2, %esi
	subl	%eax, %esi
	jmp	.LBB7_106
.LBB7_21:                               # %.thread49
	movq	272(%rsp), %r15                 # 8-byte Reload
	xorl	%edx, %edx
.LBB7_171:
	movl	$1, %eax
	testl	%edx, %edx
	jne	.LBB7_173
	jmp	.LBB7_172
.LBB7_43:
	movl	$23, %esi
	jmp	.LBB7_106
.LBB7_46:
	movl	$22, %esi
	jmp	.LBB7_106
.LBB7_49:
	movl	$21, %esi
	jmp	.LBB7_106
.LBB7_52:
	movl	$20, %esi
	jmp	.LBB7_106
.LBB7_1:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.2(%rip), %r8
	movl	$108, %ecx
	jmp	.LBB7_2
.LBB7_4:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.7(%rip), %r8
	movl	$109, %ecx
	jmp	.LBB7_2
.LBB7_6:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.4(%rip), %r8
	movl	$110, %ecx
	jmp	.LBB7_2
.LBB7_8:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.8(%rip), %r8
	movl	$111, %ecx
	jmp	.LBB7_2
.LBB7_10:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.9(%rip), %r8
	movl	$112, %ecx
	jmp	.LBB7_2
.LBB7_12:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.10(%rip), %r8
	movl	$113, %ecx
.LBB7_2:
	xorl	%eax, %eax
	callq	fprintf@PLT
	callq	abort@PLT
.LBB7_55:
	movl	$19, %esi
	jmp	.LBB7_106
.LBB7_58:
	movl	$18, %esi
	jmp	.LBB7_106
.LBB7_61:
	movl	$17, %esi
	jmp	.LBB7_106
.LBB7_64:
	movl	$16, %esi
	jmp	.LBB7_106
.LBB7_67:
	movl	$15, %esi
	jmp	.LBB7_106
.LBB7_70:
	movl	$14, %esi
	jmp	.LBB7_106
.LBB7_73:
	movl	$13, %esi
	jmp	.LBB7_106
.LBB7_76:
	movl	$12, %esi
	jmp	.LBB7_106
.LBB7_79:
	movl	$11, %esi
	jmp	.LBB7_106
.LBB7_82:
	movl	$10, %esi
	jmp	.LBB7_106
.LBB7_85:
	movl	$9, %esi
	jmp	.LBB7_106
.LBB7_88:
	movl	$8, %esi
	jmp	.LBB7_106
.LBB7_91:
	movl	$7, %esi
	jmp	.LBB7_106
.LBB7_94:
	movl	$6, %esi
	jmp	.LBB7_106
.LBB7_97:
	movl	$5, %esi
	jmp	.LBB7_106
.LBB7_100:
	movl	$4, %esi
	jmp	.LBB7_106
.LBB7_103:
	movl	$3, %esi
.LBB7_106:                              # %.critedge.i
	movq	%rsi, 264(%rsp)                 # 8-byte Spill
	leaq	64(%rsp), %r12
	cmpb	$0, 64(%rsp)
	jne	.LBB7_169
# %bb.107:                              # %.critedge.i
	testq	%rdx, %rdx
	js	.LBB7_169
# %bb.108:
	movq	%rdx, %rax
	shrq	$56, %rax
	leaq	65(%rsp), %r12
	movl	$32, %ecx
	movq	%rcx, (%rsp)                    # 8-byte Spill
	testb	%al, %al
	jne	.LBB7_169
# %bb.109:
	movq	%rdx, %r15
	shrq	$48, %r15
	movq	%rdx, %rsi
	shrq	$40, %rsi
	movq	%rdx, %r11
	shrq	$32, %r11
	movq	%rdx, %r13
	shrq	$24, %r13
	movq	%rdx, %r10
	shrq	$16, %r10
	movq	%rdx, %rdi
	shrq	$8, %rdi
	movq	256(%rsp), %rax                 # 8-byte Reload
	movq	%rax, %r8
	shrq	$56, %r8
	movq	%rax, %rcx
	shrq	$48, %rcx
	movq	%rax, %r9
	shrq	$40, %r9
	movq	%rax, %r14
	shrq	$32, %r14
	movq	%rax, %rbx
	shrq	$24, %rbx
	movq	%rbx, 232(%rsp)                 # 8-byte Spill
	movq	%rax, %rbx
	shrq	$16, %rbx
	movq	%rbx, 224(%rsp)                 # 8-byte Spill
	shrq	$8, %rax
	movq	%rax, 216(%rsp)                 # 8-byte Spill
	movq	248(%rsp), %rbx                 # 8-byte Reload
	movq	%rbx, %rbp
	shrq	$56, %rbp
	movq	%rbp, 208(%rsp)                 # 8-byte Spill
	movq	%rbx, %rbp
	shrq	$48, %rbp
	movq	%rbp, 200(%rsp)                 # 8-byte Spill
	movq	%rbx, %rbp
	shrq	$40, %rbp
	movq	%rbp, 192(%rsp)                 # 8-byte Spill
	movq	%rbx, %rbp
	shrq	$32, %rbp
	movq	%rbp, 184(%rsp)                 # 8-byte Spill
	movq	%rbx, %rbp
	shrq	$24, %rbp
	movq	%rbp, 176(%rsp)                 # 8-byte Spill
	movq	%rbx, %rbp
	shrq	$16, %rbp
	movq	%rbp, 168(%rsp)                 # 8-byte Spill
	shrq	$8, %rbx
	movq	%rbx, 160(%rsp)                 # 8-byte Spill
	movq	240(%rsp), %rbx                 # 8-byte Reload
	movq	%rbx, %rbp
	shrq	$56, %rbp
	movq	%rbp, 144(%rsp)                 # 8-byte Spill
	movq	%rbx, %rbp
	shrq	$48, %rbp
	movq	%rbp, 152(%rsp)                 # 8-byte Spill
	movq	%rbx, %rbp
	shrq	$40, %rbp
	movq	%rbp, 104(%rsp)                 # 8-byte Spill
	movq	%rbx, %rbp
	shrq	$32, %rbp
	movq	%rbp, 112(%rsp)                 # 8-byte Spill
	movq	%rbx, %rbp
	shrq	$24, %rbp
	movq	%rbp, 120(%rsp)                 # 8-byte Spill
	movq	%rbx, %rbp
	shrq	$16, %rbp
	movq	%rbp, 128(%rsp)                 # 8-byte Spill
	shrq	$8, %rbx
	movq	%rbx, 136(%rsp)                 # 8-byte Spill
	testb	%r15b, %r15b
	js	.LBB7_169
# %bb.110:
	leaq	66(%rsp), %r12
	movl	$31, %ebx
	movq	%rbx, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.111:
	testb	%sil, %sil
	js	.LBB7_169
# %bb.112:
	leaq	67(%rsp), %r12
	movl	$30, %esi
	movq	%rsi, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.113:
	testb	%r11b, %r11b
	js	.LBB7_169
# %bb.114:
	leaq	68(%rsp), %r12
	movl	$29, %esi
	movq	%rsi, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.115:
	testb	%r13b, %r13b
	js	.LBB7_169
# %bb.116:
	leaq	69(%rsp), %r12
	movl	$28, %esi
	movq	%rsi, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.117:
	testb	%r10b, %r10b
	js	.LBB7_169
# %bb.118:
	leaq	70(%rsp), %r12
	movl	$27, %esi
	movq	%rsi, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.119:
	testb	%dil, %dil
	js	.LBB7_169
# %bb.120:
	leaq	71(%rsp), %r12
	movl	$26, %esi
	movq	%rsi, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.121:
	testb	%dl, %dl
	js	.LBB7_169
# %bb.122:
	leaq	72(%rsp), %r12
	movl	$25, %edx
	movq	%rdx, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.123:
	cmpq	$0, 256(%rsp)                   # 8-byte Folded Reload
	js	.LBB7_169
# %bb.124:
	leaq	73(%rsp), %r12
	movl	$24, %edx
	movq	%rdx, (%rsp)                    # 8-byte Spill
	testb	%r8b, %r8b
	jne	.LBB7_169
# %bb.125:
	testb	%cl, %cl
	js	.LBB7_169
# %bb.126:
	leaq	74(%rsp), %r12
	movl	$23, %ecx
	movq	%rcx, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.127:
	testb	%r9b, %r9b
	js	.LBB7_169
# %bb.128:
	leaq	75(%rsp), %r12
	movl	$22, %ecx
	movq	%rcx, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.129:
	testb	%r14b, %r14b
	js	.LBB7_169
# %bb.130:
	leaq	76(%rsp), %r12
	movl	$21, %ecx
	movq	%rcx, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.131:
	cmpb	$0, 232(%rsp)                   # 1-byte Folded Reload
	js	.LBB7_169
# %bb.132:
	leaq	77(%rsp), %r12
	movl	$20, %ecx
	movq	%rcx, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.133:
	cmpb	$0, 224(%rsp)                   # 1-byte Folded Reload
	js	.LBB7_169
# %bb.134:
	leaq	78(%rsp), %r12
	movl	$19, %ecx
	movq	%rcx, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.135:
	cmpb	$0, 216(%rsp)                   # 1-byte Folded Reload
	js	.LBB7_169
# %bb.136:
	leaq	79(%rsp), %r12
	movl	$18, %eax
	movq	%rax, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.137:
	cmpb	$0, 256(%rsp)                   # 1-byte Folded Reload
	js	.LBB7_169
# %bb.138:
	leaq	80(%rsp), %r12
	movl	$17, %eax
	movq	%rax, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.139:
	cmpq	$0, 248(%rsp)                   # 8-byte Folded Reload
	js	.LBB7_169
# %bb.140:
	leaq	81(%rsp), %r12
	movl	$16, %eax
	movq	%rax, (%rsp)                    # 8-byte Spill
	cmpb	$0, 208(%rsp)                   # 1-byte Folded Reload
	jne	.LBB7_169
# %bb.141:
	cmpb	$0, 200(%rsp)                   # 1-byte Folded Reload
	js	.LBB7_169
# %bb.142:
	leaq	82(%rsp), %r12
	movl	$15, %eax
	movq	%rax, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.143:
	cmpb	$0, 192(%rsp)                   # 1-byte Folded Reload
	js	.LBB7_169
# %bb.144:
	leaq	83(%rsp), %r12
	movl	$14, %eax
	movq	%rax, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.145:
	cmpb	$0, 184(%rsp)                   # 1-byte Folded Reload
	js	.LBB7_169
# %bb.146:
	leaq	84(%rsp), %r12
	movl	$13, %eax
	movq	%rax, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.147:
	cmpb	$0, 176(%rsp)                   # 1-byte Folded Reload
	js	.LBB7_169
# %bb.148:
	leaq	85(%rsp), %r12
	movl	$12, %eax
	movq	%rax, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.149:
	cmpb	$0, 168(%rsp)                   # 1-byte Folded Reload
	js	.LBB7_169
# %bb.150:
	leaq	86(%rsp), %r12
	movl	$11, %eax
	movq	%rax, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.151:
	cmpb	$0, 160(%rsp)                   # 1-byte Folded Reload
	js	.LBB7_169
# %bb.152:
	leaq	87(%rsp), %r12
	movl	$10, %eax
	movq	%rax, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.153:
	cmpb	$0, 248(%rsp)                   # 1-byte Folded Reload
	js	.LBB7_169
# %bb.154:
	leaq	88(%rsp), %r12
	movl	$9, %eax
	movq	%rax, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.155:
	cmpq	$0, 240(%rsp)                   # 8-byte Folded Reload
	js	.LBB7_169
# %bb.156:
	leaq	89(%rsp), %r12
	movl	$8, %eax
	movq	%rax, (%rsp)                    # 8-byte Spill
	cmpb	$0, 144(%rsp)                   # 1-byte Folded Reload
	jne	.LBB7_169
# %bb.157:
	cmpb	$0, 152(%rsp)                   # 1-byte Folded Reload
	js	.LBB7_169
# %bb.158:
	leaq	90(%rsp), %r12
	movl	$7, %eax
	movq	%rax, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.159:
	cmpb	$0, 104(%rsp)                   # 1-byte Folded Reload
	js	.LBB7_169
# %bb.160:
	leaq	91(%rsp), %r12
	movl	$6, %eax
	movq	%rax, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.161:
	cmpb	$0, 112(%rsp)                   # 1-byte Folded Reload
	js	.LBB7_169
# %bb.162:
	leaq	92(%rsp), %r12
	movl	$5, %eax
	movq	%rax, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.163:
	cmpb	$0, 120(%rsp)                   # 1-byte Folded Reload
	js	.LBB7_169
# %bb.164:
	leaq	93(%rsp), %r12
	movl	$4, %eax
	movq	%rax, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.165:
	cmpb	$0, 128(%rsp)                   # 1-byte Folded Reload
	js	.LBB7_169
# %bb.166:
	leaq	94(%rsp), %r12
	movl	$3, %eax
	movq	%rax, (%rsp)                    # 8-byte Spill
	jne	.LBB7_169
# %bb.167:
	cmpb	$0, 136(%rsp)                   # 1-byte Folded Reload
	js	.LBB7_169
# %bb.168:
	leaq	95(%rsp), %rax
	leaq	96(%rsp), %r12
	sete	%cl
	cmpb	$0, 240(%rsp)                   # 1-byte Folded Reload
	setns	%dl
	andb	%cl, %dl
	cmoveq	%rax, %r12
	movzbl	%dl, %eax
	movl	$2, %ecx
	subl	%eax, %ecx
	movq	%rcx, (%rsp)                    # 8-byte Spill
.LBB7_169:                              # %.critedge2.i
	movq	(%rsp), %r14                    # 8-byte Reload
	movq	264(%rsp), %rcx                 # 8-byte Reload
	leal	(%rcx,%r14), %eax
	addl	$6, %eax
	xorl	%edx, %edx
	movq	272(%rsp), %r15                 # 8-byte Reload
	cmpl	%eax, (%r15)
	jl	.LBB7_171
# %bb.170:
	movl	%eax, (%r15)
	movq	280(%rsp), %rbp                 # 8-byte Reload
	movb	$48, (%rbp)
	movl	%ecx, %ebx
	leaq	4(%rbx), %rax
	addl	%r14d, %eax
	movb	%al, 1(%rbp)
	movb	$2, 2(%rbp)
	movb	%cl, 3(%rbp)
	leaq	4(%rbp), %rdi
	movq	8(%rsp), %rsi                   # 8-byte Reload
	movq	%rbx, %rdx
	callq	memcpy@PLT
	movb	$2, 4(%rbp,%rbx)
	movb	%r14b, 5(%rbp,%rbx)
	leaq	(%rbx,%rbp), %rdi
	addq	$6, %rdi
	movl	%r14d, %edx
	movq	%r12, %rsi
	callq	memcpy@PLT
	movl	$1, %edx
	jmp	.LBB7_171
.Lfunc_end7:
	.size	secp256k1_ecdsa_sign, .Lfunc_end7-secp256k1_ecdsa_sign
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function secp256k1_ecdsa_sig_sign
.LCPI8_0:
	.zero	16
	.text
	.p2align	4, 0x90
	.type	secp256k1_ecdsa_sig_sign,@function
secp256k1_ecdsa_sig_sign:               # @secp256k1_ecdsa_sig_sign
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$344, %rsp                      # imm = 0x158
	.cfi_def_cfa_offset 400
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r9, 56(%rsp)                   # 8-byte Spill
	movq	%rcx, 80(%rsp)                  # 8-byte Spill
	movq	%rdx, 72(%rsp)                  # 8-byte Spill
	movq	%rsi, (%rsp)                    # 8-byte Spill
	movl	$0, 12(%rsp)
	leaq	144(%rsp), %rsi
	movq	%r8, 88(%rsp)                   # 8-byte Spill
	movq	%r8, %rdx
	callq	secp256k1_ecmult_gen
	leaq	224(%rsp), %rbp
	movq	%rbp, %rdi
	movq	%rbp, %rsi
	callq	secp256k1_fe_inv_var
	leaq	272(%rsp), %rdi
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 96(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 16(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 24(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	16(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	24(%rsp), %rsi
	movq	96(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	96(%rsp), %rdi
	leaq	272(%rsp), %rbx
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 24(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 64(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 16(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	64(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rsp), %rsi
	movq	24(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	272(%rsp), %rbx
	leaq	144(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 24(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 64(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 16(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	64(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rsp), %rsi
	movq	24(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	184(%rsp), %rdi
	leaq	96(%rsp), %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 24(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 64(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 16(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	64(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rsp), %rsi
	movq	24(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	$1, 224(%rsp)
	xorps	%xmm0, %xmm0
	movups	%xmm0, 232(%rsp)
	movups	%xmm0, 248(%rsp)
	movq	176(%rsp), %r12
	movq	%r12, %rsi
	shrq	$48, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	imulq	%rax, %rsi
	movq	%rax, %rcx
	addq	144(%rsp), %rsi
	movq	%rsi, %rbx
	shrq	$52, %rbx
	addq	152(%rsp), %rbx
	movq	%rbx, %rax
	shrq	$52, %rax
	addq	160(%rsp), %rax
	movq	216(%rsp), %r8
	movq	%rax, %rdi
	shrq	$52, %rdi
	addq	168(%rsp), %rdi
	movq	%r8, %r15
	shrq	$48, %r15
	imulq	%rcx, %r15
	addq	184(%rsp), %r15
	movq	%r15, %r9
	shrq	$52, %r9
	addq	192(%rsp), %r9
	movq	%r9, %rdx
	shrq	$52, %rdx
	addq	200(%rsp), %rdx
	movq	%rdx, %r13
	shrq	$52, %r13
	addq	208(%rsp), %r13
	movabsq	$281474976710655, %r10          # imm = 0xFFFFFFFFFFFF
	andq	%r10, %r12
	movabsq	$4503599627370495, %r11         # imm = 0xFFFFFFFFFFFFF
	andq	%r11, %rsi
	andq	%r11, %rax
	movq	%rax, %rcx
	andq	%rbx, %rcx
	andq	%r11, %rbx
	movq	%rdi, %rbp
	shrq	$52, %rbp
	addq	%r12, %rbp
	andq	%rdi, %rcx
	andq	%r11, %rdi
	movq	%rbp, %r14
	shrq	$48, %r14
	movq	%rbp, %r12
	xorq	%r10, %r12
	xorq	%r11, %rcx
	orq	%r12, %rcx
	sete	%cl
	movabsq	$4503595332402222, %r12         # imm = 0xFFFFEFFFFFC2E
	cmpq	%r12, %rsi
	seta	%r12b
	andb	%cl, %r12b
	movzbl	%r12b, %r12d
	orq	%r14, %r12
	movabsq	$4294968273, %rcx               # imm = 0x1000003D1
	imulq	%rcx, %r12
	addq	%rsi, %r12
	movq	%r12, %rsi
	shrq	$52, %rsi
	addq	%rbx, %rsi
	movq	%rsi, %rbx
	shrq	$52, %rbx
	addq	%rax, %rbx
	movq	%rbx, %rax
	shrq	$52, %rax
	addq	%rdi, %rax
	movq	%rax, %rdi
	shrq	$52, %rdi
	addq	%rbp, %rdi
	andq	%r9, %rdx
	andq	%r11, %r12
	andq	%r10, %r8
	andq	%r13, %rdx
	movq	%r13, %r14
	shrq	$52, %r14
	addq	%r8, %r14
	movq	%r15, %rbp
	andq	%r11, %rbp
	notq	%rdx
	andq	%r11, %rdx
	xorq	%r14, %r10
	orq	%r10, %rdx
	sete	%r13b
	movabsq	$4503595332402222, %rcx         # imm = 0xFFFFEFFFFFC2E
	cmpq	%rcx, %rbp
	seta	%bpl
	shlq	$16, %rdi
	movq	%rax, %rdx
	shrq	$36, %rdx
	movzwl	%dx, %edx
	orq	%rdi, %rdx
	bswapq	%rdx
	movq	%rdx, 96(%rsp)
	shlq	$28, %rax
	movq	%rbx, %rdx
	shrq	$24, %rdx
	andl	$268435455, %edx                # imm = 0xFFFFFFF
	orq	%rax, %rdx
	bswapq	%rdx
	movq	%rdx, 104(%rsp)
	movq	%rsi, %rax
	shrq	$12, %rax
	movabsq	$1099511627775, %rdx            # imm = 0xFFFFFFFFFF
	andq	%rax, %rdx
	shlq	$40, %rbx
	orq	%rbx, %rdx
	bswapq	%rdx
	movq	%rdx, 112(%rsp)
	shlq	$52, %rsi
	orq	%r12, %rsi
	bswapq	%rsi
	movq	%rsi, 120(%rsp)
	leaq	96(%rsp), %rsi
	leaq	12(%rsp), %rdx
	movq	(%rsp), %rbx                    # 8-byte Reload
	movq	%rbx, %rdi
	callq	secp256k1_scalar_set_b32
	movdqu	(%rbx), %xmm0
	movdqu	16(%rbx), %xmm1
	por	%xmm0, %xmm1
	pcmpeqb	.LCPI8_0(%rip), %xmm1
	pmovmskb	%xmm1, %ecx
	xorl	%eax, %eax
	cmpl	$65535, %ecx                    # imm = 0xFFFF
	je	.LBB8_7
# %bb.1:
	movq	56(%rsp), %rax                  # 8-byte Reload
	testq	%rax, %rax
	je	.LBB8_3
# %bb.2:
	movq	%rax, %rdx
	shrq	$48, %r14
	andb	%r13b, %bpl
	movzbl	%bpl, %eax
	orq	%r14, %rax
	movabsq	$4294968273, %rcx               # imm = 0x1000003D1
	imulq	%rcx, %rax
	addq	%r15, %rax
	xorl	%ecx, %ecx
	cmpl	$0, 12(%rsp)
	setne	%cl
	andl	$1, %eax
	leal	(%rax,%rcx,2), %eax
	movl	%eax, (%rdx)
.LBB8_3:
	leaq	272(%rsp), %rsi
	movq	(%rsp), %rdi                    # 8-byte Reload
	movq	72(%rsp), %rdx                  # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	24(%rsp), %r14
	movq	%r14, %rdi
	callq	secp256k1_scalar_reduce_512
	movq	80(%rsp), %rcx                  # 8-byte Reload
	movq	(%rcx), %r9
	addq	24(%rsp), %r9
	movq	32(%rsp), %r10
	adcq	$0, %r10
	movq	24(%rcx), %rax
	setb	%bl
	addq	8(%rcx), %r10
	movzbl	%bl, %esi
	adcq	40(%rsp), %rsi
	setb	%bl
	xorl	%edi, %edi
	addq	48(%rsp), %rax
	setb	%dil
	addq	16(%rcx), %rsi
	movzbl	%bl, %ebp
	adcq	%rbp, %rax
	setb	%r8b
	cmpq	$-1, %rax
	setne	%bl
	cmpq	$-2, %rsi
	setb	%cl
	orb	%bl, %cl
	movzbl	%cl, %ebx
	xorl	%ebp, %ebp
	cmpq	$-1, %rsi
	sete	%bpl
	movl	%ebx, %ecx
	notl	%ecx
	andl	%ebp, %ecx
	movabsq	$-4994812053365940165, %rdx     # imm = 0xBAAEDCE6AF48A03B
	xorl	%ebp, %ebp
	cmpq	%rdx, %r10
	seta	%bpl
	setb	%dl
	orb	%dl, %bl
	movzbl	%bl, %edx
	notl	%edx
	andl	%edx, %ebp
	orl	%ecx, %ebp
	movabsq	$-4624529908474429120, %rcx     # imm = 0xBFD25E8CD0364140
	xorl	%ebx, %ebx
	cmpq	%rcx, %r9
	seta	%bl
	andl	%edx, %ebx
	orl	%ebp, %ebx
	addb	$255, %r8b
	adcl	%ebx, %edi
	movabsq	$4624529908474429119, %rcx      # imm = 0x402DA1732FC9BEBF
	imulq	%rdi, %rcx
	movabsq	$4994812053365940164, %rdx      # imm = 0x4551231950B75FC4
	imulq	%rdi, %rdx
	addq	%r9, %rcx
	movq	%rcx, 24(%rsp)
	adcq	%r10, %rdx
	movq	%rdx, 32(%rsp)
	adcq	%rdi, %rsi
	movq	%rsi, 40(%rsp)
	adcq	$0, %rax
	movq	%rax, 48(%rsp)
	movq	(%rsp), %rax                    # 8-byte Reload
	leaq	32(%rax), %rbp
	movq	%rbp, %rdi
	movq	88(%rsp), %rsi                  # 8-byte Reload
	callq	secp256k1_scalar_inverse
	leaq	272(%rsp), %rsi
	movq	%rbp, %rdi
	movq	%r14, %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	movq	%rbp, %rdi
	callq	secp256k1_scalar_reduce_512
	movq	(%rsp), %rax                    # 8-byte Reload
	movq	32(%rax), %r8
	movq	40(%rax), %r10
	movq	%r10, %rdi
	orq	%r8, %rdi
	movq	48(%rax), %r11
	orq	%r11, %rdi
	movq	56(%rax), %rdx
	orq	%rdx, %rdi
	movl	$0, %eax
	je	.LBB8_7
# %bb.4:
	movabsq	$9223372036854775807, %rdi      # imm = 0x7FFFFFFFFFFFFFFF
	xorl	%ebp, %ebp
	cmpq	%rdi, %rdx
	setb	%bpl
	movq	%rdx, %rdi
	shrq	$63, %rdi
	xorl	%ebx, %ebx
	cmpq	$-1, %r11
	setne	%bl
	movl	%edi, %esi
	notl	%esi
	andl	%esi, %ebx
	orl	%ebp, %ebx
	movabsq	$6725966010171805725, %r9       # imm = 0x5D576E7357A4501D
	xorl	%ebp, %ebp
	xorl	%ecx, %ecx
	cmpq	%r9, %r10
	setb	%bpl
	seta	%cl
	andl	%esi, %ebp
	orl	%ebx, %ebp
	notl	%ebp
	andl	%ebp, %ecx
	orl	%edi, %ecx
	movabsq	$-2312264954237214560, %rsi     # imm = 0xDFE92F46681B20A0
	xorl	%edi, %edi
	cmpq	%rsi, %r8
	seta	%dil
	andl	%ebp, %edi
	movl	$1, %eax
	orl	%ecx, %edi
	je	.LBB8_7
# %bb.5:
	notq	%r8
	movabsq	$-4624529908474429118, %rcx     # imm = 0xBFD25E8CD0364142
	addq	%r8, %rcx
	notq	%r10
	adcq	$0, %r10
	setb	%bl
	movzbl	%bl, %esi
	movabsq	$-4994812053365940165, %rdi     # imm = 0xBAAEDCE6AF48A03B
	addq	%rdi, %r10
	notq	%r11
	adcq	%rsi, %r11
	setb	%bl
	movzbl	%bl, %esi
	addq	$-2, %r11
	notq	%rdx
	adcq	%rsi, %rdx
	addq	$-1, %rdx
	movq	(%rsp), %rsi                    # 8-byte Reload
	movq	%rcx, 32(%rsi)
	movq	%r10, 40(%rsi)
	movq	%r11, 48(%rsi)
	movq	%rdx, 56(%rsi)
	movq	56(%rsp), %rcx                  # 8-byte Reload
	testq	%rcx, %rcx
	je	.LBB8_7
# %bb.6:
	xorb	$1, (%rcx)
.LBB8_7:
	addq	$344, %rsp                      # imm = 0x158
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end8:
	.size	secp256k1_ecdsa_sig_sign, .Lfunc_end8-secp256k1_ecdsa_sig_sign
	.cfi_endproc
                                        # -- End function
	.globl	secp256k1_ecdsa_sign_compact    # -- Begin function secp256k1_ecdsa_sign_compact
	.p2align	4, 0x90
	.type	secp256k1_ecdsa_sign_compact,@function
secp256k1_ecdsa_sign_compact:           # @secp256k1_ecdsa_sign_compact
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$232, %rsp
	.cfi_def_cfa_offset 288
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r9, 24(%rsp)                   # 8-byte Spill
	movq	%rsi, 8(%rsp)                   # 8-byte Spill
	movl	$0, 4(%rsp)
	testq	%rdi, %rdi
	je	.LBB9_1
# %bb.3:
	movq	%rdi, %rbp
	cmpq	$0, 8(%rdi)
	je	.LBB9_4
# %bb.5:
	cmpq	$0, 8(%rsp)                     # 8-byte Folded Reload
	je	.LBB9_6
# %bb.7:
	movq	%rdx, %rbx
	testq	%rdx, %rdx
	je	.LBB9_8
# %bb.9:
	movq	%rcx, %r12
	testq	%rcx, %rcx
	je	.LBB9_10
# %bb.11:
	testq	%r8, %r8
	leaq	nonce_function_rfc6979(%rip), %r13
	cmovneq	%r8, %r13
	leaq	160(%rsp), %rdi
	leaq	4(%rsp), %rdx
	movq	%r12, %rsi
	callq	secp256k1_scalar_set_b32
	cmpl	$0, 4(%rsp)
	je	.LBB9_12
.LBB9_22:                               # %.thread47
	pxor	%xmm0, %xmm0
	movdqu	%xmm0, 48(%rbx)
	movdqu	%xmm0, 32(%rbx)
	movdqu	%xmm0, 16(%rbx)
	movdqu	%xmm0, (%rbx)
	xorl	%eax, %eax
.LBB9_23:
                                        # kill: def $eax killed $eax killed $rax
	addq	$232, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB9_12:
	.cfi_def_cfa_offset 288
	movdqa	160(%rsp), %xmm0
	por	176(%rsp), %xmm0
	pxor	%xmm1, %xmm1
	pcmpeqb	%xmm0, %xmm1
	pmovmskb	%xmm1, %eax
	cmpl	$65535, %eax                    # imm = 0xFFFF
	je	.LBB9_22
# %bb.13:
	movq	%rbp, 16(%rsp)                  # 8-byte Spill
	leaq	200(%rsp), %rdi
	movq	8(%rsp), %rbp                   # 8-byte Reload
	movq	%rbp, %rsi
	xorl	%edx, %edx
	callq	secp256k1_scalar_set_b32
	leaq	96(%rsp), %rdi
	movq	%rbp, %rsi
	movq	%r12, %rdx
	xorl	%ecx, %ecx
	movq	24(%rsp), %r8                   # 8-byte Reload
	callq	*%r13
	testl	%eax, %eax
	je	.LBB9_19
# %bb.14:                               # %.lr.ph.preheader
	movl	%eax, %ebp
	addq	$8, 16(%rsp)                    # 8-byte Folded Spill
	movl	$1, %r14d
	leaq	96(%rsp), %r15
	jmp	.LBB9_15
	.p2align	4, 0x90
.LBB9_18:                               #   in Loop: Header=BB9_15 Depth=1
	movq	%r15, %rdi
	movq	8(%rsp), %rsi                   # 8-byte Reload
	movq	%r12, %rdx
	movl	%r14d, %ecx
	movq	24(%rsp), %r8                   # 8-byte Reload
	callq	*%r13
	movl	%eax, %ebp
	addl	$1, %r14d
	testl	%eax, %eax
	je	.LBB9_19
.LBB9_15:                               # %.lr.ph
                                        # =>This Inner Loop Header: Depth=1
	leaq	128(%rsp), %rdi
	movq	%r15, %rsi
	leaq	4(%rsp), %rdx
	callq	secp256k1_scalar_set_b32
	pxor	%xmm1, %xmm1
	movdqa	%xmm1, 112(%rsp)
	movdqa	128(%rsp), %xmm0
	por	144(%rsp), %xmm0
	movdqa	%xmm1, 96(%rsp)
	pcmpeqb	%xmm1, %xmm0
	pmovmskb	%xmm0, %eax
	cmpl	$65535, %eax                    # imm = 0xFFFF
	je	.LBB9_18
# %bb.16:                               # %.lr.ph
                                        #   in Loop: Header=BB9_15 Depth=1
	cmpl	$0, 4(%rsp)
	jne	.LBB9_18
# %bb.17:                               #   in Loop: Header=BB9_15 Depth=1
	movq	16(%rsp), %rdi                  # 8-byte Reload
	leaq	32(%rsp), %rsi
	leaq	160(%rsp), %rdx
	leaq	200(%rsp), %rcx
	leaq	128(%rsp), %r8
	movq	288(%rsp), %r9
	callq	secp256k1_ecdsa_sig_sign
	testl	%eax, %eax
	je	.LBB9_18
# %bb.20:
	movq	56(%rsp), %rax
	bswapq	%rax
	movq	%rax, (%rbx)
	movq	48(%rsp), %rax
	bswapq	%rax
	movq	%rax, 8(%rbx)
	movq	40(%rsp), %rax
	bswapq	%rax
	movq	%rax, 16(%rbx)
	movq	32(%rsp), %rax
	bswapq	%rax
	movq	%rax, 24(%rbx)
	movq	88(%rsp), %rax
	bswapq	%rax
	movq	%rax, 32(%rbx)
	movq	80(%rsp), %rax
	bswapq	%rax
	movq	%rax, 40(%rbx)
	movq	72(%rsp), %rax
	bswapq	%rax
	movq	%rax, 48(%rbx)
	movq	64(%rsp), %rax
	bswapq	%rax
	movq	%rax, 56(%rbx)
	movl	%ebp, %eax
	testl	%eax, %eax
	jne	.LBB9_23
	jmp	.LBB9_22
.LBB9_19:                               # %.thread45
	xorl	%eax, %eax
	testl	%eax, %eax
	jne	.LBB9_23
	jmp	.LBB9_22
.LBB9_1:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.2(%rip), %r8
	movl	$156, %ecx
	jmp	.LBB9_2
.LBB9_4:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.7(%rip), %r8
	movl	$157, %ecx
	jmp	.LBB9_2
.LBB9_6:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.4(%rip), %r8
	movl	$158, %ecx
	jmp	.LBB9_2
.LBB9_8:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.11(%rip), %r8
	movl	$159, %ecx
	jmp	.LBB9_2
.LBB9_10:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.10(%rip), %r8
	movl	$160, %ecx
.LBB9_2:
	xorl	%eax, %eax
	callq	fprintf@PLT
	callq	abort@PLT
.Lfunc_end9:
	.size	secp256k1_ecdsa_sign_compact, .Lfunc_end9-secp256k1_ecdsa_sign_compact
	.cfi_endproc
                                        # -- End function
	.globl	secp256k1_ecdsa_recover_compact # -- Begin function secp256k1_ecdsa_recover_compact
	.p2align	4, 0x90
	.type	secp256k1_ecdsa_recover_compact,@function
secp256k1_ecdsa_recover_compact:        # @secp256k1_ecdsa_recover_compact
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$824, %rsp                      # imm = 0x338
	.cfi_def_cfa_offset 880
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	$0, 12(%rsp)
	testq	%rdi, %rdi
	je	.LBB10_1
# %bb.3:
	cmpq	$0, (%rdi)
	je	.LBB10_4
# %bb.5:
	movq	%rsi, %r14
	testq	%rsi, %rsi
	je	.LBB10_6
# %bb.7:
	movq	%rdx, %rbp
	testq	%rdx, %rdx
	je	.LBB10_8
# %bb.9:
	testq	%rcx, %rcx
	je	.LBB10_10
# %bb.11:
	movq	%r8, %rbx
	testq	%r8, %r8
	je	.LBB10_12
# %bb.13:
	movq	%rdi, 136(%rsp)                 # 8-byte Spill
	movq	%rcx, 128(%rsp)                 # 8-byte Spill
	movl	880(%rsp), %r12d
	cmpl	$4, %r12d
	jae	.LBB10_14
# %bb.15:
	movl	%r9d, %r13d
	leaq	144(%rsp), %rdi
	leaq	12(%rsp), %rdx
	movq	%rbp, %rsi
	callq	secp256k1_scalar_set_b32
	xorl	%r15d, %r15d
	cmpl	$0, 12(%rsp)
	jne	.LBB10_33
# %bb.16:
	leaq	176(%rsp), %rdi
	addq	$32, %rbp
	leaq	12(%rsp), %rdx
	movq	%rbp, %rsi
	callq	secp256k1_scalar_set_b32
	cmpl	$0, 12(%rsp)
	je	.LBB10_17
.LBB10_33:
	movl	%r15d, %eax
	addq	$824, %rsp                      # imm = 0x338
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB10_17:
	.cfi_def_cfa_offset 880
	movq	%rbx, 120(%rsp)                 # 8-byte Spill
	xorl	%r15d, %r15d
	leaq	688(%rsp), %rdi
	movq	%r14, %rsi
	xorl	%edx, %edx
	callq	secp256k1_scalar_set_b32
	movq	144(%rsp), %rdi
	movq	152(%rsp), %rbp
	movq	%rbp, %rcx
	orq	%rdi, %rcx
	movq	160(%rsp), %rbx
	orq	%rbx, %rcx
	movq	168(%rsp), %rax
	orq	%rax, %rcx
	je	.LBB10_33
# %bb.18:
	movdqu	176(%rsp), %xmm0
	movdqu	192(%rsp), %xmm1
	por	%xmm0, %xmm1
	pxor	%xmm0, %xmm0
	pcmpeqb	%xmm1, %xmm0
	pmovmskb	%xmm0, %ecx
	cmpl	$65535, %ecx                    # imm = 0xFFFF
	je	.LBB10_33
# %bb.19:                               # %secp256k1_fe_set_b32.exit.i
	movabsq	$4503599627370495, %r8          # imm = 0xFFFFFFFFFFFFF
	movq	%rdi, %rcx
	andq	%r8, %rcx
	movq	%rcx, 80(%rsp)
	shrq	$52, %rdi
	movq	%rbp, %rsi
	shlq	$12, %rsi
	leaq	-4095(%r8), %rdx
	andq	%rsi, %rdx
	orq	%rdi, %rdx
	movq	%rdx, 88(%rsp)
	shrq	$40, %rbp
	movq	%rax, %rsi
	shldq	$36, %rbx, %rsi
	shlq	$24, %rbx
	leaq	-16777215(%r8), %rdi
	andq	%rbx, %rdi
	orq	%rbp, %rdi
	movq	%rdi, 96(%rsp)
	andq	%r8, %rsi
	movq	%rsi, 104(%rsp)
	movq	%rax, %rbp
	shrq	$16, %rbp
	movq	%rbp, 112(%rsp)
	testb	$2, %r12b
	je	.LBB10_28
# %bb.20:
	cmpq	$65535, %rax                    # imm = 0xFFFF
	ja	.LBB10_33
# %bb.21:
	testq	%rsi, %rsi
	jne	.LBB10_33
# %bb.22:
	cmpq	$21319971, %rdi                 # imm = 0x1455123
	ja	.LBB10_33
# %bb.23:
	jne	.LBB10_27
# %bb.24:
	movabsq	$445351433356290, %rax          # imm = 0x1950B75FC4402
	cmpq	%rax, %rdx
	ja	.LBB10_33
# %bb.25:
	jne	.LBB10_27
# %bb.26:
	movabsq	$3836686497331949, %rax         # imm = 0xDA1722FC9BAED
	cmpq	%rax, %rcx
	ja	.LBB10_33
.LBB10_27:                              # %secp256k1_fe_cmp_var.exit.thread22.i
	movabsq	$666908835070273, %rax          # imm = 0x25E8CD0364141
	addq	%rax, %rcx
	movq	%rcx, 80(%rsp)
	movabsq	$4058248194014205, %rax         # imm = 0xE6AF48A03BBFD
	addq	%rax, %rdx
	movq	%rdx, 88(%rsp)
	leaq	(%r8,%rdi), %rax
	addq	$-21319971, %rax                # imm = 0xFEBAAEDD
	movq	%rax, 96(%rsp)
	movq	%r8, 104(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	addq	%rax, %rbp
	movq	%rbp, 112(%rsp)
.LBB10_28:
	andl	$1, %r12d
	leaq	424(%rsp), %rdi
	leaq	80(%rsp), %rsi
	movl	%r12d, %edx
	callq	secp256k1_ge_set_xo_var
	testl	%eax, %eax
	je	.LBB10_33
# %bb.29:
	movl	%r13d, 44(%rsp)                 # 4-byte Spill
	movl	504(%rsp), %eax
	movl	%eax, 632(%rsp)
	movups	424(%rsp), %xmm0
	movdqu	440(%rsp), %xmm1
	movaps	%xmm0, 512(%rsp)
	movdqa	%xmm1, 528(%rsp)
	movq	456(%rsp), %rax
	movq	%rax, 544(%rsp)
	movups	464(%rsp), %xmm0
	movups	%xmm0, 552(%rsp)
	movdqu	480(%rsp), %xmm0
	movdqu	%xmm0, 568(%rsp)
	movq	496(%rsp), %rax
	movq	%rax, 584(%rsp)
	movq	$1, 592(%rsp)
	pxor	%xmm0, %xmm0
	movdqu	%xmm0, 600(%rsp)
	movdqu	%xmm0, 616(%rsp)
	leaq	720(%rsp), %rbx
	leaq	144(%rsp), %rsi
	movq	%rbx, %rdi
	callq	secp256k1_scalar_inverse
	leaq	336(%rsp), %rsi
	movq	%rbx, %rdi
	leaq	688(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	48(%rsp), %rdi
	callq	secp256k1_scalar_reduce_512
	movq	48(%rsp), %rdi
	movq	56(%rsp), %rbx
	movq	%rbx, %rsi
	orq	%rdi, %rsi
	movq	64(%rsp), %rdx
	orq	%rdx, %rsi
	movq	72(%rsp), %rax
	xorl	%ecx, %ecx
	orq	%rax, %rsi
	setne	%cl
	negq	%rcx
	notq	%rdi
	movabsq	$-4624529908474429118, %rsi     # imm = 0xBFD25E8CD0364142
	addq	%rdi, %rsi
	notq	%rbx
	adcq	$0, %rbx
	setb	%dil
	andq	%rcx, %rsi
	movzbl	%dil, %edi
	movabsq	$-4994812053365940165, %rbp     # imm = 0xBAAEDCE6AF48A03B
	addq	%rbx, %rbp
	notq	%rdx
	adcq	%rdi, %rdx
	movq	%rsi, 48(%rsp)
	setb	%bl
	andq	%rcx, %rbp
	movzbl	%bl, %esi
	addq	$-2, %rdx
	notq	%rax
	adcq	%rsi, %rax
	movq	%rbp, 56(%rsp)
	andq	%rcx, %rdx
	movq	%rdx, 64(%rsp)
	addq	$-1, %rax
	andq	%rcx, %rax
	movq	%rax, 72(%rsp)
	leaq	336(%rsp), %rsi
	leaq	720(%rsp), %rdi
	leaq	176(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	792(%rsp), %rbx
	movq	%rbx, %rdi
	callq	secp256k1_scalar_reduce_512
	leaq	208(%rsp), %rsi
	leaq	512(%rsp), %rdx
	movq	136(%rsp), %rdi                 # 8-byte Reload
	movq	%rbx, %rcx
	leaq	48(%rsp), %r8
	callq	secp256k1_ecmult
	movl	328(%rsp), %eax
	movl	%eax, 416(%rsp)
	testl	%eax, %eax
	je	.LBB10_31
# %bb.30:                               # %secp256k1_ecdsa_sig_recover.exit.thread34
	xorl	%r15d, %r15d
	jmp	.LBB10_33
.LBB10_31:                              # %secp256k1_ecdsa_sig_recover.exit
	leaq	288(%rsp), %rbp
	movq	%rbp, %rdi
	movq	%rbp, %rsi
	callq	secp256k1_fe_inv_var
	leaq	648(%rsp), %rdi
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 32(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 16(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 24(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	16(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	24(%rsp), %rsi
	movq	32(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	752(%rsp), %rdi
	leaq	648(%rsp), %rbx
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 32(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 16(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 24(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	16(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	24(%rsp), %rsi
	movq	32(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	648(%rsp), %rbx
	leaq	208(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 32(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 16(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 24(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	16(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	24(%rsp), %rsi
	movq	32(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	248(%rsp), %rdi
	leaq	752(%rsp), %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 32(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 16(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 24(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	16(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	24(%rsp), %rsi
	movq	32(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	$1, 288(%rsp)
	pxor	%xmm0, %xmm0
	movdqu	%xmm0, 296(%rsp)
	movdqu	%xmm0, 312(%rsp)
	movups	208(%rsp), %xmm0
	movdqu	224(%rsp), %xmm1
	movaps	%xmm0, 336(%rsp)
	movdqa	%xmm1, 352(%rsp)
	movq	240(%rsp), %rax
	movq	%rax, 368(%rsp)
	movups	248(%rsp), %xmm0
	movups	%xmm0, 376(%rsp)
	movdqu	264(%rsp), %xmm0
	movdqu	%xmm0, 392(%rsp)
	movq	280(%rsp), %rax
	movq	%rax, 408(%rsp)
	xorl	%r15d, %r15d
	cmpl	$0, 328(%rsp)
	movl	44(%rsp), %ecx                  # 4-byte Reload
	movq	120(%rsp), %rdx                 # 8-byte Reload
	jne	.LBB10_33
# %bb.32:
	leaq	336(%rsp), %rdi
	movq	128(%rsp), %rsi                 # 8-byte Reload
	callq	secp256k1_eckey_pubkey_serialize
	movl	%eax, %r15d
	jmp	.LBB10_33
.LBB10_1:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.2(%rip), %r8
	movl	$204, %ecx
	jmp	.LBB10_2
.LBB10_4:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.3(%rip), %r8
	movl	$205, %ecx
	jmp	.LBB10_2
.LBB10_6:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.4(%rip), %r8
	movl	$206, %ecx
	jmp	.LBB10_2
.LBB10_8:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.11(%rip), %r8
	movl	$207, %ecx
	jmp	.LBB10_2
.LBB10_10:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.6(%rip), %r8
	movl	$208, %ecx
	jmp	.LBB10_2
.LBB10_12:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.12(%rip), %r8
	movl	$209, %ecx
	jmp	.LBB10_2
.LBB10_14:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.13(%rip), %r8
	movl	$210, %ecx
.LBB10_2:
	xorl	%eax, %eax
	callq	fprintf@PLT
	callq	abort@PLT
.Lfunc_end10:
	.size	secp256k1_ecdsa_recover_compact, .Lfunc_end10-secp256k1_ecdsa_recover_compact
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function secp256k1_eckey_pubkey_serialize
	.type	secp256k1_eckey_pubkey_serialize,@function
secp256k1_eckey_pubkey_serialize:       # @secp256k1_eckey_pubkey_serialize
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	xorl	%eax, %eax
	cmpl	$0, 80(%rdi)
	jne	.LBB11_9
# %bb.1:
	movl	%ecx, -20(%rsp)                 # 4-byte Spill
	movq	%rdx, -16(%rsp)                 # 8-byte Spill
	movabsq	$4503599627370495, %r11         # imm = 0xFFFFFFFFFFFFF
	movabsq	$4294968273, %r15               # imm = 0x1000003D1
	movq	32(%rdi), %rbx
	movq	%rbx, %rax
	shrq	$48, %rax
	imulq	%r15, %rax
	addq	(%rdi), %rax
	movabsq	$4503595332402222, %rcx         # imm = 0xFFFFEFFFFFC2E
	movq	%rax, %r8
	shrq	$52, %r8
	addq	8(%rdi), %r8
	movabsq	$281474976710655, %r14          # imm = 0xFFFFFFFFFFFF
	movq	%r8, %r9
	shrq	$52, %r9
	addq	16(%rdi), %r9
	andq	%r14, %rbx
	movq	%r9, %r10
	shrq	$52, %r10
	addq	24(%rdi), %r10
	andq	%r11, %rax
	andq	%r11, %r9
	movq	%r9, %rdx
	andq	%r8, %rdx
	andq	%r11, %r8
	movq	%r10, %r13
	shrq	$52, %r13
	addq	%rbx, %r13
	andq	%r10, %rdx
	andq	%r11, %r10
	movq	%r13, %rbx
	shrq	$48, %rbx
	movq	%r13, %rbp
	xorq	%r14, %rbp
	xorq	%r11, %rdx
	orq	%rbp, %rdx
	sete	%dl
	cmpq	%rcx, %rax
	seta	%cl
	andb	%dl, %cl
	movzbl	%cl, %ecx
	orq	%rbx, %rcx
	movq	%rsi, -8(%rsp)                  # 8-byte Spill
	je	.LBB11_3
# %bb.2:
	addq	%r15, %rax
	movq	%rax, %rcx
	shrq	$52, %rcx
	addq	%rcx, %r8
	andq	%r11, %rax
	movq	%r8, %rcx
	shrq	$52, %rcx
	addq	%rcx, %r9
	andq	%r11, %r8
	movq	%r9, %rcx
	shrq	$52, %rcx
	addq	%rcx, %r10
	andq	%r11, %r9
	movq	%r10, %rcx
	shrq	$52, %rcx
	addq	%r13, %rcx
	andq	%r11, %r10
	andq	%r14, %rcx
	movq	%rcx, %r13
.LBB11_3:                               # %secp256k1_fe_normalize_var.exit
	movq	%rax, (%rdi)
	movq	%r8, 8(%rdi)
	movq	%r9, 16(%rdi)
	movq	%r10, 24(%rdi)
	movq	%r13, 32(%rdi)
	movq	72(%rdi), %rdx
	movq	%rdx, %rbx
	shrq	$48, %rbx
	imulq	%r15, %rbx
	addq	40(%rdi), %rbx
	movq	%rbx, %r15
	shrq	$52, %r15
	addq	48(%rdi), %r15
	movq	%r15, %r12
	shrq	$52, %r12
	addq	56(%rdi), %r12
	andq	%r14, %rdx
	movq	%r12, %rbp
	shrq	$52, %rbp
	addq	64(%rdi), %rbp
	andq	%r11, %rbx
	andq	%r11, %r12
	movq	%r12, %rcx
	andq	%r15, %rcx
	andq	%r11, %r15
	movq	%r11, %rsi
	movq	%rbp, %r11
	shrq	$52, %r11
	addq	%rdx, %r11
	andq	%rbp, %rcx
	andq	%rsi, %rbp
	movq	%r11, %rdx
	shrq	$48, %rdx
	movq	%r14, %rsi
	movq	%r11, %r14
	xorq	%rsi, %r14
	movabsq	$4503599627370495, %rsi         # imm = 0xFFFFFFFFFFFFF
	xorq	%rsi, %rcx
	orq	%r14, %rcx
	sete	%r14b
	movabsq	$4503595332402222, %rcx         # imm = 0xFFFFEFFFFFC2E
	cmpq	%rcx, %rbx
	seta	%cl
	andb	%r14b, %cl
	movzbl	%cl, %ecx
	orq	%rdx, %rcx
	je	.LBB11_5
# %bb.4:
	movabsq	$4294968273, %rcx               # imm = 0x1000003D1
	addq	%rcx, %rbx
	movq	%rbx, %rcx
	shrq	$52, %rcx
	addq	%rcx, %r15
	andq	%rsi, %rbx
	movq	%r15, %rcx
	shrq	$52, %rcx
	addq	%rcx, %r12
	andq	%rsi, %r15
	movq	%r12, %rcx
	shrq	$52, %rcx
	addq	%rcx, %rbp
	andq	%rsi, %r12
	movq	%rbp, %rcx
	shrq	$52, %rcx
	addq	%r11, %rcx
	andq	%rsi, %rbp
	movabsq	$281474976710655, %rdx          # imm = 0xFFFFFFFFFFFF
	andq	%rdx, %rcx
	movq	%rcx, %r11
.LBB11_5:                               # %secp256k1_fe_normalize_var.exit22
	movq	%rbx, 40(%rdi)
	movq	%r15, 48(%rdi)
	movq	%r12, 56(%rdi)
	movq	%rbp, 64(%rdi)
	shlq	$16, %r13
	movq	%r10, %rcx
	shrq	$36, %rcx
	orq	%r13, %rcx
	bswapq	%rcx
	movq	%r11, 72(%rdi)
	movq	-8(%rsp), %rsi                  # 8-byte Reload
	movq	%rcx, 1(%rsi)
	shlq	$28, %r10
	movq	%r9, %rcx
	shrq	$24, %rcx
	orq	%r10, %rcx
	bswapq	%rcx
	shlq	$40, %r9
	movq	%r8, %rdx
	shrq	$12, %rdx
	orq	%r9, %rdx
	bswapq	%rdx
	movq	%rcx, 9(%rsi)
	movq	%rdx, 17(%rsi)
	shlq	$52, %r8
	orq	%r8, %rax
	bswapq	%rax
	movq	%rax, 25(%rsi)
	cmpl	$0, -20(%rsp)                   # 4-byte Folded Reload
	je	.LBB11_7
# %bb.6:
	movq	-16(%rsp), %rax                 # 8-byte Reload
	movl	$33, (%rax)
	movb	40(%rdi), %al
	andb	$1, %al
	orb	$2, %al
	movb	%al, (%rsi)
	jmp	.LBB11_8
.LBB11_7:
	movq	-16(%rsp), %rax                 # 8-byte Reload
	movl	$65, (%rax)
	movb	$4, (%rsi)
	movq	72(%rdi), %rcx
	movq	64(%rdi), %rbp
	movq	48(%rdi), %rax
	movq	56(%rdi), %rbx
	shlq	$16, %rcx
	movq	%rbp, %rdx
	shrq	$36, %rdx
	orq	%rcx, %rdx
	bswapq	%rdx
	movq	%rax, %rcx
	shlq	$52, %rcx
	orq	40(%rdi), %rcx
	shlq	$28, %rbp
	movq	%rbx, %rdi
	shrq	$24, %rdi
	orq	%rbp, %rdi
	bswapq	%rdi
	movq	%rdx, 33(%rsi)
	movq	%rdi, 41(%rsi)
	shlq	$40, %rbx
	shrq	$12, %rax
	orq	%rbx, %rax
	bswapq	%rax
	movq	%rax, 49(%rsi)
	bswapq	%rcx
	movq	%rcx, 57(%rsi)
.LBB11_8:
	movl	$1, %eax
.LBB11_9:
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end11:
	.size	secp256k1_eckey_pubkey_serialize, .Lfunc_end11-secp256k1_eckey_pubkey_serialize
	.cfi_endproc
                                        # -- End function
	.globl	secp256k1_ec_seckey_verify      # -- Begin function secp256k1_ec_seckey_verify
	.p2align	4, 0x90
	.type	secp256k1_ec_seckey_verify,@function
secp256k1_ec_seckey_verify:             # @secp256k1_ec_seckey_verify
	.cfi_startproc
# %bb.0:
	subq	$56, %rsp
	.cfi_def_cfa_offset 64
	testq	%rdi, %rdi
	je	.LBB12_1
# %bb.3:
	testq	%rsi, %rsi
	je	.LBB12_4
# %bb.5:
	leaq	16(%rsp), %rdi
	leaq	12(%rsp), %rdx
	callq	secp256k1_scalar_set_b32
	movdqa	16(%rsp), %xmm0
	por	32(%rsp), %xmm0
	pxor	%xmm1, %xmm1
	pcmpeqb	%xmm0, %xmm1
	pmovmskb	%xmm1, %eax
	cmpl	$65535, %eax                    # imm = 0xFFFF
	setne	%al
	cmpl	$0, 12(%rsp)
	sete	%cl
	andb	%al, %cl
	movzbl	%cl, %eax
	addq	$56, %rsp
	.cfi_def_cfa_offset 8
	retq
.LBB12_1:
	.cfi_def_cfa_offset 64
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.2(%rip), %r8
	movl	$230, %ecx
	jmp	.LBB12_2
.LBB12_4:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.10(%rip), %r8
	movl	$231, %ecx
.LBB12_2:
	xorl	%eax, %eax
	callq	fprintf@PLT
	callq	abort@PLT
.Lfunc_end12:
	.size	secp256k1_ec_seckey_verify, .Lfunc_end12-secp256k1_ec_seckey_verify
	.cfi_endproc
                                        # -- End function
	.globl	secp256k1_ec_pubkey_verify      # -- Begin function secp256k1_ec_pubkey_verify
	.p2align	4, 0x90
	.type	secp256k1_ec_pubkey_verify,@function
secp256k1_ec_pubkey_verify:             # @secp256k1_ec_pubkey_verify
	.cfi_startproc
# %bb.0:
	subq	$88, %rsp
	.cfi_def_cfa_offset 96
	testq	%rdi, %rdi
	je	.LBB13_1
# %bb.3:
	testq	%rsi, %rsi
	je	.LBB13_4
# %bb.5:
	movq	%rsp, %rdi
	callq	secp256k1_eckey_pubkey_parse
	addq	$88, %rsp
	.cfi_def_cfa_offset 8
	retq
.LBB13_1:
	.cfi_def_cfa_offset 96
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.2(%rip), %r8
	movl	$242, %ecx
	jmp	.LBB13_2
.LBB13_4:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.6(%rip), %r8
	movl	$243, %ecx
.LBB13_2:
	xorl	%eax, %eax
	callq	fprintf@PLT
	callq	abort@PLT
.Lfunc_end13:
	.size	secp256k1_ec_pubkey_verify, .Lfunc_end13-secp256k1_ec_pubkey_verify
	.cfi_endproc
                                        # -- End function
	.globl	secp256k1_ec_pubkey_create      # -- Begin function secp256k1_ec_pubkey_create
	.p2align	4, 0x90
	.type	secp256k1_ec_pubkey_create,@function
secp256k1_ec_pubkey_create:             # @secp256k1_ec_pubkey_create
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$376, %rsp                      # imm = 0x178
	.cfi_def_cfa_offset 432
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	testq	%rdi, %rdi
	je	.LBB14_1
# %bb.3:
	movq	%rdi, %rbx
	cmpq	$0, 8(%rdi)
	je	.LBB14_4
# %bb.5:
	movq	%rsi, %r15
	testq	%rsi, %rsi
	je	.LBB14_6
# %bb.7:
	movq	%rdx, %rbp
	testq	%rdx, %rdx
	je	.LBB14_8
# %bb.9:
	testq	%rcx, %rcx
	je	.LBB14_10
# %bb.11:
	movl	%r8d, %r14d
	leaq	48(%rsp), %rdi
	leaq	28(%rsp), %rdx
	movq	%rcx, %rsi
	callq	secp256k1_scalar_set_b32
	cmpl	$0, 28(%rsp)
	je	.LBB14_12
.LBB14_13:                              # %.thread
	movl	$0, (%rbp)
	xorl	%eax, %eax
	jmp	.LBB14_14
.LBB14_12:
	addq	$8, %rbx
	leaq	80(%rsp), %rsi
	leaq	48(%rsp), %rdx
	movq	%rbx, %rdi
	callq	secp256k1_ecmult_gen
	xorps	%xmm0, %xmm0
	movaps	%xmm0, 64(%rsp)
	movaps	%xmm0, 48(%rsp)
	movl	200(%rsp), %eax
	movl	%eax, 288(%rsp)
	leaq	160(%rsp), %rbx
	movq	%rbx, %rdi
	movq	%rbx, %rsi
	callq	secp256k1_fe_inv_var
	leaq	336(%rsp), %rdi
	movq	%rbx, %rsi
	movl	%r14d, 24(%rsp)                 # 4-byte Spill
	movq	%r15, 40(%rsp)                  # 8-byte Spill
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 296(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 8(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 16(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	8(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rsp), %rsi
	movq	296(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	296(%rsp), %rdi
	leaq	336(%rsp), %rbx
	leaq	160(%rsp), %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 16(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 32(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 8(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	32(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rsp), %rsi
	movq	16(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	336(%rsp), %rbx
	leaq	80(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 16(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 32(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 8(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	32(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rsp), %rsi
	movq	16(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	120(%rsp), %rdi
	leaq	296(%rsp), %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 16(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 32(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 8(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	32(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rsp), %rsi
	movq	16(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	$1, 160(%rsp)
	xorps	%xmm0, %xmm0
	movups	%xmm0, 168(%rsp)
	movups	%xmm0, 184(%rsp)
	movups	80(%rsp), %xmm0
	movups	96(%rsp), %xmm1
	movaps	%xmm0, 208(%rsp)
	movaps	%xmm1, 224(%rsp)
	movq	112(%rsp), %rax
	movq	%rax, 240(%rsp)
	movups	120(%rsp), %xmm0
	movups	%xmm0, 248(%rsp)
	movups	136(%rsp), %xmm0
	movups	%xmm0, 264(%rsp)
	movq	152(%rsp), %rax
	movq	%rax, 280(%rsp)
	leaq	208(%rsp), %rdi
	movq	40(%rsp), %rsi                  # 8-byte Reload
	movq	%rbp, %rdx
	movl	24(%rsp), %ecx                  # 4-byte Reload
	callq	secp256k1_eckey_pubkey_serialize
	testl	%eax, %eax
	je	.LBB14_13
.LBB14_14:
	addq	$376, %rsp                      # imm = 0x178
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB14_1:
	.cfi_def_cfa_offset 432
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.2(%rip), %r8
	movl	$255, %ecx
	jmp	.LBB14_2
.LBB14_4:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.7(%rip), %r8
	movl	$256, %ecx                      # imm = 0x100
	jmp	.LBB14_2
.LBB14_6:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.6(%rip), %r8
	movl	$257, %ecx                      # imm = 0x101
	jmp	.LBB14_2
.LBB14_8:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.12(%rip), %r8
	movl	$258, %ecx                      # imm = 0x102
	jmp	.LBB14_2
.LBB14_10:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.10(%rip), %r8
	movl	$259, %ecx                      # imm = 0x103
.LBB14_2:
	xorl	%eax, %eax
	callq	fprintf@PLT
	callq	abort@PLT
.Lfunc_end14:
	.size	secp256k1_ec_pubkey_create, .Lfunc_end14-secp256k1_ec_pubkey_create
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function secp256k1_ecmult_gen
	.type	secp256k1_ecmult_gen,@function
secp256k1_ecmult_gen:                   # @secp256k1_ecmult_gen
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$168, %rsp
	.cfi_def_cfa_offset 224
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movups	152(%rdi), %xmm0
	movups	%xmm0, 112(%rsi)
	movups	136(%rdi), %xmm0
	movups	%xmm0, 96(%rsi)
	movups	120(%rdi), %xmm0
	movups	%xmm0, 80(%rsi)
	movups	104(%rdi), %xmm0
	movups	%xmm0, 64(%rsi)
	movups	40(%rdi), %xmm0
	movups	56(%rdi), %xmm1
	movups	72(%rdi), %xmm2
	movups	88(%rdi), %xmm3
	movups	%xmm3, 48(%rsi)
	movups	%xmm2, 32(%rsi)
	movups	%xmm1, 16(%rsi)
	movq	%rsi, 16(%rsp)                  # 8-byte Spill
	movups	%xmm0, (%rsi)
	movq	8(%rdi), %r9
	addq	(%rdx), %r9
	movq	8(%rdx), %r11
	adcq	$0, %r11
	setb	%bl
	addq	16(%rdi), %r11
	movzbl	%bl, %esi
	adcq	16(%rdx), %rsi
	movq	32(%rdi), %rbp
	setb	%bl
	movzbl	%bl, %ebx
	xorl	%r14d, %r14d
	addq	24(%rdx), %rbp
	setb	%r14b
	movq	%rdi, 8(%rsp)                   # 8-byte Spill
	addq	24(%rdi), %rsi
	adcq	%rbx, %rbp
	setb	%r8b
	cmpq	$-1, %rbp
	setne	%bl
	cmpq	$-2, %rsi
	setb	%dl
	orb	%bl, %dl
	movzbl	%dl, %eax
	xorl	%ebx, %ebx
	cmpq	$-1, %rsi
	sete	%bl
	movl	%eax, %edx
	notl	%edx
	andl	%ebx, %edx
	movabsq	$-4994812053365940165, %r10     # imm = 0xBAAEDCE6AF48A03B
	xorl	%ebx, %ebx
	cmpq	%r10, %r11
	setb	%cl
	seta	%bl
	orb	%cl, %al
	movzbl	%al, %eax
	notl	%eax
	andl	%eax, %ebx
	orl	%edx, %ebx
	movabsq	$-4624529908474429120, %rcx     # imm = 0xBFD25E8CD0364140
	xorl	%edx, %edx
	cmpq	%rcx, %r9
	seta	%dl
	andl	%eax, %edx
	orl	%ebx, %edx
	addb	$255, %r8b
	adcl	%edx, %r14d
	movabsq	$4624529908474429119, %rax      # imm = 0x402DA1732FC9BEBF
	imulq	%r14, %rax
	movabsq	$4994812053365940164, %rcx      # imm = 0x4551231950B75FC4
	imulq	%r14, %rcx
	addq	%r9, %rax
	movq	%rax, 48(%rsp)
	adcq	%r11, %rcx
	movq	%rcx, 56(%rsp)
	adcq	%r14, %rsi
	adcq	$0, %rbp
	movq	%rsi, 64(%rsp)
	movq	%rbp, 72(%rsp)
	xorl	%edx, %edx
	xorl	%r15d, %r15d
	xorl	%r13d, %r13d
	xorl	%r14d, %r14d
	xorl	%r12d, %r12d
	xorl	%ebp, %ebp
	xorl	%r8d, %r8d
	xorl	%ebx, %ebx
	xorl	%esi, %esi
	jmp	.LBB15_1
	.p2align	4, 0x90
.LBB15_5:                               #   in Loop: Header=BB15_1 Depth=1
	movabsq	$4503599610593280, %rdi         # imm = 0xFFFFFFF000000
	leaq	16777215(%rdi), %rax
	movq	%rsi, %rcx
	andq	%rax, %rcx
	movq	%rcx, 80(%rsp)
	movq	%rsi, %rdx
	shrq	$52, %rdx
	movq	%rsi, 24(%rsp)                  # 8-byte Spill
	movq	%rbx, %rsi
	shlq	$12, %rsi
	leaq	16773120(%rdi), %rcx
	andq	%rcx, %rsi
	orq	%rdx, %rsi
	movq	%rsi, 88(%rsp)
	movq	%rbx, %rdx
	shrq	$40, %rdx
	movq	%r8, %rsi
	shlq	$24, %rsi
	andq	%rdi, %rsi
	orq	%rdx, %rsi
	movq	%rsi, 96(%rsp)
	movq	%r8, %rdx
	shrq	$28, %rdx
	movq	%rbp, %rsi
	shlq	$36, %rsi
	movq	%rbx, 32(%rsp)                  # 8-byte Spill
	movq	%r8, %rbx
	movabsq	$4503530907893760, %r8          # imm = 0xFFFF000000000
	andq	%r8, %rsi
	orq	%rdx, %rsi
	movq	%rsi, 104(%rsp)
	movq	%rbp, %rdx
	shrq	$16, %rdx
	movq	%rdx, 112(%rsp)
	andq	%r12, %rax
	movq	%rax, 120(%rsp)
	movq	%r12, %rax
	shrq	$52, %rax
	movq	%r14, %rdx
	shlq	$12, %rdx
	andq	%rcx, %rdx
	orq	%rax, %rdx
	movq	%rdx, 128(%rsp)
	movq	%r14, %rax
	shrq	$40, %rax
	movq	%r13, %rcx
	shlq	$24, %rcx
	andq	%rdi, %rcx
	orq	%rax, %rcx
	movq	%rcx, 136(%rsp)
	movq	%r13, %rax
	shrq	$28, %rax
	movq	%r15, %rcx
	shlq	$36, %rcx
	andq	%r8, %rcx
	orq	%rax, %rcx
	movq	%rcx, 144(%rsp)
	movq	%r15, %rax
	shrq	$16, %rax
	movq	%rax, 152(%rsp)
	movl	$0, 160(%rsp)
	movq	16(%rsp), %rdi                  # 8-byte Reload
	movq	%rdi, %rsi
	leaq	80(%rsp), %rdx
	callq	secp256k1_gej_add_ge
	movq	24(%rsp), %rsi                  # 8-byte Reload
	movq	%rbx, %r8
	movq	32(%rsp), %rbx                  # 8-byte Reload
	movq	40(%rsp), %rdx                  # 8-byte Reload
	addq	$1, %rdx
	cmpq	$64, %rdx
	je	.LBB15_6
.LBB15_1:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB15_2 Depth 2
	movq	%rdx, %rax
	shrq	%rax
	andl	$2147483640, %eax               # imm = 0x7FFFFFF8
	movq	48(%rsp,%rax), %rax
	leal	(,%rdx,4), %ecx
                                        # kill: def $cl killed $cl killed $ecx
	shrq	%cl, %rax
	andl	$15, %eax
	movq	8(%rsp), %rcx                   # 8-byte Reload
	movq	(%rcx), %rcx
	shlq	$6, %rax
	movq	%rdx, 40(%rsp)                  # 8-byte Spill
	shlq	$10, %rdx
	addq	%rdx, %rcx
	addq	$56, %rcx
	xorl	%edx, %edx
	jmp	.LBB15_2
	.p2align	4, 0x90
.LBB15_4:                               #   in Loop: Header=BB15_2 Depth=2
	addq	$64, %rdx
	cmpq	$1024, %rdx                     # imm = 0x400
	je	.LBB15_5
.LBB15_2:                               #   Parent Loop BB15_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	%rdx, %rax
	jne	.LBB15_4
# %bb.3:                                #   in Loop: Header=BB15_2 Depth=2
	movq	-56(%rcx,%rdx), %rsi
	movq	-48(%rcx,%rdx), %rbx
	movq	-40(%rcx,%rdx), %r8
	movq	-32(%rcx,%rdx), %rbp
	movq	-24(%rcx,%rdx), %r12
	movq	-16(%rcx,%rdx), %r14
	movq	-8(%rcx,%rdx), %r13
	movq	(%rcx,%rdx), %r15
	jmp	.LBB15_4
.LBB15_6:
	addq	$168, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end15:
	.size	secp256k1_ecmult_gen, .Lfunc_end15-secp256k1_ecmult_gen
	.cfi_endproc
                                        # -- End function
	.globl	secp256k1_ec_pubkey_decompress  # -- Begin function secp256k1_ec_pubkey_decompress
	.p2align	4, 0x90
	.type	secp256k1_ec_pubkey_decompress,@function
secp256k1_ec_pubkey_decompress:         # @secp256k1_ec_pubkey_decompress
	.cfi_startproc
# %bb.0:
	pushq	%r14
	.cfi_def_cfa_offset 16
	pushq	%rbx
	.cfi_def_cfa_offset 24
	subq	$88, %rsp
	.cfi_def_cfa_offset 112
	.cfi_offset %rbx, -24
	.cfi_offset %r14, -16
	testq	%rsi, %rsi
	je	.LBB16_1
# %bb.3:
	movq	%rdx, %r14
	testq	%rdx, %rdx
	je	.LBB16_4
# %bb.5:
	movq	%rsi, %rbx
	movl	(%r14), %edx
	movq	%rsp, %rdi
	callq	secp256k1_eckey_pubkey_parse
	testl	%eax, %eax
	je	.LBB16_6
# %bb.7:
	movq	%rsp, %rdi
	movq	%rbx, %rsi
	movq	%r14, %rdx
	xorl	%ecx, %ecx
	callq	secp256k1_eckey_pubkey_serialize
	addq	$88, %rsp
	.cfi_def_cfa_offset 24
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%r14
	.cfi_def_cfa_offset 8
	retq
.LBB16_6:
	.cfi_def_cfa_offset 112
	xorl	%eax, %eax
	addq	$88, %rsp
	.cfi_def_cfa_offset 24
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%r14
	.cfi_def_cfa_offset 8
	retq
.LBB16_1:
	.cfi_def_cfa_offset 112
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.6(%rip), %r8
	movl	$277, %ecx                      # imm = 0x115
	jmp	.LBB16_2
.LBB16_4:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.12(%rip), %r8
	movl	$278, %ecx                      # imm = 0x116
.LBB16_2:
	xorl	%eax, %eax
	callq	fprintf@PLT
	callq	abort@PLT
.Lfunc_end16:
	.size	secp256k1_ec_pubkey_decompress, .Lfunc_end16-secp256k1_ec_pubkey_decompress
	.cfi_endproc
                                        # -- End function
	.globl	secp256k1_ec_privkey_tweak_add  # -- Begin function secp256k1_ec_privkey_tweak_add
	.p2align	4, 0x90
	.type	secp256k1_ec_privkey_tweak_add,@function
secp256k1_ec_privkey_tweak_add:         # @secp256k1_ec_privkey_tweak_add
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r14
	.cfi_def_cfa_offset 24
	pushq	%rbx
	.cfi_def_cfa_offset 32
	subq	$80, %rsp
	.cfi_def_cfa_offset 112
	.cfi_offset %rbx, -32
	.cfi_offset %r14, -24
	.cfi_offset %rbp, -16
	movl	$0, 12(%rsp)
	testq	%rdi, %rdi
	je	.LBB17_1
# %bb.3:
	movq	%rsi, %rbp
	testq	%rsi, %rsi
	je	.LBB17_4
# %bb.5:
	testq	%rdx, %rdx
	je	.LBB17_6
# %bb.7:
	leaq	48(%rsp), %rdi
	leaq	12(%rsp), %rax
	movq	%rdx, %rsi
	movq	%rax, %rdx
	callq	secp256k1_scalar_set_b32
	leaq	16(%rsp), %rdi
	movq	%rbp, %rsi
	xorl	%edx, %edx
	callq	secp256k1_scalar_set_b32
	movq	48(%rsp), %r11
	addq	16(%rsp), %r11
	movq	24(%rsp), %r14
	adcq	$0, %r14
	setb	%al
	addq	56(%rsp), %r14
	movzbl	%al, %eax
	adcq	32(%rsp), %rax
	setb	%cl
	addq	64(%rsp), %rax
	movzbl	%cl, %edx
	adcq	40(%rsp), %rdx
	movq	72(%rsp), %r8
	setb	%r9b
	leaq	(%rdx,%r8), %rcx
	cmpq	$-1, %rcx
	setne	%r10b
	cmpq	$-2, %rax
	setb	%cl
	orb	%r10b, %cl
	movzbl	%cl, %edi
	xorl	%esi, %esi
	cmpq	$-1, %rax
	sete	%sil
	movl	%edi, %ecx
	notl	%ecx
	andl	%esi, %ecx
	movabsq	$-4994812053365940165, %r10     # imm = 0xBAAEDCE6AF48A03B
	xorl	%esi, %esi
	cmpq	%r10, %r14
	seta	%sil
	setb	%bl
	orb	%bl, %dil
	movzbl	%dil, %edi
	notl	%edi
	andl	%edi, %esi
	orl	%ecx, %esi
	movabsq	$-4624529908474429120, %rcx     # imm = 0xBFD25E8CD0364140
	xorl	%ebx, %ebx
	cmpq	%rcx, %r11
	seta	%bl
	andl	%edi, %ebx
	orl	%esi, %ebx
	movq	%rdx, %rcx
	addq	%r8, %rcx
	movq	%rax, 32(%rsp)
	movzbl	%r9b, %esi
	adcl	%ebx, %esi
	movabsq	$4624529908474429119, %rdi      # imm = 0x402DA1732FC9BEBF
	imulq	%rsi, %rdi
	movabsq	$4994812053365940164, %rcx      # imm = 0x4551231950B75FC4
	imulq	%rsi, %rcx
	addq	%r11, %rdi
	movq	%rdi, 16(%rsp)
	adcq	%r14, %rcx
	adcq	%rsi, %rax
	adcq	%r8, %rdx
	movq	%rcx, %rsi
	orq	%rdi, %rsi
	orq	%rax, %rsi
	orq	%rdx, %rsi
	setne	%bl
	cmpl	$0, 12(%rsp)
	movq	%rcx, 24(%rsp)
	sete	%sil
	andb	%bl, %sil
	cmpb	$1, %sil
	jne	.LBB17_9
# %bb.8:
	movq	%rdi, %r8
	movq	%rdx, %rdi
	shrq	$56, %rdi
	movb	%dil, (%rbp)
	movq	%rdx, %rdi
	shrq	$48, %rdi
	movb	%dil, 1(%rbp)
	movq	%rdx, %rdi
	shrq	$40, %rdi
	movb	%dil, 2(%rbp)
	movq	%rdx, %rdi
	shrq	$32, %rdi
	movb	%dil, 3(%rbp)
	movq	%rdx, %rdi
	shrq	$24, %rdi
	movb	%dil, 4(%rbp)
	movq	%rdx, %rdi
	shrq	$16, %rdi
	movb	%dil, 5(%rbp)
	movb	%dh, 6(%rbp)
	movb	%dl, 7(%rbp)
	movq	%rax, %rdx
	shrq	$56, %rdx
	movb	%dl, 8(%rbp)
	movq	%rax, %rdx
	shrq	$48, %rdx
	movb	%dl, 9(%rbp)
	movq	%rax, %rdx
	shrq	$40, %rdx
	movb	%dl, 10(%rbp)
	movq	%rax, %rdx
	shrq	$32, %rdx
	movb	%dl, 11(%rbp)
	movq	%rax, %rdx
	shrq	$24, %rdx
	movb	%dl, 12(%rbp)
	movq	%rax, %rdx
	shrq	$16, %rdx
	movb	%dl, 13(%rbp)
	movb	%ah, 14(%rbp)
	movb	%al, 15(%rbp)
	movq	%rcx, %rax
	shrq	$56, %rax
	movb	%al, 16(%rbp)
	movq	%rcx, %rax
	shrq	$48, %rax
	movb	%al, 17(%rbp)
	movq	%rcx, %rax
	shrq	$40, %rax
	movb	%al, 18(%rbp)
	movq	%rcx, %rax
	shrq	$32, %rax
	movb	%al, 19(%rbp)
	movq	%rcx, %rax
	shrq	$24, %rax
	movb	%al, 20(%rbp)
	movq	%rcx, %rax
	shrq	$16, %rax
	movb	%al, 21(%rbp)
	movb	%ch, 22(%rbp)
	movb	%cl, 23(%rbp)
	movq	%r8, %rcx
	movq	%r8, %rax
	shrq	$56, %rax
	movb	%al, 24(%rbp)
	movq	%r8, %rax
	shrq	$48, %rax
	movb	%al, 25(%rbp)
	movq	%r8, %rax
	shrq	$40, %rax
	movb	%al, 26(%rbp)
	movq	%r8, %rax
	shrq	$32, %rax
	movb	%al, 27(%rbp)
	movq	%r8, %rax
	shrq	$24, %rax
	movb	%al, 28(%rbp)
	movq	%r8, %rax
	shrq	$16, %rax
	movb	%al, 29(%rbp)
	movb	%ch, 30(%rbp)
	movb	%cl, 31(%rbp)
.LBB17_9:
	movzbl	%sil, %eax
	addq	$80, %rsp
	.cfi_def_cfa_offset 32
	popq	%rbx
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB17_1:
	.cfi_def_cfa_offset 112
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.2(%rip), %r8
	movl	$292, %ecx                      # imm = 0x124
	jmp	.LBB17_2
.LBB17_4:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.10(%rip), %r8
	movl	$293, %ecx                      # imm = 0x125
	jmp	.LBB17_2
.LBB17_6:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.14(%rip), %r8
	movl	$294, %ecx                      # imm = 0x126
.LBB17_2:
	xorl	%eax, %eax
	callq	fprintf@PLT
	callq	abort@PLT
.Lfunc_end17:
	.size	secp256k1_ec_privkey_tweak_add, .Lfunc_end17-secp256k1_ec_privkey_tweak_add
	.cfi_endproc
                                        # -- End function
	.globl	secp256k1_ec_pubkey_tweak_add   # -- Begin function secp256k1_ec_pubkey_tweak_add
	.p2align	4, 0x90
	.type	secp256k1_ec_pubkey_tweak_add,@function
secp256k1_ec_pubkey_tweak_add:          # @secp256k1_ec_pubkey_tweak_add
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$424, %rsp                      # imm = 0x1A8
	.cfi_def_cfa_offset 480
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	%edx, 28(%rsp)
	movl	$0, 4(%rsp)
	testq	%rdi, %rdi
	je	.LBB18_1
# %bb.3:
	movq	%rdi, %rbx
	cmpq	$0, (%rdi)
	je	.LBB18_4
# %bb.5:
	movq	%rsi, %r14
	testq	%rsi, %rsi
	je	.LBB18_6
# %bb.7:
	testq	%rcx, %rcx
	je	.LBB18_8
# %bb.9:
	movl	%edx, %r15d
	leaq	392(%rsp), %rdi
	leaq	4(%rsp), %rdx
	movq	%rcx, %rsi
	callq	secp256k1_scalar_set_b32
	xorl	%ebp, %ebp
	cmpl	$0, 4(%rsp)
	je	.LBB18_10
.LBB18_13:                              # %.critedge
	movl	%ebp, %eax
	addq	$424, %rsp                      # imm = 0x1A8
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB18_10:
	.cfi_def_cfa_offset 480
	leaq	48(%rsp), %rdi
	movq	%r14, %rsi
	movl	%r15d, %edx
	callq	secp256k1_eckey_pubkey_parse
	testl	%eax, %eax
	je	.LBB18_13
# %bb.11:
	movl	128(%rsp), %eax
	movl	%eax, 264(%rsp)
	movups	48(%rsp), %xmm0
	movups	64(%rsp), %xmm1
	movaps	%xmm0, 144(%rsp)
	movaps	%xmm1, 160(%rsp)
	movq	80(%rsp), %rax
	movq	%rax, 176(%rsp)
	movups	88(%rsp), %xmm0
	movups	%xmm0, 184(%rsp)
	movups	104(%rsp), %xmm0
	movups	%xmm0, 200(%rsp)
	movq	120(%rsp), %rax
	movq	%rax, 216(%rsp)
	movq	$1, 224(%rsp)
	xorps	%xmm0, %xmm0
	movups	%xmm0, 232(%rsp)
	movups	%xmm0, 248(%rsp)
	movq	$1, 280(%rsp)
	movups	%xmm0, 288(%rsp)
	movq	$0, 304(%rsp)
	leaq	144(%rsp), %rdx
	leaq	280(%rsp), %rcx
	leaq	392(%rsp), %r8
	movq	%rbx, %rdi
	movq	%rdx, %rsi
	callq	secp256k1_ecmult
	cmpl	$0, 264(%rsp)
	jne	.LBB18_13
# %bb.12:
	movl	$0, 128(%rsp)
	leaq	224(%rsp), %rbp
	movq	%rbp, %rdi
	movq	%rbp, %rsi
	callq	secp256k1_fe_inv_var
	leaq	352(%rsp), %rdi
	movq	%rbp, %rsi
	movq	%r14, 40(%rsp)                  # 8-byte Spill
	movl	%r15d, 24(%rsp)                 # 4-byte Spill
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 312(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 8(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 16(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	8(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rsp), %rsi
	movq	312(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	312(%rsp), %rdi
	leaq	352(%rsp), %rbx
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 16(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 32(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 8(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	32(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rsp), %rsi
	movq	16(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	352(%rsp), %rbx
	leaq	144(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 16(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 32(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 8(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	32(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rsp), %rsi
	movq	16(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	312(%rsp), %rbx
	leaq	184(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 16(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 32(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 8(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	32(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rsp), %rsi
	movq	16(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	$1, 224(%rsp)
	xorps	%xmm0, %xmm0
	movups	%xmm0, 64(%rdi)
	movups	%xmm0, 48(%rdi)
	movq	176(%rsp), %rax
	movq	%rax, 80(%rsp)
	movaps	144(%rsp), %xmm0
	movaps	160(%rsp), %xmm1
	movaps	%xmm1, 64(%rsp)
	movaps	%xmm0, 48(%rsp)
	movq	32(%rdi), %rax
	leaq	88(%rsp), %rcx
	movq	%rax, 32(%rcx)
	movups	(%rdi), %xmm0
	movups	16(%rdi), %xmm1
	movups	%xmm1, 16(%rcx)
	movups	%xmm0, (%rcx)
	xorl	%ecx, %ecx
	cmpl	$34, 24(%rsp)                   # 4-byte Folded Reload
	setl	%cl
	leaq	48(%rsp), %rdi
	leaq	28(%rsp), %rdx
	movq	40(%rsp), %rsi                  # 8-byte Reload
	callq	secp256k1_eckey_pubkey_serialize
	movl	%eax, %ebp
	jmp	.LBB18_13
.LBB18_1:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.2(%rip), %r8
	movl	$315, %ecx                      # imm = 0x13B
	jmp	.LBB18_2
.LBB18_4:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.3(%rip), %r8
	movl	$316, %ecx                      # imm = 0x13C
	jmp	.LBB18_2
.LBB18_6:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.6(%rip), %r8
	movl	$317, %ecx                      # imm = 0x13D
	jmp	.LBB18_2
.LBB18_8:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.14(%rip), %r8
	movl	$318, %ecx                      # imm = 0x13E
.LBB18_2:
	xorl	%eax, %eax
	callq	fprintf@PLT
	callq	abort@PLT
.Lfunc_end18:
	.size	secp256k1_ec_pubkey_tweak_add, .Lfunc_end18-secp256k1_ec_pubkey_tweak_add
	.cfi_endproc
                                        # -- End function
	.globl	secp256k1_ec_privkey_tweak_mul  # -- Begin function secp256k1_ec_privkey_tweak_mul
	.p2align	4, 0x90
	.type	secp256k1_ec_privkey_tweak_mul,@function
secp256k1_ec_privkey_tweak_mul:         # @secp256k1_ec_privkey_tweak_mul
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$152, %rsp
	.cfi_def_cfa_offset 208
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	$0, 12(%rsp)
	testq	%rdi, %rdi
	je	.LBB19_1
# %bb.3:
	movq	%rsi, %rbp
	testq	%rsi, %rsi
	je	.LBB19_4
# %bb.5:
	testq	%rdx, %rdx
	je	.LBB19_6
# %bb.7:
	leaq	48(%rsp), %r14
	leaq	12(%rsp), %rax
	movq	%r14, %rdi
	movq	%rdx, %rsi
	movq	%rax, %rdx
	callq	secp256k1_scalar_set_b32
	xorl	%ebx, %ebx
	leaq	16(%rsp), %rdi
	movq	%rbp, %rsi
	xorl	%edx, %edx
	callq	secp256k1_scalar_set_b32
	movdqa	48(%rsp), %xmm0
	por	64(%rsp), %xmm0
	pxor	%xmm1, %xmm1
	pcmpeqb	%xmm0, %xmm1
	pmovmskb	%xmm1, %eax
	cmpl	$65535, %eax                    # imm = 0xFFFF
	je	.LBB19_10
# %bb.8:                                # %secp256k1_eckey_privkey_tweak_mul.exit
	leaq	80(%rsp), %rsi
	leaq	16(%rsp), %rdi
	movq	%r14, %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	xorl	%ebx, %ebx
	callq	secp256k1_scalar_reduce_512
	cmpl	$0, 12(%rsp)
	jne	.LBB19_10
# %bb.9:
	movq	40(%rsp), %rax
	bswapq	%rax
	movq	%rax, (%rbp)
	movq	32(%rsp), %rax
	bswapq	%rax
	movq	%rax, 8(%rbp)
	movq	24(%rsp), %rax
	bswapq	%rax
	movq	%rax, 16(%rbp)
	movq	16(%rsp), %rax
	bswapq	%rax
	movq	%rax, 24(%rbp)
	movl	$1, %ebx
.LBB19_10:                              # %secp256k1_eckey_privkey_tweak_mul.exit.thread
	movl	%ebx, %eax
	addq	$152, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB19_1:
	.cfi_def_cfa_offset 208
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.2(%rip), %r8
	movl	$341, %ecx                      # imm = 0x155
	jmp	.LBB19_2
.LBB19_4:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.10(%rip), %r8
	movl	$342, %ecx                      # imm = 0x156
	jmp	.LBB19_2
.LBB19_6:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.14(%rip), %r8
	movl	$343, %ecx                      # imm = 0x157
.LBB19_2:
	xorl	%eax, %eax
	callq	fprintf@PLT
	callq	abort@PLT
.Lfunc_end19:
	.size	secp256k1_ec_privkey_tweak_mul, .Lfunc_end19-secp256k1_ec_privkey_tweak_mul
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function secp256k1_ec_pubkey_tweak_mul
.LCPI20_0:
	.zero	16
	.text
	.globl	secp256k1_ec_pubkey_tweak_mul
	.p2align	4, 0x90
	.type	secp256k1_ec_pubkey_tweak_mul,@function
secp256k1_ec_pubkey_tweak_mul:          # @secp256k1_ec_pubkey_tweak_mul
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$424, %rsp                      # imm = 0x1A8
	.cfi_def_cfa_offset 480
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	%edx, 28(%rsp)
	movl	$0, 4(%rsp)
	testq	%rdi, %rdi
	je	.LBB20_1
# %bb.3:
	movq	%rdi, %rbx
	cmpq	$0, (%rdi)
	je	.LBB20_4
# %bb.5:
	movq	%rsi, %r14
	testq	%rsi, %rsi
	je	.LBB20_6
# %bb.7:
	testq	%rcx, %rcx
	je	.LBB20_8
# %bb.9:
	movl	%edx, %r15d
	leaq	272(%rsp), %rdi
	leaq	4(%rsp), %rdx
	movq	%rcx, %rsi
	callq	secp256k1_scalar_set_b32
	xorl	%ebp, %ebp
	cmpl	$0, 4(%rsp)
	je	.LBB20_10
.LBB20_13:                              # %.critedge
	movl	%ebp, %eax
	addq	$424, %rsp                      # imm = 0x1A8
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB20_10:
	.cfi_def_cfa_offset 480
	leaq	48(%rsp), %rdi
	movq	%r14, %rsi
	movl	%r15d, %edx
	callq	secp256k1_eckey_pubkey_parse
	testl	%eax, %eax
	je	.LBB20_13
# %bb.11:
	movdqa	272(%rsp), %xmm0
	por	288(%rsp), %xmm0
	pxor	%xmm1, %xmm1
	pcmpeqb	%xmm1, %xmm0
	pmovmskb	%xmm0, %eax
	cmpl	$65535, %eax                    # imm = 0xFFFF
	je	.LBB20_13
# %bb.12:
	movdqa	%xmm1, 320(%rsp)
	movdqa	%xmm1, 304(%rsp)
	movl	128(%rsp), %eax
	movl	%eax, 264(%rsp)
	movaps	48(%rsp), %xmm0
	movaps	64(%rsp), %xmm1
	movaps	%xmm0, 144(%rsp)
	movaps	%xmm1, 160(%rsp)
	movq	80(%rsp), %rax
	movq	%rax, 176(%rsp)
	movups	88(%rsp), %xmm0
	movups	%xmm0, 184(%rsp)
	movups	104(%rsp), %xmm0
	movups	%xmm0, 200(%rsp)
	movq	120(%rsp), %rax
	movq	%rax, 216(%rsp)
	leaq	224(%rsp), %rbp
	movq	$1, 224(%rsp)
	xorps	%xmm0, %xmm0
	movups	%xmm0, 248(%rsp)
	movups	%xmm0, 232(%rsp)
	leaq	144(%rsp), %rdx
	leaq	272(%rsp), %rcx
	leaq	304(%rsp), %r8
	movq	%rbx, %rdi
	movq	%rdx, %rsi
	callq	secp256k1_ecmult
	movl	264(%rsp), %eax
	movl	%eax, 128(%rsp)
	movq	%rbp, %rbx
	movq	%rbp, %rdi
	movq	%rbp, %rsi
	callq	secp256k1_fe_inv_var
	leaq	384(%rsp), %rbp
	movq	%rbp, %rdi
	movq	%rbx, %rsi
	movq	%r14, 40(%rsp)                  # 8-byte Spill
	movl	%r15d, 24(%rsp)                 # 4-byte Spill
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 344(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 8(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 16(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	8(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rsp), %rsi
	movq	344(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	344(%rsp), %rdi
	movq	%rbp, %rbx
	leaq	224(%rsp), %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 16(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 32(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 8(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	32(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rsp), %rsi
	movq	16(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	144(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 16(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 32(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 8(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	32(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rsp), %rsi
	movq	16(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	344(%rsp), %rbx
	leaq	184(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 16(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 32(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 8(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	32(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rsp), %rsi
	movq	16(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	xorps	%xmm0, %xmm0
	movups	%xmm0, 232(%rsp)
	movaps	144(%rsp), %xmm0
	movdqa	160(%rsp), %xmm1
	movaps	%xmm0, 48(%rsp)
	movdqa	%xmm1, 64(%rsp)
	movq	176(%rsp), %rax
	movq	%rax, 80(%rsp)
	movups	184(%rsp), %xmm0
	movups	%xmm0, 88(%rsp)
	movdqu	200(%rsp), %xmm0
	movdqu	%xmm0, 104(%rsp)
	movq	216(%rsp), %rax
	movq	%rax, 120(%rsp)
	xorl	%ecx, %ecx
	cmpl	$34, 24(%rsp)                   # 4-byte Folded Reload
	setl	%cl
	leaq	48(%rsp), %rdi
	leaq	28(%rsp), %rdx
	movq	40(%rsp), %rsi                  # 8-byte Reload
	callq	secp256k1_eckey_pubkey_serialize
	movl	%eax, %ebp
	jmp	.LBB20_13
.LBB20_1:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.2(%rip), %r8
	movl	$363, %ecx                      # imm = 0x16B
	jmp	.LBB20_2
.LBB20_4:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.3(%rip), %r8
	movl	$364, %ecx                      # imm = 0x16C
	jmp	.LBB20_2
.LBB20_6:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.6(%rip), %r8
	movl	$365, %ecx                      # imm = 0x16D
	jmp	.LBB20_2
.LBB20_8:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.14(%rip), %r8
	movl	$366, %ecx                      # imm = 0x16E
.LBB20_2:
	xorl	%eax, %eax
	callq	fprintf@PLT
	callq	abort@PLT
.Lfunc_end20:
	.size	secp256k1_ec_pubkey_tweak_mul, .Lfunc_end20-secp256k1_ec_pubkey_tweak_mul
	.cfi_endproc
                                        # -- End function
	.globl	secp256k1_ec_privkey_export     # -- Begin function secp256k1_ec_privkey_export
	.p2align	4, 0x90
	.type	secp256k1_ec_privkey_export,@function
secp256k1_ec_privkey_export:            # @secp256k1_ec_privkey_export
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$392, %rsp                      # imm = 0x188
	.cfi_def_cfa_offset 448
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	testq	%rsi, %rsi
	je	.LBB21_1
# %bb.3:
	testq	%rdx, %rdx
	je	.LBB21_4
# %bb.5:
	testq	%rcx, %rcx
	je	.LBB21_6
# %bb.7:
	movq	%rdi, %rbx
	testq	%rdi, %rdi
	je	.LBB21_8
# %bb.9:
	movl	%r8d, 36(%rsp)                  # 4-byte Spill
	movq	%rdx, 40(%rsp)                  # 8-byte Spill
	movq	%rcx, 88(%rsp)                  # 8-byte Spill
	cmpq	$0, 8(%rbx)
	je	.LBB21_10
# %bb.11:
	addq	$8, %rbx
	leaq	56(%rsp), %r14
	movq	%r14, %rdi
	xorl	%edx, %edx
	callq	secp256k1_scalar_set_b32
	movl	$0, 12(%rsp)
	leaq	96(%rsp), %rsi
	movq	%rbx, %rdi
	movq	%r14, %rdx
	callq	secp256k1_ecmult_gen
	movl	216(%rsp), %eax
	movl	%eax, 304(%rsp)
	leaq	176(%rsp), %rbp
	movq	%rbp, %rdi
	movq	%rbp, %rsi
	callq	secp256k1_fe_inv_var
	leaq	352(%rsp), %rdi
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 312(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 16(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 24(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	16(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	24(%rsp), %rsi
	movq	312(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	312(%rsp), %rdi
	leaq	352(%rsp), %rbx
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 24(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 48(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 16(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	48(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rsp), %rsi
	movq	24(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	352(%rsp), %rbx
	leaq	96(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 24(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 48(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 16(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	48(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rsp), %rsi
	movq	24(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	136(%rsp), %rdi
	leaq	312(%rsp), %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 24(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 48(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 16(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	48(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rsp), %rsi
	movq	24(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	$1, 176(%rsp)
	xorps	%xmm0, %xmm0
	movups	%xmm0, 184(%rsp)
	movups	%xmm0, 200(%rsp)
	movups	96(%rsp), %xmm0
	movups	112(%rsp), %xmm1
	movaps	%xmm0, 224(%rsp)
	movaps	%xmm1, 240(%rsp)
	movq	128(%rsp), %rax
	movq	%rax, 256(%rsp)
	movups	136(%rsp), %xmm0
	movups	%xmm0, 264(%rsp)
	movups	152(%rsp), %xmm0
	movups	%xmm0, 280(%rsp)
	movq	168(%rsp), %rax
	movq	%rax, 296(%rsp)
	cmpl	$0, 36(%rsp)                    # 4-byte Folded Reload
	je	.LBB21_13
# %bb.12:
	movabsq	$2306970012974547248, %rax      # imm = 0x2004010102D38130
	movq	40(%rsp), %rbx                  # 8-byte Reload
	movq	%rax, (%rbx)
	movq	80(%rsp), %rax
	bswapq	%rax
	movq	%rax, 8(%rbx)
	movq	72(%rsp), %rax
	bswapq	%rax
	movq	%rax, 16(%rbx)
	movq	64(%rsp), %rax
	bswapq	%rax
	movq	%rax, 24(%rbx)
	movq	56(%rsp), %rax
	bswapq	%rax
	movq	%rax, 32(%rbx)
	leaq	40(%rbx), %rdi
	leaq	secp256k1_eckey_privkey_serialize.middle(%rip), %rsi
	movl	$141, %edx
	callq	memcpy@PLT
	leaq	181(%rbx), %rsi
	leaq	224(%rsp), %rdi
	leaq	12(%rsp), %rdx
	movl	$1, %ecx
	callq	secp256k1_eckey_pubkey_serialize
	movl	$181, %ecx
	testl	%eax, %eax
	movl	$0, %ebp
	je	.LBB21_15
.LBB21_14:                              # %.critedge.sink.split.i
	addl	%ebx, %ecx
	addl	12(%rsp), %ecx
	subl	%ebx, %ecx
	movq	88(%rsp), %rax                  # 8-byte Reload
	movl	%ecx, (%rax)
	movl	$1, %ebp
.LBB21_15:                              # %secp256k1_eckey_privkey_serialize.exit
	movl	%ebp, %eax
	addq	$392, %rsp                      # imm = 0x188
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB21_13:
	.cfi_def_cfa_offset 448
	movabsq	$288512959548850736, %rax       # imm = 0x401010213018230
	movq	40(%rsp), %rbx                  # 8-byte Reload
	movq	%rax, (%rbx)
	movb	$32, 8(%rbx)
	movq	80(%rsp), %rax
	bswapq	%rax
	movq	%rax, 9(%rbx)
	movq	72(%rsp), %rax
	bswapq	%rax
	movq	%rax, 17(%rbx)
	movq	64(%rsp), %rax
	bswapq	%rax
	movq	%rax, 25(%rbx)
	movq	56(%rsp), %rax
	bswapq	%rax
	movq	%rax, 33(%rbx)
	leaq	41(%rbx), %rdi
	leaq	secp256k1_eckey_privkey_serialize.middle.79(%rip), %rsi
	movl	$173, %edx
	callq	memcpy@PLT
	leaq	214(%rbx), %rsi
	leaq	224(%rsp), %rdi
	leaq	12(%rsp), %rdx
	xorl	%ebp, %ebp
	xorl	%ecx, %ecx
	callq	secp256k1_eckey_pubkey_serialize
	movl	$214, %ecx
	testl	%eax, %eax
	jne	.LBB21_14
	jmp	.LBB21_15
.LBB21_1:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.10(%rip), %r8
	movl	$387, %ecx                      # imm = 0x183
	jmp	.LBB21_2
.LBB21_4:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.15(%rip), %r8
	movl	$388, %ecx                      # imm = 0x184
	jmp	.LBB21_2
.LBB21_6:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.16(%rip), %r8
	movl	$389, %ecx                      # imm = 0x185
	jmp	.LBB21_2
.LBB21_8:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.2(%rip), %r8
	movl	$390, %ecx                      # imm = 0x186
	jmp	.LBB21_2
.LBB21_10:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.7(%rip), %r8
	movl	$391, %ecx                      # imm = 0x187
.LBB21_2:
	xorl	%eax, %eax
	callq	fprintf@PLT
	callq	abort@PLT
.Lfunc_end21:
	.size	secp256k1_ec_privkey_export, .Lfunc_end21-secp256k1_ec_privkey_export
	.cfi_endproc
                                        # -- End function
	.globl	secp256k1_ec_privkey_import     # -- Begin function secp256k1_ec_privkey_import
	.p2align	4, 0x90
	.type	secp256k1_ec_privkey_import,@function
secp256k1_ec_privkey_import:            # @secp256k1_ec_privkey_import
	.cfi_startproc
# %bb.0:
	pushq	%r14
	.cfi_def_cfa_offset 16
	pushq	%rbx
	.cfi_def_cfa_offset 24
	subq	$88, %rsp
	.cfi_def_cfa_offset 112
	.cfi_offset %rbx, -24
	.cfi_offset %r14, -16
	testq	%rsi, %rsi
	je	.LBB22_1
# %bb.3:
	testq	%rdx, %rdx
	je	.LBB22_4
# %bb.5:
	xorps	%xmm0, %xmm0
	movaps	%xmm0, 64(%rsp)
	movaps	%xmm0, 48(%rsp)
	movl	$0, 12(%rsp)
	testl	%ecx, %ecx
	jle	.LBB22_22
# %bb.6:
	cmpl	$1, %ecx
	je	.LBB22_22
# %bb.7:
	cmpb	$48, (%rdx)
	jne	.LBB22_22
# %bb.8:
	movq	%rsi, %rbx
	movzbl	1(%rdx), %esi
	testb	$-128, %sil
	je	.LBB22_22
# %bb.9:
	andl	$127, %esi
	leal	-3(%rsi), %eax
	cmpl	$-2, %eax
	jb	.LBB22_22
# %bb.10:
	movslq	%ecx, %rax
	addq	%rdx, %rax
	addq	$2, %rdx
	movq	%rsi, %rcx
	addq	%rdx, %rcx
	cmpq	%rcx, %rax
	jb	.LBB22_22
# %bb.11:
	leal	-1(%rsi), %edi
	movzbl	(%rdx,%rdi), %r8d
	xorl	%edi, %edi
	cmpl	$2, %esi
	jb	.LBB22_13
# %bb.12:
	movzbl	(%rdx), %edi
	shll	$8, %edi
.LBB22_13:
	movl	%edi, %edx
	orq	%r8, %rdx
	addq	%rcx, %rdx
	cmpq	%rdx, %rax
	jb	.LBB22_22
# %bb.14:
	leaq	3(%rcx), %rdx
	cmpq	%rdx, %rax
	jb	.LBB22_22
# %bb.15:
	cmpb	$2, (%rcx)
	jne	.LBB22_22
# %bb.16:
	cmpb	$1, 1(%rcx)
	jne	.LBB22_22
# %bb.17:
	cmpb	$1, 2(%rcx)
	jne	.LBB22_22
# %bb.18:
	leaq	2(%rdx), %rsi
	cmpq	%rsi, %rax
	jb	.LBB22_22
# %bb.19:
	cmpb	$4, (%rdx)
	jne	.LBB22_22
# %bb.20:
	movzbl	1(%rdx), %edx
	cmpq	$32, %rdx
	ja	.LBB22_22
# %bb.21:
	leaq	(%rsi,%rdx), %rcx
	cmpq	%rcx, %rax
	jae	.LBB22_23
.LBB22_22:                              # %secp256k1_eckey_privkey_parse.exit.thread
	xorl	%eax, %eax
.LBB22_25:
	addq	$88, %rsp
	.cfi_def_cfa_offset 24
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%r14
	.cfi_def_cfa_offset 8
	retq
.LBB22_23:                              # %secp256k1_eckey_privkey_parse.exit
	.cfi_def_cfa_offset 112
	leaq	48(%rsp), %r14
	movq	%r14, %rdi
	subq	%rdx, %rdi
	addq	$32, %rdi
	callq	memcpy@PLT
	leaq	16(%rsp), %rdi
	leaq	12(%rsp), %rdx
	movq	%r14, %rsi
	callq	secp256k1_scalar_set_b32
	xorl	%eax, %eax
	cmpl	$0, 12(%rsp)
	jne	.LBB22_25
# %bb.24:
	movq	40(%rsp), %rax
	bswapq	%rax
	movq	%rax, (%rbx)
	movq	32(%rsp), %rax
	bswapq	%rax
	movq	%rax, 8(%rbx)
	movq	24(%rsp), %rax
	bswapq	%rax
	movq	%rax, 16(%rbx)
	movq	16(%rsp), %rax
	bswapq	%rax
	movq	%rax, 24(%rbx)
	movl	$1, %eax
	addq	$88, %rsp
	.cfi_def_cfa_offset 24
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%r14
	.cfi_def_cfa_offset 8
	retq
.LBB22_1:
	.cfi_def_cfa_offset 112
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.10(%rip), %r8
	movl	$402, %ecx                      # imm = 0x192
	jmp	.LBB22_2
.LBB22_4:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.15(%rip), %r8
	movl	$403, %ecx                      # imm = 0x193
.LBB22_2:
	xorl	%eax, %eax
	callq	fprintf@PLT
	callq	abort@PLT
.Lfunc_end22:
	.size	secp256k1_ec_privkey_import, .Lfunc_end22-secp256k1_ec_privkey_import
	.cfi_endproc
                                        # -- End function
	.globl	secp256k1_context_randomize     # -- Begin function secp256k1_context_randomize
	.p2align	4, 0x90
	.type	secp256k1_context_randomize,@function
secp256k1_context_randomize:            # @secp256k1_context_randomize
	.cfi_startproc
# %bb.0:
	pushq	%rax
	.cfi_def_cfa_offset 16
	testq	%rdi, %rdi
	je	.LBB23_1
# %bb.3:
	cmpq	$0, 8(%rdi)
	je	.LBB23_4
# %bb.5:
	addq	$8, %rdi
	callq	secp256k1_ecmult_gen_blind
	movl	$1, %eax
	popq	%rcx
	.cfi_def_cfa_offset 8
	retq
.LBB23_1:
	.cfi_def_cfa_offset 16
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.2(%rip), %r8
	movl	$415, %ecx                      # imm = 0x19F
	jmp	.LBB23_2
.LBB23_4:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.1(%rip), %rdx
	leaq	.L.str.7(%rip), %r8
	movl	$416, %ecx                      # imm = 0x1A0
.LBB23_2:
	xorl	%eax, %eax
	callq	fprintf@PLT
	callq	abort@PLT
.Lfunc_end23:
	.size	secp256k1_context_randomize, .Lfunc_end23-secp256k1_context_randomize
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function secp256k1_ecmult_gen_blind
.LCPI24_0:
	.quad	15814739681549316               # 0x382f6c04ef1c04
	.quad	16736317891044920               # 0x3b7597aabe6638
.LCPI24_1:
	.quad	14055019942963272               # 0x31eef75702e848
	.quad	14559364240130108               # 0x33b9aa25b0403c
.LCPI24_2:
	.quad	1046482296297817                # 0x3b7c52588d959
	.quad	1                               # 0x1
	.text
	.p2align	4, 0x90
	.type	secp256k1_ecmult_gen_blind,@function
secp256k1_ecmult_gen_blind:             # @secp256k1_ecmult_gen_blind
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$360, %rsp                      # imm = 0x168
	.cfi_def_cfa_offset 416
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rdi, %rbp
	testq	%rsi, %rsi
	je	.LBB24_2
# %bb.1:                                # %._crit_edge
	movq	32(%rbp), %rdx
	movq	16(%rbp), %rax
	movq	24(%rbp), %rcx
	jmp	.LBB24_3
.LBB24_2:
	movq	%rsp, %rsi
	movq	secp256k1_ge_const_g+32(%rip), %rax
	movq	%rax, 72(%rbp)
	movups	secp256k1_ge_const_g+16(%rip), %xmm0
	movups	%xmm0, 56(%rbp)
	movups	secp256k1_ge_const_g(%rip), %xmm0
	movups	%xmm0, 40(%rbp)
	xorps	%xmm0, %xmm0
	movups	%xmm0, 128(%rbp)
	movups	%xmm0, 144(%rbp)
	movl	$0, 160(%rbp)
	movaps	.LCPI24_0(%rip), %xmm1          # xmm1 = [15814739681549316,16736317891044920]
	movups	%xmm1, 80(%rbp)
	movaps	.LCPI24_1(%rip), %xmm1          # xmm1 = [14055019942963272,14559364240130108]
	movups	%xmm1, 96(%rbp)
	movaps	.LCPI24_2(%rip), %xmm1          # xmm1 = [1046482296297817,1]
	movups	%xmm1, 112(%rbp)
	movq	$1, 8(%rbp)
	movups	%xmm0, 16(%rbp)
	movq	$0, 32(%rbp)
	xorl	%eax, %eax
	xorl	%ecx, %ecx
	xorl	%edx, %edx
.LBB24_3:
	movabsq	$4503599627370495, %r12         # imm = 0xFFFFFFFFFFFFF
	leaq	8(%rbp), %rdi
	movq	%rdi, 152(%rsp)                 # 8-byte Spill
	movq	%rdx, %rdi
	shrq	$56, %rdi
	movb	%dil, (%rsp)
	movq	%rdx, %rdi
	shrq	$48, %rdi
	movb	%dil, 1(%rsp)
	movq	%rdx, %rdi
	shrq	$40, %rdi
	movb	%dil, 2(%rsp)
	movq	%rdx, %rdi
	shrq	$32, %rdi
	movb	%dil, 3(%rsp)
	movq	%rdx, %rdi
	shrq	$24, %rdi
	movb	%dil, 4(%rsp)
	movq	%rdx, %rdi
	shrq	$16, %rdi
	movb	%dil, 5(%rsp)
	movb	%dh, 6(%rsp)
	movb	%dl, 7(%rsp)
	movq	%rcx, %rdx
	shrq	$56, %rdx
	movb	%dl, 8(%rsp)
	movq	%rcx, %rdx
	shrq	$48, %rdx
	movb	%dl, 9(%rsp)
	movq	%rcx, %rdx
	shrq	$40, %rdx
	movb	%dl, 10(%rsp)
	movq	%rcx, %rdx
	shrq	$32, %rdx
	movb	%dl, 11(%rsp)
	movq	%rcx, %rdx
	shrq	$24, %rdx
	movb	%dl, 12(%rsp)
	movq	%rcx, %rdx
	shrq	$16, %rdx
	movb	%dl, 13(%rsp)
	movb	%ch, 14(%rsp)
	movb	%cl, 15(%rsp)
	movq	%rax, %rcx
	shrq	$56, %rcx
	movb	%cl, 16(%rsp)
	movq	%rax, %rcx
	shrq	$48, %rcx
	movb	%cl, 17(%rsp)
	movq	%rax, %rcx
	shrq	$40, %rcx
	movb	%cl, 18(%rsp)
	movq	%rax, %rcx
	shrq	$32, %rcx
	movb	%cl, 19(%rsp)
	movq	%rax, %rcx
	shrq	$24, %rcx
	movb	%cl, 20(%rsp)
	movq	%rax, %rcx
	shrq	$16, %rcx
	movb	%cl, 21(%rsp)
	movb	%ah, 22(%rsp)
	movb	%al, 23(%rsp)
	movq	%rbp, 88(%rsp)                  # 8-byte Spill
	movq	8(%rbp), %rax
	bswapq	%rax
	movq	%rax, 24(%rsp)
	leaq	288(%rsp), %rdi
	movq	%rsp, %rdx
	xorl	%ecx, %ecx
	xorl	%r8d, %r8d
	callq	secp256k1_rfc6979_hmac_sha256_initialize
	leaq	-4095(%r12), %r14
	leaq	-16777215(%r12), %r15
	movabsq	$281474976710655, %r13          # imm = 0xFFFFFFFFFFFF
	jmp	.LBB24_4
	.p2align	4, 0x90
.LBB24_7:                               #   in Loop: Header=BB24_4 Depth=1
	xorl	%ebp, %ebp
.LBB24_8:                               # %secp256k1_fe_set_b32.exit
                                        #   in Loop: Header=BB24_4 Depth=1
	orq	%rcx, %rdi
	orq	%rdx, %rdi
	orq	%rax, %rdi
	xorl	%eax, %eax
	orq	%rbx, %rdi
	sete	%al
	orl	%ebp, %eax
	movl	%eax, 36(%rsp)
	je	.LBB24_9
.LBB24_4:                               # =>This Inner Loop Header: Depth=1
	leaq	288(%rsp), %rdi
	movq	%rsp, %rsi
	callq	secp256k1_rfc6979_hmac_sha256_generate
	movq	(%rsp), %rcx
	movq	8(%rsp), %rax
	bswapq	%rcx
	bswapq	%rax
	movq	16(%rsp), %rsi
	bswapq	%rsi
	movq	24(%rsp), %rdi
	bswapq	%rdi
	movq	%rdi, %rdx
	andq	%r12, %rdx
	movq	%rdx, 96(%rsp)
	shrq	$52, %rdi
	movq	%rsi, %rbx
	shlq	$12, %rbx
	andq	%r14, %rbx
	orq	%rdi, %rbx
	movq	%rbx, 104(%rsp)
	shrq	$40, %rsi
	movq	%rcx, %rdi
	shldq	$36, %rax, %rdi
	shlq	$24, %rax
	andq	%r15, %rax
	orq	%rsi, %rax
	movq	%rax, 112(%rsp)
	andq	%r12, %rdi
	movq	%rdi, 120(%rsp)
	shrq	$16, %rcx
	movq	%rcx, 128(%rsp)
	cmpq	%r13, %rcx
	jne	.LBB24_7
# %bb.5:                                #   in Loop: Header=BB24_4 Depth=1
	movq	%rax, %rsi
	andq	%rdi, %rsi
	andq	%rbx, %rsi
	cmpq	%r12, %rsi
	jne	.LBB24_7
# %bb.6:                                #   in Loop: Header=BB24_4 Depth=1
	movl	$1, %ebp
	movabsq	$4503595332402222, %rsi         # imm = 0xFFFFEFFFFFC2E
	cmpq	%rsi, %rdx
	ja	.LBB24_8
	jmp	.LBB24_7
.LBB24_9:
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	40(%rax), %rbp
	leaq	160(%rsp), %rdi
	leaq	96(%rsp), %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 56(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 40(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 48(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	40(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	48(%rsp), %rsi
	movq	56(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	160(%rsp), %rbx
	movq	%rbp, %rdi
	movq	%rbp, 144(%rsp)                 # 8-byte Spill
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 56(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 40(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 48(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	40(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	48(%rsp), %rsi
	movq	56(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	88(%rsp), %rbp                  # 8-byte Reload
	leaq	80(%rbp), %rdi
	leaq	160(%rsp), %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 56(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 40(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 48(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	40(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	48(%rsp), %rsi
	movq	56(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	96(%rsp), %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 56(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 40(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 48(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	40(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	48(%rsp), %rsi
	movq	56(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	120(%rbp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 56(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 40(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 48(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	40(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	48(%rsp), %rsi
	movq	56(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	xorps	%xmm0, %xmm0
	movaps	%xmm0, 96(%rsp)
	movaps	%xmm0, 112(%rsp)
	movq	$0, 128(%rsp)
	movq	%rsp, %r13
	.p2align	4, 0x90
.LBB24_10:                              # =>This Inner Loop Header: Depth=1
	leaq	288(%rsp), %rdi
	movq	%r13, %rsi
	callq	secp256k1_rfc6979_hmac_sha256_generate
	leaq	56(%rsp), %rdi
	movq	%r13, %rsi
	leaq	36(%rsp), %rdx
	callq	secp256k1_scalar_set_b32
	movq	56(%rsp), %r15
	movq	64(%rsp), %r12
	movq	%r12, %r14
	orq	%r15, %r14
	movq	72(%rsp), %rbp
	orq	%rbp, %r14
	movq	80(%rsp), %rbx
	xorl	%eax, %eax
	orq	%rbx, %r14
	sete	%al
	orl	%eax, 36(%rsp)
	jne	.LBB24_10
# %bb.11:
	xorps	%xmm0, %xmm0
	movaps	%xmm0, 16(%rsp)
	movaps	%xmm0, (%rsp)
	leaq	160(%rsp), %rsi
	leaq	56(%rsp), %rdx
	movq	88(%rsp), %rdi                  # 8-byte Reload
	callq	secp256k1_ecmult_gen
	negq	%r14
	sbbq	%rax, %rax
	notq	%r15
	movabsq	$-4624529908474429118, %rcx     # imm = 0xBFD25E8CD0364142
	addq	%r15, %rcx
	notq	%r12
	adcq	$0, %r12
	setb	%dl
	andq	%rax, %rcx
	movq	%rcx, 56(%rsp)
	movzbl	%dl, %ecx
	movabsq	$-4994812053365940165, %rdx     # imm = 0xBAAEDCE6AF48A03B
	addq	%r12, %rdx
	notq	%rbp
	adcq	%rcx, %rbp
	setb	%cl
	andq	%rax, %rdx
	movq	%rdx, 64(%rsp)
	movzbl	%cl, %ecx
	addq	$-2, %rbp
	notq	%rbx
	adcq	%rcx, %rbx
	andq	%rax, %rbp
	movq	%rbp, 72(%rsp)
	addq	$-1, %rbx
	andq	%rax, %rbx
	movq	%rbx, 80(%rsp)
	movq	56(%rsp), %rax
	movq	152(%rsp), %rcx                 # 8-byte Reload
	movq	%rax, (%rcx)
	movq	64(%rsp), %rax
	movq	%rax, 8(%rcx)
	movq	72(%rsp), %rax
	movq	%rax, 16(%rcx)
	movq	80(%rsp), %rax
	movq	%rax, 24(%rcx)
	movups	160(%rsp), %xmm0
	movups	176(%rsp), %xmm1
	movups	192(%rsp), %xmm2
	movups	208(%rsp), %xmm3
	movq	144(%rsp), %rax                 # 8-byte Reload
	movups	%xmm0, (%rax)
	movups	%xmm1, 16(%rax)
	movups	%xmm2, 32(%rax)
	movups	%xmm3, 48(%rax)
	movups	224(%rsp), %xmm0
	movups	%xmm0, 64(%rax)
	movups	240(%rsp), %xmm0
	movups	%xmm0, 80(%rax)
	movups	256(%rsp), %xmm0
	movups	%xmm0, 96(%rax)
	movups	272(%rsp), %xmm0
	movups	%xmm0, 112(%rax)
	addq	$360, %rsp                      # imm = 0x168
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end24:
	.size	secp256k1_ecmult_gen_blind, .Lfunc_end24-secp256k1_ecmult_gen_blind
	.cfi_endproc
                                        # -- End function
	.globl	hash160                         # -- Begin function hash160
	.p2align	4, 0x90
	.type	hash160,@function
hash160:                                # @hash160
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$88, %rsp
	.cfi_def_cfa_offset 144
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rsi, %r9
	movq	%rdi, 80(%rsp)                  # 8-byte Spill
	movl	(%rsi), %eax
	movl	4(%rsi), %edx
	bswapl	%eax
	movq	%rax, %rcx
	bswapl	%edx
	movq	%rdx, -56(%rsp)                 # 8-byte Spill
	movl	8(%rsi), %eax
	bswapl	%eax
	movl	%eax, %ebx
	movl	%eax, -72(%rsp)                 # 4-byte Spill
	leal	-1731730782(%rcx), %edi
	movl	%edi, %eax
	roll	$26, %eax
	leal	-66549683(%rcx), %r11d
	movq	%rcx, %r15
	movl	%edi, %ecx
	roll	$21, %ecx
	xorl	%eax, %ecx
	movl	%edi, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movl	%edi, %r14d
	andl	$-905233677, %r14d              # imm = 0xCA0B3AF3
	xorl	$-1694144372, %r14d             # imm = 0x9B05688C
	addl	%edx, %r14d
	movl	%r11d, %ecx
	roll	$30, %ecx
	movl	%r11d, %edx
	roll	$19, %edx
	movl	%r11d, %ebp
	roll	$10, %ebp
	xorl	%ecx, %edx
	xorl	%edx, %ebp
	movl	%r11d, %ecx
	andl	$-781301534, %ecx               # imm = 0xD16E48E2
	addl	%ebp, %ecx
	leal	(%rax,%r14), %r12d
	addl	$-852880978, %r12d              # imm = 0xCD2A11AE
                                        # kill: def $r14d killed $r14d killed $r14 def $r14
	addl	%eax, %r14d
	movl	%r12d, %eax
	roll	$26, %eax
	leal	(%r14,%rcx), %r8d
	addl	$-1162034111, %r8d              # imm = 0xBABCC441
	movl	%r12d, %ecx
	roll	$21, %ecx
	xorl	%eax, %ecx
	movl	%r12d, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movl	%edi, %ecx
	xorl	$1359893119, %ecx               # imm = 0x510E527F
	andl	%r12d, %ecx
	xorl	$1359893119, %ecx               # imm = 0x510E527F
	movl	%r8d, %edx
	roll	$30, %edx
	addl	%ebx, %ecx
	movl	%r8d, %ebp
	roll	$19, %ebp
	xorl	%edx, %ebp
	movl	%r8d, %edx
	roll	$10, %edx
	xorl	%ebp, %edx
	movl	%r8d, %ebp
	andl	%r11d, %ebp
	movl	%r8d, %ebx
	orl	%r11d, %ebx
	andl	$1779033703, %ebx               # imm = 0x6A09E667
	orl	%ebp, %ebx
	leal	(%rcx,%rax), %ebp
	movq	%rbp, -88(%rsp)                 # 8-byte Spill
	addl	%edx, %ebx
	leal	(%rax,%rcx), %r13d
	addl	$204346080, %r13d               # imm = 0xC2E12E0
	movl	%r13d, %ecx
	roll	$26, %ecx
	leal	(%rbx,%rbp), %r10d
	addl	$1355179099, %r10d              # imm = 0x50C6645B
	movl	%r13d, %edx
	roll	$21, %edx
	xorl	%ecx, %edx
	movl	%r13d, %ecx
	roll	$7, %ecx
	xorl	%edx, %ecx
	movl	%r12d, %edx
	xorl	%edi, %edx
	andl	%r13d, %edx
	xorl	%edi, %edx
	movl	%r10d, %edi
	roll	$30, %edi
	movl	%r10d, %ebx
	roll	$19, %ebx
	movl	%r10d, %eax
	roll	$10, %eax
	xorl	%edi, %ebx
	xorl	%ebx, %eax
	movl	%r10d, %ebp
	orl	%r8d, %ebp
	andl	%r11d, %ebp
	movl	%r10d, %edi
	andl	%r8d, %edi
	orl	%edi, %ebp
	addl	%eax, %ebp
	movl	12(%rsi), %eax
	bswapl	%eax
	movl	%eax, -120(%rsp)                # 4-byte Spill
	addl	%eax, %edx
	leal	(%rdx,%rcx), %r11d
	leal	(%rcx,%rdx), %edi
	addl	$-1529998197, %edi              # imm = 0xA4CE148B
	movl	%edi, %eax
	roll	$26, %eax
	movl	%edi, %ecx
	roll	$21, %ecx
	xorl	%eax, %ecx
	movl	%edi, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movl	%r13d, %ecx
	xorl	%r12d, %ecx
	andl	%edi, %ecx
	xorl	%r12d, %ecx
	movl	16(%rsi), %edx
	bswapl	%edx
	movq	%rdx, -104(%rsp)                # 8-byte Spill
	movq	%r15, %r12
	movq	%r15, -96(%rsp)                 # 8-byte Spill
	addl	%r15d, %edx
	addl	%edx, %ecx
	leal	(%rcx,%rax), %esi
	addl	%ecx, %eax
	addl	$-769743619, %eax               # imm = 0xD21EA4FD
	leal	(%r11,%rbp), %ebx
	addl	$985935396, %ebx                # imm = 0x3AC42E24
	movl	%ebx, %ecx
	roll	$30, %ecx
	movl	%ebx, %edx
	roll	$19, %edx
	movl	%ebx, %ebp
	roll	$10, %ebp
	xorl	%ecx, %edx
	xorl	%edx, %ebp
	movl	%ebx, %ecx
	andl	%r10d, %ecx
	movl	%ebx, %r15d
	orl	%r10d, %r15d
	andl	%r8d, %r15d
	orl	%ecx, %r15d
	addl	%ebp, %r15d
	leal	(%r12,%rsi), %edx
	addl	$-836293302, %edx               # imm = 0xCE272D4A
	movl	%edx, %r12d
	roll	$26, %r12d
	movl	%edx, %ecx
	roll	$21, %ecx
	addl	%eax, %r15d
	xorl	%r12d, %ecx
	movl	%edx, %r12d
	roll	$7, %r12d
	xorl	%ecx, %r12d
	movl	%edi, %ecx
	xorl	%r13d, %ecx
	andl	%edx, %ecx
	xorl	%r13d, %ecx
	movl	20(%r9), %eax
	bswapl	%eax
	movq	%rax, -112(%rsp)                # 8-byte Spill
	addl	%eax, %r14d
	addl	$-852880978, %r14d              # imm = 0xCD2A11AE
	addl	%ecx, %r14d
	movl	%r15d, %ecx
	roll	$30, %ecx
	movl	%r15d, %eax
	roll	$19, %eax
	addl	%r12d, %r14d
	addl	$1508970993, %r14d              # imm = 0x59F111F1
	xorl	%ecx, %eax
	movl	%r15d, %r12d
	roll	$10, %r12d
	xorl	%eax, %r12d
	movl	%r15d, %eax
	andl	%ebx, %eax
	movl	%r15d, %ecx
	orl	%ebx, %ecx
	andl	%r10d, %ecx
	orl	%eax, %ecx
	addl	%r12d, %ecx
	addl	%r14d, %r8d
	addl	%r14d, %ecx
	movl	%r8d, %r14d
	roll	$26, %r14d
	movl	%r8d, %eax
	roll	$21, %eax
	xorl	%r14d, %eax
	movl	%r8d, %r14d
	roll	$7, %r14d
	xorl	%eax, %r14d
	movl	%edx, %eax
	xorl	%edi, %eax
	andl	%r8d, %eax
	xorl	%edi, %eax
	movl	24(%r9), %edi
	movq	%r9, %rsi
	bswapl	%edi
	movq	%rdi, -128(%rsp)                # 8-byte Spill
	movq	-88(%rsp), %rbp                 # 8-byte Reload
	addl	%ebp, %edi
	addl	$204346080, %edi                # imm = 0xC2E12E0
	addl	%eax, %edi
	leal	(%r14,%rdi), %r9d
	addl	$-1841331548, %r9d              # imm = 0x923F82A4
	movl	%ecx, %eax
	roll	$30, %eax
	movl	%ecx, %edi
	roll	$19, %edi
	xorl	%eax, %edi
	movl	%ecx, %r14d
	roll	$10, %r14d
	xorl	%edi, %r14d
	movl	%ecx, %edi
	andl	%r15d, %edi
	movl	%ecx, %eax
	orl	%r15d, %eax
	andl	%ebx, %eax
	orl	%edi, %eax
	addl	%r14d, %eax
	addl	%r9d, %r10d
	movl	%r10d, %r14d
	roll	$26, %r14d
	addl	%r9d, %eax
	movl	%r10d, %edi
	roll	$21, %edi
	xorl	%r14d, %edi
	movl	%r10d, %r9d
	roll	$7, %r9d
	xorl	%edi, %r9d
	movl	28(%rsi), %r12d
	bswapl	%r12d
	movl	%r8d, %edi
	xorl	%edx, %edi
	andl	%r10d, %edi
	xorl	%edx, %edi
	addl	%r12d, %r11d
	addl	$-1529998197, %r11d             # imm = 0xA4CE148B
	addl	%edi, %r11d
	movl	%eax, %r14d
	roll	$30, %r14d
	addl	%r11d, %r9d
	addl	$-1424204075, %r9d              # imm = 0xAB1C5ED5
	movl	%eax, %edi
	roll	$19, %edi
	xorl	%r14d, %edi
	movl	%eax, %r11d
	roll	$10, %r11d
	xorl	%edi, %r11d
	movl	%eax, %r14d
	andl	%ecx, %r14d
	movl	%eax, %edi
	orl	%ecx, %edi
	andl	%r15d, %edi
	orl	%r14d, %edi
	addl	%r11d, %edi
	addl	%r9d, %ebx
	addl	%r9d, %edi
	movl	%ebx, %r9d
	roll	$26, %r9d
	movl	%ebx, %r11d
	roll	$21, %r11d
	movl	%ebx, %r14d
	roll	$7, %r14d
	xorl	%r9d, %r11d
	xorl	%r11d, %r14d
	movzbl	32(%rsi), %ebp
	shll	$24, %ebp
	movq	%rbp, 24(%rsp)                  # 8-byte Spill
	movl	%r10d, %esi
	xorl	%r8d, %esi
	andl	%ebx, %esi
	xorl	%r8d, %esi
	addl	%ebp, %edx
	addl	%esi, %edx
	leal	(%r14,%rdx), %r9d
	addl	$-662197608, %r9d               # imm = 0xD887AA98
	movl	%edi, %r11d
	roll	$30, %r11d
	movl	%edi, %ebp
	roll	$19, %ebp
	movl	%edi, %esi
	roll	$10, %esi
	xorl	%r11d, %ebp
	xorl	%ebp, %esi
	movl	%edi, %r11d
	andl	%eax, %r11d
	movl	%edi, %ebp
	orl	%eax, %ebp
	andl	%ecx, %ebp
	orl	%r11d, %ebp
	addl	%esi, %ebp
	addl	%r9d, %r15d
	movl	%r15d, %r11d
	roll	$26, %r11d
	movl	%r15d, %esi
	roll	$21, %esi
	addl	%r9d, %ebp
	xorl	%r11d, %esi
	movl	%r15d, %r9d
	roll	$7, %r9d
	xorl	%esi, %r9d
	movl	%ebx, %esi
	xorl	%r10d, %esi
	andl	%r15d, %esi
	xorl	%r10d, %esi
	addl	%r8d, %esi
	leal	(%r9,%rsi), %r8d
	addl	$310598401, %r8d                # imm = 0x12835B01
	movl	%ebp, %r9d
	roll	$30, %r9d
	movl	%ebp, %esi
	roll	$19, %esi
	movl	%ebp, %r11d
	roll	$10, %r11d
	xorl	%r9d, %esi
	xorl	%esi, %r11d
	movl	%ebp, %esi
	andl	%edi, %esi
	movl	%ebp, %edx
	orl	%edi, %edx
	andl	%eax, %edx
	orl	%esi, %edx
	addl	%r11d, %edx
	movq	%rdx, %r11
	addl	%r8d, %ecx
	movl	%ecx, %r9d
	roll	$26, %r9d
	movl	%ecx, %esi
	roll	$21, %esi
	addl	%r8d, %r11d
	xorl	%r9d, %esi
	movl	%ecx, %r8d
	roll	$7, %r8d
	xorl	%esi, %r8d
	movl	%r15d, %esi
	xorl	%ebx, %esi
	andl	%ecx, %esi
	xorl	%ebx, %esi
	addl	%r10d, %esi
	leal	(%r8,%rsi), %r9d
	addl	$607225278, %r9d                # imm = 0x243185BE
	movl	%r11d, %r8d
	roll	$30, %r8d
	movl	%r11d, %esi
	roll	$19, %esi
	movl	%r11d, %r10d
	roll	$10, %r10d
	xorl	%r8d, %esi
	xorl	%esi, %r10d
	movl	%r11d, %esi
	andl	%ebp, %esi
	movl	%r11d, %r8d
	orl	%ebp, %r8d
	andl	%edi, %r8d
	orl	%esi, %r8d
	addl	%r10d, %r8d
	addl	%r9d, %eax
	movl	%eax, %r10d
	roll	$26, %r10d
	movl	%eax, %esi
	roll	$21, %esi
	addl	%r9d, %r8d
	xorl	%r10d, %esi
	movl	%eax, %r9d
	roll	$7, %r9d
	xorl	%esi, %r9d
	movl	%ecx, %esi
	xorl	%r15d, %esi
	andl	%eax, %esi
	xorl	%r15d, %esi
	addl	%ebx, %esi
	addl	%esi, %r9d
	addl	$1426881987, %r9d               # imm = 0x550C7DC3
	movl	%r8d, %r10d
	roll	$30, %r10d
	movl	%r8d, %esi
	roll	$19, %esi
	movl	%r8d, %ebx
	roll	$10, %ebx
	xorl	%r10d, %esi
	xorl	%esi, %ebx
	movl	%r8d, %esi
	andl	%r11d, %esi
	movl	%r8d, %edx
	orl	%r11d, %edx
	andl	%ebp, %edx
	orl	%esi, %edx
	addl	%ebx, %edx
	movq	%rdx, %r10
	addl	%r9d, %edi
	movl	%edi, %esi
	roll	$26, %esi
	movl	%edi, %ebx
	roll	$21, %ebx
	addl	%r9d, %r10d
	xorl	%esi, %ebx
	movl	%edi, %esi
	roll	$7, %esi
	xorl	%ebx, %esi
	movl	%eax, %ebx
	xorl	%ecx, %ebx
	andl	%edi, %ebx
	xorl	%ecx, %ebx
	addl	%r15d, %ebx
	leal	(%rsi,%rbx), %r9d
	addl	$1925078388, %r9d               # imm = 0x72BE5D74
	movl	%r10d, %esi
	roll	$30, %esi
	movl	%r10d, %ebx
	roll	$19, %ebx
	movl	%r10d, %edx
	roll	$10, %edx
	xorl	%esi, %ebx
	xorl	%ebx, %edx
	movl	%r10d, %esi
	andl	%r8d, %esi
	movl	%r10d, %r14d
	orl	%r8d, %r14d
	andl	%r11d, %r14d
	orl	%esi, %r14d
	addl	%edx, %r14d
	addl	%r9d, %ebp
	movl	%ebp, %edx
	roll	$26, %edx
	movl	%ebp, %esi
	roll	$21, %esi
	addl	%r9d, %r14d
	xorl	%edx, %esi
	movl	%ebp, %edx
	roll	$7, %edx
	xorl	%esi, %edx
	movl	%edi, %esi
	xorl	%eax, %esi
	andl	%ebp, %esi
	xorl	%eax, %esi
	addl	%ecx, %esi
	leal	(%rdx,%rsi), %ecx
	addl	$-2132889090, %ecx              # imm = 0x80DEB1FE
	movl	%r14d, %edx
	roll	$30, %edx
	movl	%r14d, %esi
	roll	$19, %esi
	movl	%r14d, %ebx
	roll	$10, %ebx
	xorl	%edx, %esi
	xorl	%esi, %ebx
	movl	%r14d, %edx
	andl	%r10d, %edx
	movl	%r14d, %r15d
	orl	%r10d, %r15d
	andl	%r8d, %r15d
	orl	%edx, %r15d
	addl	%ebx, %r15d
	addl	%ecx, %r11d
	movl	%r11d, %edx
	roll	$26, %edx
	movl	%r11d, %esi
	roll	$21, %esi
	addl	%ecx, %r15d
	xorl	%edx, %esi
	movl	%r11d, %ecx
	roll	$7, %ecx
	xorl	%esi, %ecx
	movl	%ebp, %edx
	xorl	%edi, %edx
	andl	%r11d, %edx
	xorl	%edi, %edx
	addl	%eax, %edx
	leal	(%rcx,%rdx), %eax
	addl	$-1680079193, %eax              # imm = 0x9BDC06A7
	movl	%r15d, %ecx
	roll	$30, %ecx
	movl	%r15d, %edx
	roll	$19, %edx
	movl	%r15d, %esi
	roll	$10, %esi
	xorl	%ecx, %edx
	xorl	%edx, %esi
	movl	%r15d, %ecx
	andl	%r14d, %ecx
	movl	%r15d, %edx
	orl	%r14d, %edx
	andl	%r10d, %edx
	orl	%ecx, %edx
	addl	%esi, %edx
	movq	%rdx, %rbx
	addl	%eax, %r8d
	movl	%r8d, %ecx
	roll	$26, %ecx
	movl	%r8d, %edx
	roll	$21, %edx
	addl	%eax, %ebx
	xorl	%ecx, %edx
	movl	%r8d, %eax
	roll	$7, %eax
	xorl	%edx, %eax
	movl	%r11d, %ecx
	xorl	%ebp, %ecx
	andl	%r8d, %ecx
	xorl	%ebp, %ecx
	addl	%edi, %ecx
	addl	%ecx, %eax
	addl	$-1046744452, %eax              # imm = 0xC19BF27C
	movl	%ebx, %ecx
	roll	$30, %ecx
	movl	%ebx, %edx
	roll	$19, %edx
	movl	%ebx, %esi
	roll	$10, %esi
	xorl	%ecx, %edx
	xorl	%edx, %esi
	movl	%ebx, %ecx
	andl	%r15d, %ecx
	movl	%ebx, %r13d
	orl	%r15d, %r13d
	andl	%r14d, %r13d
	orl	%ecx, %r13d
	addl	%esi, %r13d
	addl	%eax, %r10d
	movq	-56(%rsp), %rdx                 # 8-byte Reload
	movl	%edx, %ecx
	roll	$25, %ecx
	movl	%edx, %esi
	roll	$14, %esi
	addl	%eax, %r13d
	xorl	%ecx, %esi
	movl	%edx, %edi
	shrl	$3, %edi
	xorl	%esi, %edi
	movq	%r10, -40(%rsp)                 # 8-byte Spill
	movl	%r10d, %eax
	roll	$26, %eax
	movl	%r10d, %ecx
	roll	$21, %ecx
	addl	-96(%rsp), %edi                 # 4-byte Folded Reload
	movl	%edi, -80(%rsp)                 # 4-byte Spill
	xorl	%eax, %ecx
	movl	%r10d, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movl	%r8d, %ecx
	movq	%r11, -64(%rsp)                 # 8-byte Spill
	xorl	%r11d, %ecx
	andl	%r10d, %ecx
	xorl	%r11d, %ecx
	addl	%edi, %ebp
	addl	%ecx, %ebp
	movl	%r13d, %ecx
	roll	$30, %ecx
	addl	%ebp, %eax
	addl	$-459576895, %eax               # imm = 0xE49B69C1
	movl	%r13d, %esi
	roll	$19, %esi
	xorl	%ecx, %esi
	movl	%r13d, %ecx
	roll	$10, %ecx
	xorl	%esi, %ecx
	movl	%r13d, %esi
	movq	%rbx, -8(%rsp)                  # 8-byte Spill
	andl	%ebx, %esi
	movl	%r13d, %ebp
	orl	%ebx, %ebp
	andl	%r15d, %ebp
	orl	%esi, %ebp
	addl	%ecx, %ebp
	addl	%eax, %r14d
	addl	%eax, %ebp
	movl	-72(%rsp), %r10d                # 4-byte Reload
	movl	%r10d, %eax
	roll	$25, %eax
	roll	$14, %r10d
	movl	-120(%rsp), %edx                # 4-byte Reload
	movl	%edx, %ecx
	roll	$25, %ecx
	xorl	%eax, %r10d
	roll	$14, %edx
	xorl	%ecx, %edx
	movl	%edx, -16(%rsp)                 # 4-byte Spill
	movq	-104(%rsp), %rcx                # 8-byte Reload
	movl	%ecx, %eax
	roll	$25, %eax
	movl	%ecx, %r9d
	roll	$14, %r9d
	movq	-112(%rsp), %rdx                # 8-byte Reload
	movl	%edx, %ecx
	roll	$25, %ecx
	movl	%edx, %esi
	roll	$14, %esi
	xorl	%eax, %r9d
	xorl	%ecx, %esi
	movq	-128(%rsp), %rbx                # 8-byte Reload
	movl	%ebx, %eax
	roll	$25, %eax
	movl	%ebx, %edx
	roll	$14, %edx
	movl	%r12d, %edi
	roll	$25, %edi
	xorl	%eax, %edx
	movl	%r12d, %eax
	movq	%r12, %r11
	movq	%r12, -88(%rsp)                 # 8-byte Spill
	roll	$14, %eax
	xorl	%edi, %eax
                                        # kill: def $r11d killed $r11d killed $r11 def $r11
	shrl	$3, %r11d
	xorl	%eax, %r11d
	addl	%ebx, %r11d
	movl	%ebx, %eax
	shrl	$3, %eax
	xorl	%edx, %eax
	movq	-112(%rsp), %rcx                # 8-byte Reload
	addl	%ecx, %eax
	movl	%eax, -128(%rsp)                # 4-byte Spill
	movl	%ecx, %eax
	shrl	$3, %eax
	xorl	%esi, %eax
	movq	-104(%rsp), %rcx                # 8-byte Reload
	addl	%ecx, %eax
	movl	%eax, -96(%rsp)                 # 4-byte Spill
	movl	%ecx, %eax
	shrl	$3, %eax
	xorl	%r9d, %eax
	movl	-120(%rsp), %ebx                # 4-byte Reload
	addl	%ebx, %eax
	movl	%eax, -104(%rsp)                # 4-byte Spill
	shrl	$3, %ebx
	xorl	-16(%rsp), %ebx                 # 4-byte Folded Reload
	movl	-72(%rsp), %r9d                 # 4-byte Reload
	addl	%r9d, %ebx
	movl	%r9d, %eax
	shrl	$3, %eax
	xorl	%r10d, %eax
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, %esi
	leal	(%rcx,%rax), %r10d
	addl	$10813440, %r10d                # imm = 0xA50000
	movl	%r14d, %edx
	roll	$26, %edx
	addl	%eax, %esi
	movq	%rsi, -56(%rsp)                 # 8-byte Spill
	movl	%r14d, %eax
	roll	$21, %eax
	xorl	%edx, %eax
	movl	%r14d, %edx
	roll	$7, %edx
	xorl	%eax, %edx
	movq	-40(%rsp), %r12                 # 8-byte Reload
	movl	%r12d, %eax
	xorl	%r8d, %eax
	andl	%r14d, %eax
	xorl	%r8d, %eax
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	addl	%r10d, %ecx
	addl	%eax, %ecx
	movl	%ebp, %eax
	roll	$30, %eax
	movl	%ebp, %esi
	roll	$19, %esi
	addl	%ecx, %edx
	addl	$-272742522, %edx               # imm = 0xEFBE4786
	xorl	%eax, %esi
	movl	%ebp, %eax
	roll	$10, %eax
	xorl	%esi, %eax
	movl	%ebp, %edi
	andl	%r13d, %edi
	movl	%ebp, %esi
	orl	%r13d, %esi
	movq	-8(%rsp), %r9                   # 8-byte Reload
	andl	%r9d, %esi
	orl	%edi, %esi
	addl	%eax, %esi
	addl	%edx, %r15d
	addl	%edx, %esi
	movl	-80(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, %eax
	roll	$15, %eax
	movl	%ecx, %edx
	roll	$13, %edx
	xorl	%eax, %edx
	movl	%ecx, %eax
	shrl	$10, %eax
	xorl	%edx, %eax
	addl	%eax, %ebx
	movl	%r15d, %eax
	roll	$26, %eax
	movl	%r15d, %edx
	roll	$21, %edx
	xorl	%eax, %edx
	movl	%r15d, %eax
	roll	$7, %eax
	xorl	%edx, %eax
	movl	%r14d, %edx
	xorl	%r12d, %edx
	andl	%r15d, %edx
	movq	%r15, %rcx
	xorl	%r12d, %edx
	addl	%ebx, %r8d
	addl	%edx, %r8d
	leal	(%rax,%r8), %r15d
	addl	$264347078, %r15d               # imm = 0xFC19DC6
	movl	%esi, %edx
	roll	$30, %edx
	movl	%esi, %edi
	roll	$19, %edi
	movl	%esi, %eax
	roll	$10, %eax
	xorl	%edx, %edi
	xorl	%edi, %eax
	movl	%esi, %edx
	andl	%ebp, %edx
	movl	%esi, %r8d
	orl	%ebp, %r8d
	andl	%r13d, %r8d
	orl	%edx, %r8d
	addl	%eax, %r8d
	addl	%r15d, %r9d
	movl	%r10d, -120(%rsp)               # 4-byte Spill
	movl	%r10d, %eax
	roll	$15, %eax
	movl	%r10d, %edx
	roll	$13, %edx
	addl	%r15d, %r8d
	xorl	%eax, %edx
	shrl	$10, %r10d
	xorl	%edx, %r10d
	movq	%r9, %r12
	movl	%r12d, %edx
	roll	$26, %edx
	movl	%r12d, %edi
	roll	$21, %edi
	movl	-104(%rsp), %r9d                # 4-byte Reload
	addl	%r10d, %r9d
	movl	%r9d, -104(%rsp)                # 4-byte Spill
	xorl	%edx, %edi
	movl	%r12d, %eax
	roll	$7, %eax
	xorl	%edi, %eax
	movl	%ecx, %edx
	xorl	%r14d, %edx
	andl	%r12d, %edx
	xorl	%r14d, %edx
	movq	-40(%rsp), %rdi                 # 8-byte Reload
	addl	%r9d, %edi
	addl	%edx, %edi
	movl	%r8d, %edx
	roll	$30, %edx
	addl	%edi, %eax
	addl	$604807628, %eax                # imm = 0x240CA1CC
	movl	%r8d, %edi
	roll	$19, %edi
	xorl	%edx, %edi
	movl	%r8d, %edx
	roll	$10, %edx
	xorl	%edi, %edx
	movl	%r8d, %edi
	andl	%esi, %edi
	movl	%r8d, %r10d
	orl	%esi, %r10d
	andl	%ebp, %r10d
	orl	%edi, %r10d
	addl	%edx, %r10d
	addl	%eax, %r13d
	addl	%eax, %r10d
	movl	%ebx, -64(%rsp)                 # 4-byte Spill
	movl	%ebx, %eax
	roll	$15, %eax
	movl	%ebx, %edx
	roll	$13, %edx
	xorl	%eax, %edx
	movl	%ebx, %eax
	shrl	$10, %eax
	xorl	%edx, %eax
	movl	-96(%rsp), %ebx                 # 4-byte Reload
	addl	%eax, %ebx
	movl	%ebx, -96(%rsp)                 # 4-byte Spill
	movl	%r13d, %eax
	roll	$26, %eax
	movl	%r13d, %edx
	roll	$21, %edx
	movl	%r13d, %edi
	roll	$7, %edi
	xorl	%eax, %edx
	xorl	%edx, %edi
	movl	%r12d, %eax
	xorl	%ecx, %eax
	andl	%r13d, %eax
	xorl	%ecx, %eax
	addl	%ebx, %r14d
	addl	%eax, %r14d
	addl	%edi, %r14d
	addl	$770255983, %r14d               # imm = 0x2DE92C6F
	movl	%r10d, %edx
	roll	$30, %edx
	movl	%r10d, %edi
	roll	$19, %edi
	xorl	%edx, %edi
	movl	%r10d, %edx
	roll	$10, %edx
	xorl	%edi, %edx
	movl	%r10d, %eax
	andl	%r8d, %eax
	movl	%r10d, %r15d
	orl	%r8d, %r15d
	andl	%esi, %r15d
	orl	%eax, %r15d
	addl	%edx, %r15d
	addl	%r14d, %ebp
	movl	-104(%rsp), %ebx                # 4-byte Reload
	movl	%ebx, %eax
	roll	$15, %eax
	addl	%r14d, %r15d
	movl	%ebx, %edx
	roll	$13, %edx
	xorl	%eax, %edx
	movl	%ebx, %eax
	shrl	$10, %eax
	xorl	%edx, %eax
	movl	%ebp, %edx
	roll	$26, %edx
	movl	-128(%rsp), %edi                # 4-byte Reload
	addl	%eax, %edi
	movl	%edi, -128(%rsp)                # 4-byte Spill
	movl	%ebp, %eax
	roll	$21, %eax
	xorl	%edx, %eax
	movl	%ebp, %edx
	roll	$7, %edx
	xorl	%eax, %edx
	movl	%r13d, %eax
	xorl	%r12d, %eax
	andl	%ebp, %eax
	xorl	%r12d, %eax
	movq	%r12, %r14
	addl	%edi, %ecx
	addl	%eax, %ecx
	movl	%r15d, %eax
	roll	$30, %eax
	movl	%r15d, %edi
	roll	$19, %edi
	addl	%ecx, %edx
	addl	$1249150122, %edx               # imm = 0x4A7484AA
	xorl	%eax, %edi
	movl	%r15d, %eax
	roll	$10, %eax
	xorl	%edi, %eax
	movl	%r15d, %edi
	andl	%r10d, %edi
	movl	%r15d, %r9d
	orl	%r10d, %r9d
	andl	%r8d, %r9d
	orl	%edi, %r9d
	addl	%eax, %r9d
	addl	%edx, %esi
	addl	%edx, %r9d
	movl	-96(%rsp), %r12d                # 4-byte Reload
	movl	%r12d, %eax
	roll	$15, %eax
	movl	%r12d, %edx
	roll	$13, %edx
	xorl	%eax, %edx
	movl	%r12d, %eax
	shrl	$10, %eax
	movl	%esi, %edi
	roll	$26, %edi
	xorl	%edx, %eax
	movl	%esi, %edx
	roll	$21, %edx
	leal	(%r11,%rax), %ebx
	movq	%rbx, (%rsp)                    # 8-byte Spill
	leal	(%rax,%r11), %ebx
	addl	$264, %ebx                      # imm = 0x108
	xorl	%edi, %edx
	movl	%esi, %eax
	roll	$7, %eax
	xorl	%edx, %eax
	movl	%ebp, %ecx
	xorl	%r13d, %ecx
	andl	%esi, %ecx
	xorl	%r13d, %ecx
	movq	%r14, %rdx
	addl	%ebx, %edx
	addl	%ecx, %edx
	movl	%r9d, %ecx
	roll	$30, %ecx
	addl	%eax, %edx
	addl	$1555081692, %edx               # imm = 0x5CB0A9DC
	movl	%r9d, %eax
	roll	$19, %eax
	xorl	%ecx, %eax
	movl	%r9d, %ecx
	roll	$10, %ecx
	xorl	%eax, %ecx
	movl	%r9d, %edi
	andl	%r15d, %edi
	movl	%r9d, %r11d
	orl	%r15d, %r11d
	andl	%r10d, %r11d
	orl	%edi, %r11d
	addl	%ecx, %r11d
	addl	%edx, %r8d
	addl	%edx, %r11d
	movl	-128(%rsp), %eax                # 4-byte Reload
	movl	%eax, %ecx
	roll	$15, %ecx
	movl	%eax, %edx
	roll	$13, %edx
	xorl	%ecx, %edx
	movl	%eax, %ecx
	shrl	$10, %ecx
	xorl	%edx, %ecx
	movq	24(%rsp), %r12                  # 8-byte Reload
	leal	8388608(%r12), %edx
	movl	%edx, %edi
	shrl	$7, %edi
	movl	%edx, %eax
	shrl	$18, %eax
	orl	%edi, %eax
	shrl	$3, %edx
	xorl	%eax, %edx
	movq	-88(%rsp), %rax                 # 8-byte Reload
	addl	-80(%rsp), %eax                 # 4-byte Folded Reload
	addl	%edx, %eax
	addl	%ecx, %eax
	movq	%rax, %r14
	movl	%r8d, %eax
	roll	$26, %eax
	movl	%r8d, %ecx
	roll	$21, %ecx
	movl	%r8d, %edx
	roll	$7, %edx
	xorl	%eax, %ecx
	xorl	%ecx, %edx
	movl	%esi, %eax
	xorl	%ebp, %eax
	andl	%r8d, %eax
	xorl	%ebp, %eax
	addl	%r14d, %r13d
	addl	%eax, %r13d
	leal	(%rdx,%r13), %eax
	addl	$1996064986, %eax               # imm = 0x76F988DA
	movl	%r11d, %ecx
	roll	$30, %ecx
	movl	%r11d, %edx
	roll	$19, %edx
	xorl	%ecx, %edx
	movl	%r11d, %edi
	roll	$10, %edi
	xorl	%edx, %edi
	movl	%r11d, %edx
	andl	%r9d, %edx
	movl	%r11d, %ecx
	orl	%r9d, %ecx
	andl	%r15d, %ecx
	orl	%edx, %ecx
	addl	%edi, %ecx
	addl	%eax, %r10d
	movl	%ebx, -72(%rsp)                 # 4-byte Spill
	movl	%ebx, %edx
	roll	$15, %edx
	addl	%eax, %ecx
	movl	%ebx, %eax
	roll	$13, %eax
	xorl	%edx, %eax
	movl	%ebx, %edx
	shrl	$10, %edx
	xorl	%eax, %edx
	movq	-56(%rsp), %rax                 # 8-byte Reload
	addl	%eax, %r12d
	addl	$19202048, %r12d                # imm = 0x1250000
	addl	%edx, %r12d
	movl	%r10d, %eax
	roll	$26, %eax
	movl	%r10d, %edx
	roll	$21, %edx
	xorl	%eax, %edx
	movl	%r10d, %eax
	roll	$7, %eax
	xorl	%edx, %eax
	movl	%r8d, %edx
	xorl	%esi, %edx
	andl	%r10d, %edx
	xorl	%esi, %edx
	addl	%r12d, %ebp
	addl	%edx, %ebp
	addl	%ebp, %eax
	addl	$-1740746414, %eax              # imm = 0x983E5152
	movl	%ecx, %edx
	roll	$30, %edx
	movl	%ecx, %edi
	roll	$19, %edi
	movl	%ecx, %ebp
	roll	$10, %ebp
	xorl	%edx, %edi
	xorl	%edi, %ebp
	movl	%ecx, %edx
	andl	%r11d, %edx
	movl	%ecx, %ebx
	orl	%r11d, %ebx
	andl	%r9d, %ebx
	orl	%edx, %ebx
	addl	%ebp, %ebx
	addl	%eax, %r15d
	movl	%r14d, %edx
	roll	$15, %edx
	movl	%r14d, %edi
	movq	%r14, -88(%rsp)                 # 8-byte Spill
	roll	$13, %edi
	addl	%eax, %ebx
	xorl	%edx, %edi
	movl	%r14d, %ebp
	shrl	$10, %ebp
	xorl	%edi, %ebp
	movl	%r15d, %eax
	roll	$26, %eax
	movl	%r15d, %edx
	roll	$21, %edx
	xorl	%eax, %edx
	movl	%r15d, %eax
	roll	$7, %eax
	xorl	%edx, %eax
	addl	-64(%rsp), %ebp                 # 4-byte Folded Reload
	movl	%r10d, %edx
	xorl	%r8d, %edx
	andl	%r15d, %edx
	xorl	%r8d, %edx
	addl	%ebp, %esi
	movl	%ebp, %r14d
	addl	%edx, %esi
	movl	%ebx, %edx
	roll	$30, %edx
	addl	%esi, %eax
	addl	$-1473132947, %eax              # imm = 0xA831C66D
	movl	%ebx, %esi
	roll	$19, %esi
	xorl	%edx, %esi
	movl	%ebx, %edi
	roll	$10, %edi
	xorl	%esi, %edi
	movl	%ebx, %esi
	andl	%ecx, %esi
	movl	%ebx, %edx
	orl	%ecx, %edx
	andl	%r11d, %edx
	orl	%esi, %edx
	addl	%edi, %edx
	addl	%eax, %r9d
	addl	%eax, %edx
	movl	%r12d, %ebp
	movl	%r12d, 40(%rsp)                 # 4-byte Spill
	movl	%r12d, %eax
	roll	$15, %eax
	movl	%r12d, %esi
	roll	$13, %esi
	xorl	%eax, %esi
	shrl	$10, %ebp
	movl	%r9d, %eax
	roll	$26, %eax
	movl	%r9d, %edi
	roll	$21, %edi
	xorl	%esi, %ebp
	xorl	%eax, %edi
	movl	%r9d, %eax
	roll	$7, %eax
	xorl	%edi, %eax
	addl	-104(%rsp), %ebp                # 4-byte Folded Reload
	movl	%r15d, %esi
	xorl	%r10d, %esi
	andl	%r9d, %esi
	xorl	%r10d, %esi
	addl	%ebp, %r8d
	addl	%esi, %r8d
	addl	%r8d, %eax
	addl	$-1341970488, %eax              # imm = 0xB00327C8
	movl	%edx, %esi
	roll	$30, %esi
	movl	%edx, %edi
	roll	$19, %edi
	xorl	%esi, %edi
	movl	%edx, %esi
	roll	$10, %esi
	xorl	%edi, %esi
	movl	%edx, %edi
	andl	%ebx, %edi
	movl	%edx, %r8d
	orl	%ebx, %r8d
	andl	%ecx, %r8d
	orl	%edi, %r8d
	addl	%esi, %r8d
	addl	%eax, %r11d
	movl	%r14d, %r12d
	movl	%r14d, -8(%rsp)                 # 4-byte Spill
	movl	%r14d, %esi
	roll	$15, %esi
	addl	%eax, %r8d
	movl	%r14d, %eax
	roll	$13, %eax
	xorl	%esi, %eax
	shrl	$10, %r12d
	xorl	%eax, %r12d
	movl	%r11d, %eax
	roll	$26, %eax
	movl	%r11d, %esi
	roll	$21, %esi
	movl	%r11d, %edi
	roll	$7, %edi
	xorl	%eax, %esi
	xorl	%esi, %edi
	addl	-96(%rsp), %r12d                # 4-byte Folded Reload
	movl	%r9d, %eax
	xorl	%r15d, %eax
	andl	%r11d, %eax
	xorl	%r15d, %eax
	addl	%r12d, %r10d
	addl	%eax, %r10d
	movl	%r8d, %eax
	roll	$30, %eax
	movl	%r8d, %esi
	roll	$19, %esi
	addl	%r10d, %edi
	addl	$-1084653625, %edi              # imm = 0xBF597FC7
	xorl	%eax, %esi
	movl	%r8d, %eax
	roll	$10, %eax
	xorl	%esi, %eax
	movl	%r8d, %esi
	andl	%edx, %esi
	movl	%r8d, %r10d
	orl	%edx, %r10d
	andl	%ebx, %r10d
	orl	%esi, %r10d
	addl	%eax, %r10d
	addl	%edi, %ecx
	addl	%edi, %r10d
	movl	%ebp, %r14d
	movl	%ebp, -32(%rsp)                 # 4-byte Spill
	movl	%ebp, %eax
	roll	$15, %eax
	movl	%ebp, %esi
	roll	$13, %esi
	xorl	%eax, %esi
	shrl	$10, %r14d
	movl	%ecx, %eax
	roll	$26, %eax
	xorl	%esi, %r14d
	movl	%ecx, %esi
	roll	$21, %esi
	xorl	%eax, %esi
	movl	%ecx, %eax
	roll	$7, %eax
	xorl	%esi, %eax
	addl	-128(%rsp), %r14d               # 4-byte Folded Reload
	movl	%r11d, %esi
	xorl	%r9d, %esi
	andl	%ecx, %esi
	xorl	%r9d, %esi
	addl	%r14d, %r15d
	addl	%esi, %r15d
	addl	%eax, %r15d
	addl	$-958395405, %r15d              # imm = 0xC6E00BF3
	movl	%r10d, %esi
	roll	$30, %esi
	movl	%r10d, %edi
	roll	$19, %edi
	movl	%r10d, %eax
	roll	$10, %eax
	xorl	%esi, %edi
	xorl	%edi, %eax
	movl	%r10d, %esi
	andl	%r8d, %esi
	movl	%r10d, %ebp
	orl	%r8d, %ebp
	andl	%edx, %ebp
	orl	%esi, %ebp
	addl	%eax, %ebp
	addl	%r15d, %ebx
	movl	%r12d, -44(%rsp)                # 4-byte Spill
	movl	%r12d, %eax
	roll	$15, %eax
	movl	%r12d, %esi
	roll	$13, %esi
	addl	%r15d, %ebp
	xorl	%eax, %esi
	movl	%r12d, %eax
	shrl	$10, %eax
	xorl	%esi, %eax
	movl	%ebx, %esi
	roll	$26, %esi
	movl	%ebx, %edi
	roll	$21, %edi
	xorl	%esi, %edi
	movl	%ebx, %esi
	roll	$7, %esi
	xorl	%edi, %esi
	movq	(%rsp), %rdi                    # 8-byte Reload
	leal	(%rax,%rdi), %r15d
	addl	$264, %r15d                     # imm = 0x108
	movl	%ecx, %eax
	xorl	%r11d, %eax
	andl	%ebx, %eax
	xorl	%r11d, %eax
	addl	%r15d, %r9d
	addl	%eax, %r9d
	movl	%ebp, %eax
	roll	$30, %eax
	addl	%esi, %r9d
	addl	$-710438585, %r9d               # imm = 0xD5A79147
	movl	%ebp, %esi
	roll	$19, %esi
	xorl	%eax, %esi
	movl	%ebp, %eax
	roll	$10, %eax
	xorl	%esi, %eax
	movl	%ebp, %edi
	andl	%r10d, %edi
	movl	%ebp, %r13d
	orl	%r10d, %r13d
	andl	%r8d, %r13d
	orl	%edi, %r13d
	addl	%eax, %r13d
	addl	%r9d, %edx
	addl	%r9d, %r13d
	movl	%r14d, -28(%rsp)                # 4-byte Spill
	movl	%r14d, %eax
	roll	$15, %eax
	movl	%r14d, %edi
	roll	$13, %edi
	xorl	%eax, %edi
	movl	%r14d, %eax
	shrl	$10, %eax
	movq	%rax, %r14
	movl	%edx, %r9d
	roll	$26, %r9d
	movl	%edx, %eax
	roll	$21, %eax
	xorl	%edi, %r14d
	movq	%r14, 56(%rsp)                  # 8-byte Spill
	xorl	%r9d, %eax
	movl	%edx, %edi
	roll	$7, %edi
	xorl	%eax, %edi
	movl	%ebx, %eax
	xorl	%ecx, %eax
	andl	%edx, %eax
	xorl	%ecx, %eax
	movq	-88(%rsp), %rsi                 # 8-byte Reload
	addl	%r14d, %esi
	addl	$272760867, %esi                # imm = 0x10420023
	movl	%esi, -112(%rsp)                # 4-byte Spill
	addl	%esi, %r11d
	addl	%eax, %r11d
	addl	%edi, %r11d
	addl	$113926993, %r11d               # imm = 0x6CA6351
	movl	%r13d, %edi
	roll	$30, %edi
	movl	%r13d, %eax
	roll	$19, %eax
	xorl	%edi, %eax
	movl	%r13d, %edi
	roll	$10, %edi
	xorl	%eax, %edi
	movl	%r13d, %eax
	andl	%ebp, %eax
	movl	%r13d, %r9d
	orl	%ebp, %r9d
	andl	%r10d, %r9d
	orl	%eax, %r9d
	addl	%edi, %r9d
	addl	%r11d, %r8d
	movl	%r15d, -20(%rsp)                # 4-byte Spill
	movl	%r15d, %eax
	roll	$15, %eax
	addl	%r11d, %r9d
	movl	%r15d, %edi
	roll	$13, %edi
	xorl	%eax, %edi
                                        # kill: def $r15d killed $r15d def $r15
	shrl	$10, %r15d
	xorl	%edi, %r15d
	movl	-80(%rsp), %esi                 # 4-byte Reload
	movl	%esi, %eax
	roll	$25, %eax
	movl	%esi, %r14d
	roll	$14, %r14d
	movl	-120(%rsp), %r12d               # 4-byte Reload
	movl	%r12d, %r11d
	roll	$25, %r11d
	xorl	%eax, %r14d
	movl	%r12d, %eax
	roll	$14, %eax
	xorl	%r11d, %eax
	shrl	$3, %r12d
	xorl	%eax, %r12d
	addl	%esi, %r12d
	movl	%esi, %eax
	shrl	$3, %eax
	xorl	%r14d, %eax
	addl	40(%rsp), %eax                  # 4-byte Folded Reload
	leal	(%rax,%r15), %edi
	movq	%rdi, 72(%rsp)                  # 8-byte Spill
	leal	(%r15,%rax), %esi
	addl	$264, %esi                      # imm = 0x108
	movl	%r8d, %eax
	roll	$26, %eax
	movl	%r8d, %edi
	roll	$21, %edi
	xorl	%eax, %edi
	movl	%r8d, %eax
	roll	$7, %eax
	xorl	%edi, %eax
	movl	%edx, %edi
	xorl	%ebx, %edi
	andl	%r8d, %edi
	xorl	%ebx, %edi
	addl	%esi, %ecx
	addl	%edi, %ecx
	leal	(%rax,%rcx), %r11d
	addl	$338241895, %r11d               # imm = 0x14292967
	movl	%r9d, %ecx
	roll	$30, %ecx
	movl	%r9d, %edi
	roll	$19, %edi
	movl	%r9d, %eax
	roll	$10, %eax
	xorl	%ecx, %edi
	xorl	%edi, %eax
	movl	%r9d, %ecx
	andl	%r13d, %ecx
	movl	%r9d, %edi
	movq	%r9, %r14
	orl	%r13d, %edi
	andl	%ebp, %edi
	orl	%ecx, %edi
	addl	%eax, %edi
	addl	%r11d, %r10d
	movl	-112(%rsp), %eax                # 4-byte Reload
	movl	%eax, %r9d
	roll	$15, %r9d
	movl	%eax, %ecx
	roll	$13, %ecx
	addl	%r11d, %edi
	movq	%rdi, -40(%rsp)                 # 8-byte Spill
	xorl	%r9d, %ecx
	shrl	$10, %eax
	xorl	%ecx, %eax
	addl	-8(%rsp), %r12d                 # 4-byte Folded Reload
	addl	%eax, %r12d
	movl	%r10d, %r11d
	roll	$26, %r11d
	movl	%r10d, %ecx
	roll	$21, %ecx
	movl	%r10d, %eax
	roll	$7, %eax
	xorl	%r11d, %ecx
	xorl	%ecx, %eax
	movl	%r8d, %ecx
	xorl	%edx, %ecx
	andl	%r10d, %ecx
	xorl	%edx, %ecx
	addl	%r12d, %ebx
	addl	%ecx, %ebx
	leal	(%rax,%rbx), %r11d
	addl	$666307205, %r11d               # imm = 0x27B70A85
	movl	%edi, %ecx
	roll	$30, %ecx
	movl	%edi, %ebx
	roll	$19, %ebx
	xorl	%ecx, %ebx
	movl	%edi, %ecx
	roll	$10, %ecx
	xorl	%ebx, %ecx
	movl	%edi, %eax
	movq	%r14, 8(%rsp)                   # 8-byte Spill
	andl	%r14d, %eax
                                        # kill: def $edi killed $edi killed $rdi def $rdi
	orl	%r14d, %edi
	andl	%r13d, %edi
	orl	%eax, %edi
	addl	%ecx, %edi
	addl	%r11d, %ebp
	movq	%rsi, 16(%rsp)                  # 8-byte Spill
	movl	%esi, %eax
	roll	$15, %eax
	addl	%r11d, %edi
	movq	%rdi, -16(%rsp)                 # 8-byte Spill
	movl	%esi, %ecx
	roll	$13, %ecx
	xorl	%eax, %ecx
	movl	%esi, %ebx
	shrl	$10, %ebx
	xorl	%ecx, %ebx
	movl	-64(%rsp), %eax                 # 4-byte Reload
	movl	%eax, %r11d
	roll	$25, %r11d
	movl	%eax, %r15d
	roll	$14, %r15d
	movl	-104(%rsp), %edi                # 4-byte Reload
	movl	%edi, %esi
	roll	$25, %esi
	xorl	%r11d, %r15d
	movl	%edi, %ecx
	roll	$14, %ecx
	xorl	%esi, %ecx
	shrl	$3, %edi
	xorl	%ecx, %edi
	addl	%eax, %edi
	shrl	$3, %eax
	xorl	%r15d, %eax
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	leal	(%rax,%rcx), %r14d
	addl	$10813440, %r14d                # imm = 0xA50000
	addl	-32(%rsp), %r14d                # 4-byte Folded Reload
	addl	%ebx, %r14d
	movl	%ebp, %eax
	roll	$26, %eax
	movl	%ebp, %ecx
	roll	$21, %ecx
	xorl	%eax, %ecx
	movl	%ebp, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movl	%r10d, %ecx
	xorl	%r8d, %ecx
	andl	%ebp, %ecx
	xorl	%r8d, %ecx
	addl	%r14d, %edx
	addl	%ecx, %edx
	leal	(%rax,%rdx), %r11d
	addl	$773529912, %r11d               # imm = 0x2E1B2138
	movq	-16(%rsp), %r15                 # 8-byte Reload
	movl	%r15d, %ecx
	roll	$30, %ecx
	movl	%r15d, %edx
	roll	$19, %edx
	movl	%r15d, %eax
	roll	$10, %eax
	xorl	%ecx, %edx
	xorl	%edx, %eax
	movl	%r15d, %ecx
	movq	-40(%rsp), %r9                  # 8-byte Reload
	andl	%r9d, %ecx
	movl	%r15d, %ebx
	orl	%r9d, %ebx
	movq	8(%rsp), %rsi                   # 8-byte Reload
	andl	%esi, %ebx
	orl	%ecx, %ebx
	addl	%eax, %ebx
	addl	%r11d, %r13d
	movl	%r12d, -120(%rsp)               # 4-byte Spill
	movl	%r12d, %eax
	roll	$15, %eax
	movl	%r12d, %ecx
	roll	$13, %ecx
	addl	%r11d, %ebx
	xorl	%eax, %ecx
	shrl	$10, %r12d
	xorl	%ecx, %r12d
	addl	-44(%rsp), %edi                 # 4-byte Folded Reload
	addl	%r12d, %edi
	movl	%r13d, %eax
	roll	$26, %eax
	movl	%r13d, %ecx
	roll	$21, %ecx
	movl	%r13d, %edx
	roll	$7, %edx
	xorl	%eax, %ecx
	xorl	%ecx, %edx
	movl	%ebp, %eax
	xorl	%r10d, %eax
	andl	%r13d, %eax
	xorl	%r10d, %eax
	addl	%edi, %r8d
	addl	%eax, %r8d
	addl	%edx, %r8d
	addl	$1294757372, %r8d               # imm = 0x4D2C6DFC
	movq	%rbx, -64(%rsp)                 # 8-byte Spill
	movl	%ebx, %ecx
	roll	$30, %ecx
	movl	%ebx, %edx
	roll	$19, %edx
	xorl	%ecx, %edx
	movl	%ebx, %ecx
	roll	$10, %ecx
	xorl	%edx, %ecx
	movl	%ebx, %edx
	andl	%r15d, %edx
                                        # kill: def $ebx killed $ebx killed $rbx def $rbx
	orl	%r15d, %ebx
	andl	%r9d, %ebx
	orl	%edx, %ebx
	addl	%ecx, %ebx
	addl	%r8d, %esi
	movl	%r14d, 24(%rsp)                 # 4-byte Spill
	movl	%r14d, %eax
	roll	$15, %eax
	addl	%r8d, %ebx
	movl	%r14d, %edx
	roll	$13, %edx
	xorl	%eax, %edx
	shrl	$10, %r14d
	xorl	%edx, %r14d
	movl	-96(%rsp), %r12d                # 4-byte Reload
	movl	%r12d, %r11d
	roll	$25, %r11d
	movl	%r12d, %r8d
	roll	$14, %r8d
	movl	-128(%rsp), %ecx                # 4-byte Reload
	movl	%ecx, %edx
	roll	$25, %edx
	xorl	%r11d, %r8d
	movl	%ecx, %eax
	roll	$14, %eax
	xorl	%edx, %eax
	movl	%ecx, %edx
	shrl	$3, %edx
	xorl	%eax, %edx
	addl	%r12d, %edx
	movl	%edx, %r11d
	movl	%r12d, %eax
	shrl	$3, %eax
	xorl	%r8d, %eax
	addl	-104(%rsp), %eax                # 4-byte Folded Reload
	addl	-28(%rsp), %eax                 # 4-byte Folded Reload
	addl	%r14d, %eax
	movl	%eax, %r12d
	movl	%esi, %eax
	roll	$26, %eax
	movl	%esi, %ecx
	roll	$21, %ecx
	xorl	%eax, %ecx
	movl	%esi, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movl	%r13d, %ecx
	xorl	%ebp, %ecx
	andl	%esi, %ecx
	movq	%rsi, %r14
	xorl	%ebp, %ecx
	addl	%r12d, %r10d
	addl	%ecx, %r10d
	addl	%eax, %r10d
	addl	$1396182291, %r10d              # imm = 0x53380D13
	movl	%ebx, %ecx
	roll	$30, %ecx
	movl	%ebx, %edx
	roll	$19, %edx
	movl	%ebx, %eax
	roll	$10, %eax
	xorl	%ecx, %edx
	xorl	%edx, %eax
	movl	%ebx, %ecx
	movq	-64(%rsp), %rsi                 # 8-byte Reload
	andl	%esi, %ecx
	movl	%ebx, %edx
	movq	%rbx, %r9
	orl	%esi, %edx
	movq	%rsi, %rbx
	andl	%r15d, %edx
	orl	%ecx, %edx
	addl	%eax, %edx
	movq	-40(%rsp), %rsi                 # 8-byte Reload
	addl	%r10d, %esi
	movl	%edi, 36(%rsp)                  # 4-byte Spill
	movl	%edi, %eax
	roll	$15, %eax
	movl	%edi, %ecx
	roll	$13, %ecx
	addl	%r10d, %edx
	xorl	%eax, %ecx
	shrl	$10, %edi
	xorl	%ecx, %edi
	addl	-20(%rsp), %r11d                # 4-byte Folded Reload
	addl	%edi, %r11d
	movl	%esi, %eax
	roll	$26, %eax
	movl	%esi, %ecx
	roll	$21, %ecx
	movl	%esi, %r8d
	roll	$7, %r8d
	xorl	%eax, %ecx
	xorl	%ecx, %r8d
	movl	%r14d, %eax
	xorl	%r13d, %eax
	andl	%esi, %eax
	movq	%rsi, %rdi
	xorl	%r13d, %eax
	addl	%r11d, %ebp
	addl	%eax, %ebp
	leal	(%r8,%rbp), %eax
	addl	$1695183700, %eax               # imm = 0x650A7354
	movl	%edx, %ecx
	roll	$30, %ecx
	movl	%edx, %esi
	roll	$19, %esi
	xorl	%ecx, %esi
	movl	%edx, %ecx
	roll	$10, %ecx
	xorl	%esi, %ecx
	movl	%edx, %esi
	andl	%r9d, %esi
	movl	%edx, %ebp
	movq	%rdx, %r8
	orl	%r9d, %ebp
	movq	%r9, %r10
	movq	%r9, -80(%rsp)                  # 8-byte Spill
	andl	%ebx, %ebp
	orl	%esi, %ebp
	addl	%ecx, %ebp
	addl	%eax, %r15d
	movl	%r12d, 8(%rsp)                  # 4-byte Spill
	movl	%r12d, %ecx
	roll	$15, %ecx
	addl	%eax, %ebp
	movl	%r12d, %eax
	roll	$13, %eax
	xorl	%ecx, %eax
	movl	%r12d, %ecx
	shrl	$10, %ecx
	xorl	%eax, %ecx
	movl	-72(%rsp), %r12d                # 4-byte Reload
	movl	%r12d, %eax
	roll	$25, %eax
	movl	%r12d, %edx
	roll	$14, %edx
	xorl	%eax, %edx
	shrl	$3, %r12d
	xorl	%edx, %r12d
	addl	-128(%rsp), %r12d               # 4-byte Folded Reload
	addl	-112(%rsp), %r12d               # 4-byte Folded Reload
	addl	%ecx, %r12d
	movq	%r15, -16(%rsp)                 # 8-byte Spill
	movl	%r15d, %eax
	roll	$26, %eax
	movl	%r15d, %ecx
	roll	$21, %ecx
	xorl	%eax, %ecx
	movl	%r15d, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movl	%edi, %ecx
	xorl	%r14d, %ecx
	andl	%r15d, %ecx
	xorl	%r14d, %ecx
	addl	%r12d, %r13d
	addl	%ecx, %r13d
	addl	%r13d, %eax
	addl	$1986661051, %eax               # imm = 0x766A0ABB
	movq	%rbp, %r13
	movq	%rbp, -56(%rsp)                 # 8-byte Spill
	movl	%r13d, %ecx
	roll	$30, %ecx
	movl	%r13d, %edx
	roll	$19, %edx
	movl	%r13d, %esi
	roll	$10, %esi
	xorl	%ecx, %edx
	xorl	%edx, %esi
	movl	%r13d, %ecx
	movq	%r8, -96(%rsp)                  # 8-byte Spill
	andl	%r8d, %ecx
                                        # kill: def $r13d killed $r13d killed $r13 def $r13
	orl	%r8d, %r13d
	andl	%r10d, %r13d
	orl	%ecx, %r13d
	addl	%esi, %r13d
	addl	%eax, %ebx
	movl	%r11d, %r10d
	movl	%r11d, -24(%rsp)                # 4-byte Spill
	movl	%r11d, %ecx
	roll	$15, %ecx
	movl	%r11d, %edx
	roll	$13, %edx
	addl	%eax, %r13d
	xorl	%ecx, %edx
	shrl	$10, %r10d
	xorl	%edx, %r10d
	movq	-88(%rsp), %r8                  # 8-byte Reload
	movl	%r8d, %esi
	roll	$25, %esi
	movl	%r8d, %edx
	roll	$14, %edx
	movl	40(%rsp), %r9d                  # 4-byte Reload
	movl	%r9d, %ebp
	roll	$25, %ebp
	movl	%r9d, %eax
	roll	$14, %eax
	xorl	%esi, %edx
	xorl	%ebp, %eax
	movl	-8(%rsp), %esi                  # 4-byte Reload
	movl	%esi, %ecx
	roll	$25, %ecx
	movl	%esi, %ebp
	roll	$14, %ebp
	xorl	%ecx, %ebp
	shrl	$3, %esi
	xorl	%ebp, %esi
	addl	%r9d, %esi
	movl	%esi, -64(%rsp)                 # 4-byte Spill
	shrl	$3, %r9d
	xorl	%eax, %r9d
	movq	%r8, %rax
	movq	56(%rsp), %rsi                  # 8-byte Reload
	addl	%r8d, %esi
	movq	%rsi, 48(%rsp)                  # 8-byte Spill
	addl	%eax, %r9d
                                        # kill: def $eax killed $eax killed $rax def $rax
	shrl	$3, %eax
	xorl	%edx, %eax
	movq	(%rsp), %rcx                    # 8-byte Reload
	leal	(%rax,%rcx), %r8d
	addl	$264, %r8d                      # imm = 0x108
	addl	16(%rsp), %r8d                  # 4-byte Folded Reload
	movl	%ebx, %eax
	roll	$26, %eax
	addl	%r10d, %r8d
	movl	%ebx, %ecx
	roll	$21, %ecx
	xorl	%eax, %ecx
	movl	%ebx, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movq	-16(%rsp), %r11                 # 8-byte Reload
	movl	%r11d, %ecx
	xorl	%edi, %ecx
	andl	%ebx, %ecx
	movq	%rbx, %rbp
	xorl	%edi, %ecx
	addl	%r8d, %r14d
	addl	%ecx, %r14d
	movq	%r13, %rsi
	movl	%esi, %ecx
	roll	$30, %ecx
	movl	%esi, %edx
	roll	$19, %edx
	leal	(%rax,%r14), %r10d
	addl	$-2117940946, %r10d             # imm = 0x81C2C92E
	xorl	%ecx, %edx
	movl	%esi, %ecx
	roll	$10, %ecx
	xorl	%edx, %ecx
	movq	-56(%rsp), %r15                 # 8-byte Reload
	andl	%r15d, %r13d
	movl	%esi, %eax
	orl	%r15d, %eax
	movq	-96(%rsp), %r14                 # 8-byte Reload
	andl	%r14d, %eax
	orl	%r13d, %eax
	addl	%ecx, %eax
	movq	-80(%rsp), %rbx                 # 8-byte Reload
	addl	%r10d, %ebx
	addl	%r10d, %eax
	movl	%r12d, -72(%rsp)                # 4-byte Spill
	movl	%r12d, %edx
	roll	$15, %edx
	movl	%r12d, %ecx
	roll	$13, %ecx
	xorl	%edx, %ecx
	movl	%r12d, %r10d
	shrl	$10, %r10d
	xorl	%ecx, %r10d
	addl	-120(%rsp), %r9d                # 4-byte Folded Reload
	movl	%ebx, %ecx
	roll	$26, %ecx
	movl	%ebx, %edx
	roll	$21, %edx
	addl	%r10d, %r9d
	xorl	%ecx, %edx
	movl	%ebx, %r12d
	roll	$7, %r12d
	xorl	%edx, %r12d
	movl	%ebp, %ecx
	xorl	%r11d, %ecx
	andl	%ebx, %ecx
	movq	%rbx, %r10
	movq	%rbx, -80(%rsp)                 # 8-byte Spill
	xorl	%r11d, %ecx
	movq	%r11, %rbx
	addl	%r9d, %edi
	addl	%ecx, %edi
	movl	%eax, %ecx
	roll	$30, %ecx
	leal	(%r12,%rdi), %r11d
	addl	$-1838011259, %r11d             # imm = 0x92722C85
	movl	%eax, %edx
	roll	$19, %edx
	xorl	%ecx, %edx
	movl	%eax, %ecx
	roll	$10, %ecx
	xorl	%edx, %ecx
	movl	%eax, %edx
	movq	%rsi, %rdi
	andl	%edi, %edx
	movl	%eax, %esi
	movq	%rax, %r13
	orl	%edi, %esi
	movq	%rdi, -40(%rsp)                 # 8-byte Spill
	movq	%r15, %r12
	andl	%r12d, %esi
	orl	%edx, %esi
	addl	%ecx, %esi
	addl	%r11d, %r14d
	addl	%r11d, %esi
	movl	%r8d, (%rsp)                    # 4-byte Spill
	movl	%r8d, %eax
	roll	$15, %eax
	movl	%r8d, %ecx
	roll	$13, %ecx
	xorl	%eax, %ecx
	shrl	$10, %r8d
	xorl	%ecx, %r8d
	movl	-64(%rsp), %r11d                # 4-byte Reload
	addl	24(%rsp), %r11d                 # 4-byte Folded Reload
	movq	%r14, -96(%rsp)                 # 8-byte Spill
	movl	%r14d, %ecx
	roll	$26, %ecx
	addl	%r8d, %r11d
	movl	%r11d, -64(%rsp)                # 4-byte Spill
	movl	%r14d, %eax
	roll	$21, %eax
	xorl	%ecx, %eax
	movl	%r14d, %ecx
	roll	$7, %ecx
	xorl	%eax, %ecx
	movl	%r10d, %eax
	xorl	%ebp, %eax
	andl	%r14d, %eax
	xorl	%ebp, %eax
	movq	%rbp, %r15
	addl	%r11d, %ebx
	addl	%eax, %ebx
	movq	%rsi, %rbp
	movl	%ebp, %eax
	roll	$30, %eax
	movl	%ebp, %edx
	roll	$19, %edx
	addl	%ebx, %ecx
	addl	$-1564481375, %ecx              # imm = 0xA2BFE8A1
	xorl	%eax, %edx
	movl	%ebp, %eax
	roll	$10, %eax
	xorl	%edx, %eax
	movl	%ebp, %edx
	andl	%r13d, %edx
	orl	%r13d, %esi
	andl	%edi, %esi
	orl	%edx, %esi
	addl	%eax, %esi
	addl	%ecx, %r12d
	movq	%r12, -56(%rsp)                 # 8-byte Spill
	addl	%ecx, %esi
	movq	%rsi, -104(%rsp)                # 8-byte Spill
	movl	%r9d, -128(%rsp)                # 4-byte Spill
	movl	%r9d, %eax
	roll	$15, %eax
	movl	%r9d, %edx
	roll	$13, %edx
	xorl	%eax, %edx
	shrl	$10, %r9d
	movl	-32(%rsp), %r14d                # 4-byte Reload
	movl	%r14d, %eax
	roll	$25, %eax
	xorl	%edx, %r9d
	movl	%r14d, %r11d
	roll	$14, %r11d
	xorl	%eax, %r11d
	movl	-44(%rsp), %eax                 # 4-byte Reload
	movl	%eax, %ebx
	roll	$25, %ebx
	movl	%eax, %r8d
	movl	%eax, %r12d
	roll	$14, %r8d
	movl	-28(%rsp), %r10d                # 4-byte Reload
	movl	%r10d, %eax
	roll	$25, %eax
	movl	%r10d, %edi
	roll	$14, %edi
	xorl	%ebx, %r8d
	xorl	%eax, %edi
	movl	-20(%rsp), %esi                 # 4-byte Reload
	movl	%esi, %eax
	roll	$25, %eax
	movl	%esi, %ebx
	roll	$14, %ebx
	movl	-112(%rsp), %ecx                # 4-byte Reload
	movl	%ecx, %edx
	roll	$25, %edx
	xorl	%eax, %ebx
	movl	%ecx, %eax
	roll	$14, %eax
	xorl	%edx, %eax
	shrl	$3, %ecx
	xorl	%eax, %ecx
	addl	%esi, %ecx
	movl	%ecx, -112(%rsp)                # 4-byte Spill
	movl	%esi, %eax
	shrl	$3, %eax
	xorl	%ebx, %eax
	addl	%r10d, %eax
	movl	%eax, -16(%rsp)                 # 4-byte Spill
	shrl	$3, %r10d
	xorl	%edi, %r10d
	addl	%r12d, %r10d
	movl	%r10d, 56(%rsp)                 # 4-byte Spill
	shrl	$3, %r12d
	xorl	%r8d, %r12d
	addl	%r14d, %r12d
	movl	%r12d, -44(%rsp)                # 4-byte Spill
	shrl	$3, %r14d
	xorl	%r11d, %r14d
	addl	-8(%rsp), %r14d                 # 4-byte Folded Reload
	movl	36(%rsp), %esi                  # 4-byte Reload
	addl	%esi, %r14d
	movq	-56(%rsp), %rbx                 # 8-byte Reload
	movl	%ebx, %eax
	roll	$26, %eax
	movl	%ebx, %ecx
	roll	$21, %ecx
	addl	%r9d, %r14d
	movl	%r14d, -20(%rsp)                # 4-byte Spill
	xorl	%eax, %ecx
	movl	%ebx, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movq	-96(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	movq	-80(%rsp), %rdx                 # 8-byte Reload
	xorl	%edx, %ecx
	andl	%ebx, %ecx
	xorl	%edx, %ecx
	movq	%r15, %rdx
	addl	%r14d, %edx
	addl	%ecx, %edx
	movq	-104(%rsp), %rbx                # 8-byte Reload
	movl	%ebx, %ecx
	roll	$30, %ecx
	addl	%edx, %eax
	addl	$-1474664885, %eax              # imm = 0xA81A664B
	movl	%ebx, %edx
	roll	$19, %edx
	xorl	%ecx, %edx
	movl	%ebx, %ecx
	roll	$10, %ecx
	xorl	%edx, %ecx
	movl	%ebx, %edx
	movq	%rbp, 40(%rsp)                  # 8-byte Spill
	andl	%ebp, %edx
                                        # kill: def $ebx killed $ebx killed $rbx def $rbx
	orl	%ebp, %ebx
	andl	%r13d, %ebx
	movq	%r13, %rbp
	orl	%edx, %ebx
	addl	%ecx, %ebx
	movq	-40(%rsp), %rcx                 # 8-byte Reload
	addl	%eax, %ecx
	movq	%rcx, -40(%rsp)                 # 8-byte Spill
	addl	%eax, %ebx
	movq	%rbx, -88(%rsp)                 # 8-byte Spill
	movl	-64(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, %eax
	roll	$15, %eax
	roll	$13, %ecx
	movl	%ecx, %edi
	movq	16(%rsp), %rdx                  # 8-byte Reload
	movl	%edx, %ecx
	roll	$25, %ecx
	xorl	%eax, %edi
	movl	%edi, 68(%rsp)                  # 4-byte Spill
	movl	%edx, %eax
	roll	$14, %eax
	xorl	%ecx, %eax
	shrl	$3, %edx
	xorl	%eax, %edx
	movl	-120(%rsp), %ecx                # 4-byte Reload
	movl	%ecx, %eax
	roll	$25, %eax
	movq	48(%rsp), %rdi                  # 8-byte Reload
	addl	%edi, %edx
	addl	$272760867, %edx                # imm = 0x10420023
	movl	%edx, -8(%rsp)                  # 4-byte Spill
	movl	%ecx, %edx
	roll	$14, %edx
	xorl	%eax, %edx
	movl	%edx, 48(%rsp)                  # 4-byte Spill
	movl	24(%rsp), %r12d                 # 4-byte Reload
	movl	%r12d, %eax
	roll	$25, %eax
	roll	$14, %r12d
	movl	%esi, %r15d
	movl	%esi, %ecx
	roll	$25, %ecx
	roll	$14, %r15d
	xorl	%eax, %r12d
	xorl	%ecx, %r15d
	movl	8(%rsp), %r14d                  # 4-byte Reload
	movl	%r14d, %eax
	roll	$25, %eax
	roll	$14, %r14d
	movl	-24(%rsp), %r9d                 # 4-byte Reload
	movl	%r9d, %ecx
	roll	$25, %ecx
	xorl	%eax, %r14d
	roll	$14, %r9d
	xorl	%ecx, %r9d
	movl	-72(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, %eax
	roll	$25, %eax
	roll	$14, %ecx
	movl	(%rsp), %r11d                   # 4-byte Reload
	movl	%r11d, %edx
	roll	$25, %edx
	movl	%r11d, %ebx
	roll	$14, %ebx
	xorl	%eax, %ecx
	xorl	%edx, %ebx
	movl	-128(%rsp), %r13d               # 4-byte Reload
	movl	%r13d, %edx
	roll	$25, %edx
	movl	%r13d, %r10d
	roll	$14, %r10d
	movl	-64(%rsp), %r8d                 # 4-byte Reload
	movl	%r8d, %eax
	roll	$25, %eax
	xorl	%edx, %r10d
	movl	%r8d, %edx
	roll	$14, %edx
	xorl	%eax, %edx
	movl	%r8d, %esi
	shrl	$3, %esi
	xorl	%edx, %esi
	addl	%r13d, -8(%rsp)                 # 4-byte Folded Spill
	addl	%r13d, %esi
	movl	%esi, 16(%rsp)                  # 4-byte Spill
	shrl	$3, %r13d
	xorl	%r10d, %r13d
	addl	%r11d, -112(%rsp)               # 4-byte Folded Spill
	addl	%r11d, %r13d
	movl	%r13d, -28(%rsp)                # 4-byte Spill
	shrl	$3, %r11d
	xorl	%ebx, %r11d
	movl	-72(%rsp), %eax                 # 4-byte Reload
	addl	%eax, -16(%rsp)                 # 4-byte Folded Spill
	addl	%eax, %r11d
	movl	%r11d, -32(%rsp)                # 4-byte Spill
	movl	%eax, %edx
	shrl	$3, %edx
	xorl	%ecx, %edx
	movl	-24(%rsp), %eax                 # 4-byte Reload
	movl	56(%rsp), %r13d                 # 4-byte Reload
	addl	%eax, %r13d
	addl	%eax, %edx
	movl	%edx, (%rsp)                    # 4-byte Spill
	movl	%eax, %ecx
	shrl	$3, %ecx
	xorl	%r9d, %ecx
	movl	8(%rsp), %eax                   # 4-byte Reload
	movl	-44(%rsp), %esi                 # 4-byte Reload
	addl	%eax, %esi
	addl	%eax, %ecx
	movl	%ecx, -24(%rsp)                 # 4-byte Spill
	shrl	$3, %eax
	xorl	%r14d, %eax
	movl	36(%rsp), %ecx                  # 4-byte Reload
	addl	%ecx, %eax
	movl	%eax, 8(%rsp)                   # 4-byte Spill
	movl	%ecx, %eax
	shrl	$3, %eax
	xorl	%r15d, %eax
	movl	24(%rsp), %ecx                  # 4-byte Reload
	addl	%ecx, %eax
	movl	%eax, -44(%rsp)                 # 4-byte Spill
                                        # kill: def $ecx killed $ecx def $rcx
	shrl	$3, %ecx
	xorl	%r12d, %ecx
	movl	-120(%rsp), %eax                # 4-byte Reload
	addl	%eax, %ecx
	movq	%rcx, -128(%rsp)                # 8-byte Spill
                                        # kill: def $eax killed $eax def $rax
	shrl	$3, %eax
	xorl	48(%rsp), %eax                  # 4-byte Folded Reload
	movq	72(%rsp), %rcx                  # 8-byte Reload
	leal	(%rax,%rcx), %edx
	addl	$264, %edx                      # imm = 0x108
	movl	-20(%rsp), %r12d                # 4-byte Reload
	movl	%r12d, %eax
	roll	$25, %eax
	movl	%r12d, %ecx
	roll	$14, %ecx
	xorl	%eax, %ecx
	movl	%r12d, %eax
	shrl	$3, %eax
	xorl	%ecx, %eax
	addl	%r8d, %edx
	movl	%edx, -120(%rsp)                # 4-byte Spill
	addl	%r8d, %eax
	movl	%eax, -72(%rsp)                 # 4-byte Spill
	shrl	$10, %r8d
	xorl	68(%rsp), %r8d                  # 4-byte Folded Reload
	addl	%r8d, %esi
	movq	-40(%rsp), %rdx                 # 8-byte Reload
	movl	%edx, %eax
	roll	$26, %eax
	movl	%edx, %ecx
	roll	$21, %ecx
	xorl	%eax, %ecx
	movl	%edx, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movq	-56(%rsp), %r14                 # 8-byte Reload
	movl	%r14d, %ecx
	movq	-96(%rsp), %rbx                 # 8-byte Reload
	xorl	%ebx, %ecx
	andl	%edx, %ecx
	movq	%rdx, %r9
	xorl	%ebx, %ecx
	movq	%rbx, %r11
	movq	-80(%rsp), %rdx                 # 8-byte Reload
	addl	%esi, %edx
	movl	%esi, %ebx
	addl	%ecx, %edx
	leal	(%rax,%rdx), %ecx
	addl	$-1035236496, %ecx              # imm = 0xC24B8B70
	movq	-88(%rsp), %r15                 # 8-byte Reload
	movl	%r15d, %eax
	roll	$30, %eax
	movl	%r15d, %edx
	roll	$19, %edx
	movl	%r15d, %esi
	roll	$10, %esi
	xorl	%eax, %edx
	xorl	%edx, %esi
	movl	%r15d, %eax
	movq	-104(%rsp), %rdx                # 8-byte Reload
	andl	%edx, %eax
	movl	%r15d, %r8d
	orl	%edx, %r8d
	movq	40(%rsp), %r10                  # 8-byte Reload
	andl	%r10d, %r8d
	orl	%eax, %r8d
	addl	%esi, %r8d
	movq	%rbp, %rsi
	addl	%ecx, %esi
	movl	%r12d, %edx
	roll	$15, %edx
	movl	%r12d, %eax
	roll	$13, %eax
	addl	%ecx, %r8d
	xorl	%edx, %eax
	movl	%ebx, %ecx
	roll	$25, %ecx
	movl	%ebx, %edx
	roll	$14, %edx
	xorl	%ecx, %edx
	movl	%ebx, %ecx
	shrl	$3, %ecx
	xorl	%edx, %ecx
	movq	-128(%rsp), %rdx                # 8-byte Reload
	addl	%r12d, %edx
	movq	%rdx, -128(%rsp)                # 8-byte Spill
	addl	%r12d, %ecx
	movl	%ecx, -80(%rsp)                 # 4-byte Spill
	shrl	$10, %r12d
	xorl	%eax, %r12d
	addl	%r12d, %r13d
	movq	%rsi, %rdi
	movl	%edi, %eax
	roll	$26, %eax
	movl	%edi, %ecx
	roll	$21, %ecx
	movl	%edi, %edx
	roll	$7, %edx
	xorl	%eax, %ecx
	xorl	%ecx, %edx
	movl	%r9d, %eax
	movq	%r14, %rsi
	xorl	%esi, %eax
	andl	%edi, %eax
	movq	%rdi, %rbp
	xorl	%esi, %eax
	movq	%r11, %rcx
	addl	%r13d, %ecx
	addl	%eax, %ecx
	leal	(%rdx,%rcx), %eax
	addl	$-949202525, %eax               # imm = 0xC76C51A3
	movl	%r8d, %ecx
	roll	$30, %ecx
	movl	%r8d, %edx
	roll	$19, %edx
	xorl	%ecx, %edx
	movl	%r8d, %ecx
	roll	$10, %ecx
	xorl	%edx, %ecx
	movl	%r8d, %edx
	andl	%r15d, %edx
	movl	%r8d, %r11d
	movq	%r8, %r12
	orl	%r15d, %r11d
	movq	-104(%rsp), %r8                 # 8-byte Reload
	andl	%r8d, %r11d
	orl	%edx, %r11d
	addl	%ecx, %r11d
	addl	%eax, %r10d
	movl	%ebx, %ecx
	roll	$15, %ecx
	addl	%eax, %r11d
	movl	%ebx, %eax
	roll	$13, %eax
	xorl	%ecx, %eax
	movl	%r13d, %ecx
	roll	$25, %ecx
	movl	%r13d, %edx
	roll	$14, %edx
	xorl	%ecx, %edx
	movl	%r13d, %ecx
	shrl	$3, %ecx
	xorl	%edx, %ecx
	addl	%ebx, -44(%rsp)                 # 4-byte Folded Spill
	addl	%ebx, %ecx
	movl	%ecx, -96(%rsp)                 # 4-byte Spill
	movl	%ebx, %ecx
	shrl	$10, %ecx
	xorl	%eax, %ecx
	movl	-16(%rsp), %edi                 # 4-byte Reload
	addl	%ecx, %edi
	movl	%r10d, %eax
	roll	$26, %eax
	movl	%r10d, %ecx
	roll	$21, %ecx
	xorl	%eax, %ecx
	movl	%r10d, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movl	%ebp, %ecx
	xorl	%r9d, %ecx
	andl	%r10d, %ecx
	xorl	%r9d, %ecx
	movq	%rsi, %rdx
	addl	%edi, %edx
	movl	%edi, %r14d
	addl	%ecx, %edx
	leal	(%rax,%rdx), %ecx
	addl	$-778901479, %ecx               # imm = 0xD192E819
	movl	%r11d, %eax
	roll	$30, %eax
	movl	%r11d, %edx
	roll	$19, %edx
	movl	%r11d, %esi
	roll	$10, %esi
	xorl	%eax, %edx
	xorl	%edx, %esi
	movl	%r11d, %eax
	andl	%r12d, %eax
	movl	%r11d, %ebx
	orl	%r12d, %ebx
	andl	%r15d, %ebx
	orl	%eax, %ebx
	addl	%esi, %ebx
	addl	%ecx, %r8d
	movl	%r13d, %edx
	roll	$15, %edx
	movl	%r13d, %eax
	roll	$13, %eax
	addl	%ecx, %ebx
	xorl	%edx, %eax
	movl	%edi, %esi
	movl	%r14d, %ecx
	roll	$25, %ecx
	movl	%r14d, %edx
	roll	$14, %edx
	xorl	%ecx, %edx
	movl	%r14d, %ecx
	shrl	$3, %ecx
	xorl	%edx, %ecx
	addl	%r13d, 8(%rsp)                  # 4-byte Folded Spill
	addl	%r13d, %ecx
	movl	%ecx, -56(%rsp)                 # 4-byte Spill
	movl	%r13d, %ecx
	shrl	$10, %ecx
	xorl	%eax, %ecx
	movl	-112(%rsp), %r13d               # 4-byte Reload
	addl	%ecx, %r13d
	movl	%r8d, %eax
	roll	$26, %eax
	movl	%r8d, %ecx
	roll	$21, %ecx
	movl	%r8d, %edx
	roll	$7, %edx
	xorl	%eax, %ecx
	xorl	%ecx, %edx
	movl	%r10d, %eax
	xorl	%ebp, %eax
	andl	%r8d, %eax
	movq	%r8, %rdi
	movq	%r8, -104(%rsp)                 # 8-byte Spill
	xorl	%ebp, %eax
	addl	%r13d, %r9d
	addl	%eax, %r9d
	leal	(%rdx,%r9), %eax
	addl	$-694614492, %eax               # imm = 0xD6990624
	movl	%ebx, %ecx
	roll	$30, %ecx
	movl	%ebx, %edx
	roll	$19, %edx
	xorl	%ecx, %edx
	movl	%ebx, %ecx
	roll	$10, %ecx
	xorl	%edx, %ecx
	movl	%ebx, %edx
	andl	%r11d, %edx
	movl	%ebx, %r14d
	orl	%r11d, %r14d
	andl	%r12d, %r14d
	orl	%edx, %r14d
	addl	%ecx, %r14d
	addl	%eax, %r15d
	movl	%esi, %ecx
	roll	$15, %ecx
	addl	%eax, %r14d
	movl	%esi, %eax
	roll	$13, %eax
	xorl	%ecx, %eax
	movl	%r13d, %ecx
	roll	$25, %ecx
	movl	%r13d, %edx
	roll	$14, %edx
	xorl	%ecx, %edx
	movl	%r13d, %ecx
	shrl	$3, %ecx
	xorl	%edx, %ecx
	addl	%esi, -24(%rsp)                 # 4-byte Folded Spill
	addl	%esi, %ecx
	movl	%ecx, -112(%rsp)                # 4-byte Spill
	movl	%esi, %ecx
	shrl	$10, %ecx
	xorl	%eax, %ecx
	movl	-8(%rsp), %r8d                  # 4-byte Reload
	addl	%ecx, %r8d
	movl	%r15d, %eax
	roll	$26, %eax
	movl	%r15d, %ecx
	roll	$21, %ecx
	xorl	%eax, %ecx
	movl	%r15d, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movl	%edi, %ecx
	xorl	%r10d, %ecx
	andl	%r15d, %ecx
	movq	%r15, %r9
	movq	%r15, -88(%rsp)                 # 8-byte Spill
	xorl	%r10d, %ecx
	addl	%r8d, %ebp
	addl	%ecx, %ebp
	leal	(%rax,%rbp), %edx
	addl	$-200395387, %edx               # imm = 0xF40E3585
	movl	%r14d, %eax
	roll	$30, %eax
	movl	%r14d, %ecx
	roll	$19, %ecx
	movl	%r14d, %esi
	roll	$10, %esi
	xorl	%eax, %ecx
	xorl	%ecx, %esi
	movl	%r14d, %ecx
	andl	%ebx, %ecx
	movl	%r14d, %edi
	orl	%ebx, %edi
	andl	%r11d, %edi
	orl	%ecx, %edi
	addl	%esi, %edi
	addl	%edx, %r12d
	movl	%r13d, %esi
	roll	$15, %esi
	movl	%r13d, %ecx
	roll	$13, %ecx
	addl	%edx, %edi
	xorl	%esi, %ecx
	movl	%r8d, %edx
	roll	$25, %edx
	movl	%r8d, %esi
	roll	$14, %esi
	xorl	%edx, %esi
	movl	%r8d, %edx
	shrl	$3, %edx
	xorl	%esi, %edx
	addl	%r13d, (%rsp)                   # 4-byte Folded Spill
	addl	%r13d, %edx
	movl	%edx, -64(%rsp)                 # 4-byte Spill
	movl	%r13d, %edx
	shrl	$10, %edx
	xorl	%ecx, %edx
	movl	-120(%rsp), %r15d               # 4-byte Reload
	addl	%edx, %r15d
	movl	%r12d, %ecx
	roll	$26, %ecx
	movl	%r12d, %edx
	roll	$21, %edx
	movl	%r12d, %esi
	roll	$7, %esi
	xorl	%ecx, %edx
	xorl	%edx, %esi
	movl	%r9d, %ecx
	movq	-104(%rsp), %r13                # 8-byte Reload
	xorl	%r13d, %ecx
	andl	%r12d, %ecx
	movq	%r12, -40(%rsp)                 # 8-byte Spill
	xorl	%r13d, %ecx
	addl	%r15d, %r10d
	addl	%ecx, %r10d
	leal	(%rsi,%r10), %edx
	addl	$275423344, %edx                # imm = 0x106AA070
	movl	%edi, %ecx
	roll	$30, %ecx
	movl	%edi, %esi
	roll	$19, %esi
	xorl	%ecx, %esi
	movl	%edi, %ebp
	roll	$10, %ebp
	xorl	%esi, %ebp
	movl	%edi, %esi
	andl	%r14d, %esi
	movl	%edi, %r9d
	orl	%r14d, %r9d
	andl	%ebx, %r9d
	orl	%esi, %r9d
	addl	%ebp, %r9d
	addl	%edx, %r11d
	movl	%r8d, %eax
	movl	%r8d, %esi
	roll	$15, %esi
	addl	%edx, %r9d
	roll	$13, %r8d
	xorl	%esi, %r8d
	movl	%r15d, %edx
	roll	$25, %edx
	movl	%r15d, %esi
	roll	$14, %esi
	xorl	%edx, %esi
	movl	%r15d, %ecx
	shrl	$3, %ecx
	xorl	%esi, %ecx
	addl	%eax, -32(%rsp)                 # 4-byte Folded Spill
	addl	%eax, %ecx
	movq	%rcx, -120(%rsp)                # 8-byte Spill
	movl	%eax, %edx
	shrl	$10, %edx
	xorl	%r8d, %edx
	movq	-128(%rsp), %rcx                # 8-byte Reload
	addl	%edx, %ecx
	movl	%r11d, %edx
	roll	$26, %edx
	movl	%r11d, %esi
	roll	$21, %esi
	xorl	%edx, %esi
	movl	%r11d, %edx
	roll	$7, %edx
	xorl	%esi, %edx
	movl	%r12d, %esi
	movq	-88(%rsp), %r12                 # 8-byte Reload
	xorl	%r12d, %esi
	andl	%r11d, %esi
	xorl	%r12d, %esi
	movq	%r13, %rax
	addl	%ecx, %eax
	addl	%esi, %eax
	leal	(%rdx,%rax), %r10d
	addl	$430227734, %r10d               # imm = 0x19A4C116
	movl	%r9d, %r8d
	roll	$30, %r8d
	movl	%r9d, %edx
	roll	$19, %edx
	movl	%r9d, %esi
	roll	$10, %esi
	xorl	%r8d, %edx
	xorl	%edx, %esi
	movl	%r9d, %edx
	andl	%edi, %edx
	movl	%r9d, %r13d
	orl	%edi, %r13d
	andl	%r14d, %r13d
	orl	%edx, %r13d
	addl	%esi, %r13d
	addl	%r10d, %ebx
	movl	%r15d, %edx
	roll	$15, %edx
	movl	%r15d, %r8d
	roll	$13, %r8d
	addl	%r10d, %r13d
	xorl	%edx, %r8d
	movl	%ecx, %edx
	roll	$25, %edx
	movl	%ecx, %eax
	roll	$14, %eax
	xorl	%edx, %eax
	addl	%ecx, 16(%rsp)                  # 4-byte Folded Spill
	movl	%ecx, %ebp
	movl	%ecx, %esi
	movl	%ecx, %r10d
	shrl	$3, %ecx
	xorl	%eax, %ecx
	addl	%r15d, -28(%rsp)                # 4-byte Folded Spill
	addl	%r15d, %ecx
	movq	%rcx, -128(%rsp)                # 8-byte Spill
	movl	%r15d, %eax
	shrl	$10, %eax
	xorl	%r8d, %eax
	movl	%ebx, %r8d
	roll	$26, %r8d
	movl	-44(%rsp), %r15d                # 4-byte Reload
	addl	%eax, %r15d
	movl	%ebx, %eax
	roll	$21, %eax
	xorl	%r8d, %eax
	movl	%ebx, %r8d
	roll	$7, %r8d
	xorl	%eax, %r8d
	movl	%r11d, %eax
	movq	-40(%rsp), %rdx                 # 8-byte Reload
	xorl	%edx, %eax
	andl	%ebx, %eax
	xorl	%edx, %eax
	addl	%r15d, %r12d
	addl	%eax, %r12d
	movl	%r13d, %ecx
	roll	$30, %ecx
	movl	%r13d, %eax
	roll	$19, %eax
	addl	%r8d, %r12d
	addl	$506948616, %r12d               # imm = 0x1E376C08
	xorl	%ecx, %eax
	movl	%r13d, %ecx
	roll	$10, %ecx
	xorl	%eax, %ecx
	movl	%r13d, %eax
	andl	%r9d, %eax
	movl	%r13d, %r8d
	orl	%r9d, %r8d
	andl	%edi, %r8d
	orl	%eax, %r8d
	addl	%ecx, %r8d
	addl	%r12d, %r14d
	addl	%r12d, %r8d
	roll	$15, %ebp
	roll	$13, %esi
	xorl	%ebp, %esi
	shrl	$10, %r10d
	xorl	%esi, %r10d
	movl	8(%rsp), %ebp                   # 4-byte Reload
	addl	%r10d, %ebp
	movl	%r14d, %eax
	roll	$26, %eax
	movl	%r14d, %ecx
	roll	$21, %ecx
	xorl	%eax, %ecx
	movl	%r14d, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movl	%ebx, %ecx
	xorl	%r11d, %ecx
	andl	%r14d, %ecx
	xorl	%r11d, %ecx
	addl	%ebp, %edx
	addl	%ecx, %edx
	addl	%edx, %eax
	addl	$659060556, %eax                # imm = 0x2748774C
	movl	%r8d, %ecx
	roll	$30, %ecx
	movl	%r8d, %edx
	roll	$19, %edx
	movl	%r8d, %esi
	roll	$10, %esi
	xorl	%ecx, %edx
	xorl	%edx, %esi
	movl	%r8d, %ecx
	andl	%r13d, %ecx
	movl	%r8d, %r12d
	orl	%r13d, %r12d
	andl	%r9d, %r12d
	orl	%ecx, %r12d
	addl	%esi, %r12d
	addl	%eax, %edi
	movl	%r15d, %ecx
	roll	$15, %ecx
	movl	%r15d, %edx
	roll	$13, %edx
	addl	%eax, %r12d
	xorl	%ecx, %edx
	addl	%r15d, -72(%rsp)                # 4-byte Folded Spill
	shrl	$10, %r15d
	xorl	%edx, %r15d
	movl	-24(%rsp), %r10d                # 4-byte Reload
	addl	%r15d, %r10d
	movl	%edi, %eax
	roll	$26, %eax
	movl	%edi, %ecx
	roll	$21, %ecx
	movl	%edi, %edx
	roll	$7, %edx
	xorl	%eax, %ecx
	xorl	%ecx, %edx
	movl	%r14d, %eax
	xorl	%ebx, %eax
	andl	%edi, %eax
	xorl	%ebx, %eax
	addl	%r10d, %r11d
	addl	%eax, %r11d
	leal	(%rdx,%r11), %eax
	addl	$883997877, %eax                # imm = 0x34B0BCB5
	movl	%r12d, %ecx
	roll	$30, %ecx
	movl	%r12d, %edx
	roll	$19, %edx
	xorl	%ecx, %edx
	movl	%r12d, %ecx
	roll	$10, %ecx
	xorl	%edx, %ecx
	movl	%r12d, %edx
	andl	%r8d, %edx
	movl	%r12d, %r11d
	orl	%r8d, %r11d
	andl	%r13d, %r11d
	orl	%edx, %r11d
	addl	%ecx, %r11d
	addl	%eax, %r9d
	movl	%ebp, %ecx
	roll	$15, %ecx
	addl	%eax, %r11d
	movl	%ebp, %eax
	roll	$13, %eax
	xorl	%ecx, %eax
	addl	%ebp, -80(%rsp)                 # 4-byte Folded Spill
	movl	%ebp, %ecx
	shrl	$10, %ecx
	xorl	%eax, %ecx
	movl	(%rsp), %ebp                    # 4-byte Reload
	addl	%ecx, %ebp
	movl	%r9d, %eax
	roll	$26, %eax
	movl	%r9d, %ecx
	roll	$21, %ecx
	xorl	%eax, %ecx
	movl	%r9d, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movl	%edi, %ecx
	xorl	%r14d, %ecx
	andl	%r9d, %ecx
	xorl	%r14d, %ecx
	addl	%ebp, %ebx
	addl	%ecx, %ebx
	addl	%ebx, %eax
	addl	$958139571, %eax                # imm = 0x391C0CB3
	movl	%r11d, %ecx
	roll	$30, %ecx
	movl	%r11d, %edx
	roll	$19, %edx
	movl	%r11d, %esi
	roll	$10, %esi
	xorl	%ecx, %edx
	xorl	%edx, %esi
	movl	%r11d, %ecx
	andl	%r12d, %ecx
	movl	%r11d, %ebx
	orl	%r12d, %ebx
	andl	%r8d, %ebx
	orl	%ecx, %ebx
	addl	%esi, %ebx
	addl	%eax, %r13d
	movl	%r10d, %ecx
	roll	$15, %ecx
	movl	%r10d, %edx
	roll	$13, %edx
	addl	%eax, %ebx
	xorl	%ecx, %edx
	addl	%r10d, -96(%rsp)                # 4-byte Folded Spill
	movl	%r10d, %eax
	shrl	$10, %eax
	xorl	%edx, %eax
	movl	-32(%rsp), %r10d                # 4-byte Reload
	addl	%eax, %r10d
	movl	%r13d, %eax
	roll	$26, %eax
	movl	%r13d, %ecx
	roll	$21, %ecx
	movl	%r13d, %edx
	roll	$7, %edx
	xorl	%eax, %ecx
	xorl	%ecx, %edx
	movl	%r9d, %eax
	xorl	%edi, %eax
	andl	%r13d, %eax
	xorl	%edi, %eax
	addl	%r10d, %r14d
	addl	%eax, %r14d
	leal	(%rdx,%r14), %eax
	addl	$1322822218, %eax               # imm = 0x4ED8AA4A
	movl	%ebx, %ecx
	roll	$30, %ecx
	movl	%ebx, %edx
	roll	$19, %edx
	xorl	%ecx, %edx
	movl	%ebx, %ecx
	roll	$10, %ecx
	xorl	%edx, %ecx
	movl	%ebx, %esi
	andl	%r11d, %esi
	movl	%ebx, %edx
	orl	%r11d, %edx
	andl	%r12d, %edx
	orl	%esi, %edx
	addl	%ecx, %edx
	addl	%eax, %r8d
	movl	%ebp, %ecx
	roll	$15, %ecx
	addl	%eax, %edx
	movl	%ebp, %eax
	roll	$13, %eax
	xorl	%ecx, %eax
	addl	%ebp, -56(%rsp)                 # 4-byte Folded Spill
	movl	%ebp, %ecx
	shrl	$10, %ecx
	xorl	%eax, %ecx
	movl	-28(%rsp), %r15d                # 4-byte Reload
	addl	%ecx, %r15d
	movl	%r8d, %eax
	roll	$26, %eax
	movl	%r8d, %ecx
	roll	$21, %ecx
	xorl	%eax, %ecx
	movl	%r8d, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movl	%r13d, %ecx
	xorl	%r9d, %ecx
	andl	%r8d, %ecx
	xorl	%r9d, %ecx
	addl	%r15d, %edi
	addl	%ecx, %edi
	leal	(%rax,%rdi), %ecx
	addl	$1537002063, %ecx               # imm = 0x5B9CCA4F
	movl	%edx, %eax
	roll	$30, %eax
	movl	%edx, %esi
	roll	$19, %esi
	movl	%edx, %edi
	roll	$10, %edi
	xorl	%eax, %esi
	xorl	%esi, %edi
	movl	%edx, %esi
	andl	%ebx, %esi
	movl	%edx, %ebp
	orl	%ebx, %ebp
	andl	%r11d, %ebp
	orl	%esi, %ebp
	addl	%edi, %ebp
	addl	%ecx, %r12d
	movl	%r10d, %esi
	roll	$15, %esi
	movl	%r10d, %edi
	roll	$13, %edi
	addl	%ecx, %ebp
	xorl	%esi, %edi
	addl	%r10d, -112(%rsp)               # 4-byte Folded Spill
	movl	%r10d, %ecx
	shrl	$10, %ecx
	xorl	%edi, %ecx
	movl	16(%rsp), %r14d                 # 4-byte Reload
	addl	%ecx, %r14d
	movl	%r12d, %ecx
	roll	$26, %ecx
	movl	%r12d, %esi
	roll	$21, %esi
	movl	%r12d, %edi
	roll	$7, %edi
	xorl	%ecx, %esi
	xorl	%esi, %edi
	movl	%r8d, %ecx
	xorl	%r13d, %ecx
	andl	%r12d, %ecx
	xorl	%r13d, %ecx
	addl	%r14d, %r9d
	addl	%ecx, %r9d
	leal	(%rdi,%r9), %esi
	addl	$1747873779, %esi               # imm = 0x682E6FF3
	movl	%ebp, %ecx
	roll	$30, %ecx
	movl	%ebp, %edi
	roll	$19, %edi
	xorl	%ecx, %edi
	movl	%ebp, %eax
	roll	$10, %eax
	xorl	%edi, %eax
	movl	%ebp, %edi
	andl	%edx, %edi
	movl	%ebp, %r10d
	orl	%edx, %r10d
	andl	%ebx, %r10d
	orl	%edi, %r10d
	addl	%eax, %r10d
	addl	%esi, %r11d
	movl	%r15d, %eax
	roll	$15, %eax
	addl	%esi, %r10d
	movl	%r15d, %esi
	roll	$13, %esi
	xorl	%eax, %esi
	addl	%r15d, -64(%rsp)                # 4-byte Folded Spill
	movl	%r15d, %eax
	shrl	$10, %eax
	xorl	%esi, %eax
	movl	-72(%rsp), %r15d                # 4-byte Reload
	addl	%eax, %r15d
	movl	%r11d, %eax
	roll	$26, %eax
	movl	%r11d, %esi
	roll	$21, %esi
	xorl	%eax, %esi
	movl	%r11d, %eax
	roll	$7, %eax
	xorl	%esi, %eax
	movl	%r12d, %esi
	xorl	%r8d, %esi
	andl	%r11d, %esi
	xorl	%r8d, %esi
	addl	%r15d, %r13d
	addl	%esi, %r13d
	addl	%eax, %r13d
	addl	$1955562222, %r13d              # imm = 0x748F82EE
	movl	%r10d, %esi
	roll	$30, %esi
	movl	%r10d, %edi
	roll	$19, %edi
	movl	%r10d, %ecx
	roll	$10, %ecx
	xorl	%esi, %edi
	xorl	%edi, %ecx
	movl	%r10d, %esi
	andl	%ebp, %esi
	movl	%r10d, %r9d
	orl	%ebp, %r9d
	andl	%edx, %r9d
	orl	%esi, %r9d
	addl	%ecx, %r9d
	addl	%r13d, %ebx
	movl	%r14d, %ecx
	roll	$15, %ecx
	movl	%r14d, %esi
	roll	$13, %esi
	addl	%r13d, %r9d
	xorl	%ecx, %esi
	movq	-120(%rsp), %rax                # 8-byte Reload
	addl	%r14d, %eax
	movq	%rax, -120(%rsp)                # 8-byte Spill
	movl	%r14d, %eax
	shrl	$10, %eax
	xorl	%esi, %eax
	movl	-80(%rsp), %r13d                # 4-byte Reload
	addl	%eax, %r13d
	movl	%ebx, %eax
	roll	$26, %eax
	movl	%ebx, %ecx
	roll	$21, %ecx
	movl	%ebx, %esi
	roll	$7, %esi
	xorl	%eax, %ecx
	xorl	%ecx, %esi
	movl	%r11d, %eax
	xorl	%r12d, %eax
	andl	%ebx, %eax
	xorl	%r12d, %eax
	addl	%r13d, %r8d
	addl	%eax, %r8d
	leal	(%rsi,%r8), %eax
	addl	$2024104815, %eax               # imm = 0x78A5636F
	movl	%r9d, %ecx
	roll	$30, %ecx
	movl	%r9d, %esi
	roll	$19, %esi
	xorl	%ecx, %esi
	movl	%r9d, %ecx
	roll	$10, %ecx
	xorl	%esi, %ecx
	movl	%r9d, %esi
	andl	%r10d, %esi
	movl	%r9d, %r14d
	orl	%r10d, %r14d
	andl	%ebp, %r14d
	orl	%esi, %r14d
	addl	%ecx, %r14d
	addl	%eax, %edx
	movl	%r15d, %ecx
	roll	$15, %ecx
	addl	%eax, %r14d
	movl	%r15d, %eax
	roll	$13, %eax
	xorl	%ecx, %eax
	movq	-128(%rsp), %rcx                # 8-byte Reload
	addl	%r15d, %ecx
	movq	%rcx, -128(%rsp)                # 8-byte Spill
	shrl	$10, %r15d
	xorl	%eax, %r15d
	movl	-96(%rsp), %r8d                 # 4-byte Reload
	addl	%r15d, %r8d
	movl	%edx, %eax
	roll	$26, %eax
	movl	%edx, %ecx
	roll	$21, %ecx
	xorl	%eax, %ecx
	movl	%edx, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movl	%ebx, %ecx
	xorl	%r11d, %ecx
	andl	%edx, %ecx
	xorl	%r11d, %ecx
	addl	%r8d, %r12d
	addl	%ecx, %r12d
	addl	%r12d, %eax
	addl	$-2067236844, %eax              # imm = 0x84C87814
	movl	%r14d, %ecx
	roll	$30, %ecx
	movl	%r14d, %esi
	roll	$19, %esi
	movl	%r14d, %edi
	roll	$10, %edi
	xorl	%ecx, %esi
	xorl	%esi, %edi
	movl	%r14d, %ecx
	andl	%r9d, %ecx
	movl	%r14d, %r12d
	orl	%r9d, %r12d
	movq	%r9, -88(%rsp)                  # 8-byte Spill
	andl	%r10d, %r12d
	orl	%ecx, %r12d
	addl	%edi, %r12d
	addl	%eax, %ebp
	movl	%r13d, %edi
	movl	%r13d, %ecx
	roll	$15, %ecx
	movl	%r13d, %esi
	roll	$13, %esi
	addl	%eax, %r12d
	xorl	%ecx, %esi
	shrl	$10, %edi
	xorl	%esi, %edi
	movl	%ebp, %eax
	roll	$26, %eax
	movl	%ebp, %ecx
	roll	$21, %ecx
	movl	-56(%rsp), %esi                 # 4-byte Reload
	addl	%edi, %esi
	xorl	%eax, %ecx
	movl	%ebp, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movl	%edx, %ecx
	xorl	%ebx, %ecx
	andl	%ebp, %ecx
	xorl	%ebx, %ecx
	addl	%esi, %r11d
	movl	%esi, %edi
	addl	%ecx, %r11d
	movl	%r12d, %ecx
	roll	$30, %ecx
	addl	%r11d, %eax
	addl	$-1933114872, %eax              # imm = 0x8CC70208
	movl	%r12d, %esi
	roll	$19, %esi
	xorl	%ecx, %esi
	movl	%r12d, %ecx
	roll	$10, %ecx
	xorl	%esi, %ecx
	movl	%r12d, %esi
	andl	%r14d, %esi
	movl	%r12d, %r11d
	orl	%r14d, %r11d
	andl	%r9d, %r11d
	orl	%esi, %r11d
	addl	%ecx, %r11d
	addl	%eax, %r10d
	addl	%eax, %r11d
	movl	%r8d, %esi
	movl	%r8d, %eax
	roll	$15, %eax
	movl	%r8d, %ecx
	roll	$13, %ecx
	xorl	%eax, %ecx
	shrl	$10, %esi
	xorl	%ecx, %esi
	movl	-112(%rsp), %r8d                # 4-byte Reload
	addl	%esi, %r8d
	movl	%r8d, -112(%rsp)                # 4-byte Spill
	movl	%r10d, %eax
	roll	$26, %eax
	movl	%r10d, %ecx
	roll	$21, %ecx
	movl	%r10d, %esi
	roll	$7, %esi
	xorl	%eax, %ecx
	xorl	%ecx, %esi
	movl	%ebp, %eax
	xorl	%edx, %eax
	andl	%r10d, %eax
	xorl	%edx, %eax
	addl	%r8d, %ebx
	addl	%eax, %ebx
	addl	%esi, %ebx
	addl	$-1866530822, %ebx              # imm = 0x90BEFFFA
	movl	%r11d, %eax
	roll	$30, %eax
	movl	%r11d, %ecx
	roll	$19, %ecx
	xorl	%eax, %ecx
	movl	%r11d, %eax
	roll	$10, %eax
	xorl	%ecx, %eax
	movl	%r11d, %ecx
	andl	%r12d, %ecx
	movl	%r11d, %r13d
	orl	%r12d, %r13d
	andl	%r14d, %r13d
	orl	%ecx, %r13d
	movl	%edi, %ecx
	roll	$15, %ecx
	movl	%edi, %esi
	roll	$13, %esi
	addl	%eax, %r13d
	xorl	%ecx, %esi
	shrl	$10, %edi
	xorl	%esi, %edi
	movl	-64(%rsp), %r9d                 # 4-byte Reload
	addl	%edi, %r9d
	movq	-88(%rsp), %rax                 # 8-byte Reload
	movq	%rbx, -56(%rsp)                 # 8-byte Spill
	leal	(%rbx,%rax), %r15d
	movl	%r15d, %eax
	roll	$26, %eax
	movl	%r15d, %ecx
	roll	$21, %ecx
	movl	%r15d, %edi
	roll	$7, %edi
	xorl	%eax, %ecx
	xorl	%ecx, %edi
	movl	%r10d, %eax
	xorl	%ebp, %eax
	andl	%r15d, %eax
	xorl	%ebp, %eax
	addl	%r9d, %edx
	addl	%eax, %edx
	addl	%r13d, %ebx
	movl	%ebx, %eax
	roll	$30, %eax
	movl	%ebx, %ecx
	roll	$19, %ecx
	leal	(%rdi,%rdx), %esi
	addl	$-1538233109, %esi              # imm = 0xA4506CEB
	xorl	%eax, %ecx
	movl	%ebx, %eax
	roll	$10, %eax
	xorl	%ecx, %eax
	movl	%ebx, %ecx
	andl	%r11d, %ecx
	movl	%ebx, %r8d
	orl	%r11d, %r8d
	andl	%r12d, %r8d
	orl	%ecx, %r8d
	addl	%eax, %r8d
	movl	-112(%rsp), %edx                # 4-byte Reload
	movl	%edx, %eax
	roll	$15, %eax
	movl	%edx, %ecx
	roll	$13, %ecx
	xorl	%eax, %ecx
	shrl	$10, %edx
	xorl	%ecx, %edx
	movl	%edx, -112(%rsp)                # 4-byte Spill
	leal	(%rsi,%r14), %ecx
	movq	%rsi, -64(%rsp)                 # 8-byte Spill
	movl	%ecx, %edx
	roll	$26, %edx
	movl	%ecx, %edi
	roll	$21, %edi
	movl	%ecx, %eax
	roll	$7, %eax
	xorl	%edx, %edi
	xorl	%edi, %eax
	movq	-120(%rsp), %rdi                # 8-byte Reload
	addl	-112(%rsp), %edi                # 4-byte Folded Reload
	addl	%ebp, %edi
	movl	%r15d, %edx
	xorl	%r10d, %edx
	andl	%ecx, %edx
	xorl	%r10d, %edx
	addl	%edx, %edi
	addl	%edi, %eax
	addl	$-1090935817, %eax              # imm = 0xBEF9A3F7
	movq	%rax, -120(%rsp)                # 8-byte Spill
	leal	(%r8,%rsi), %eax
	movl	%eax, %edi
	roll	$30, %edi
	movl	%eax, %ebp
	roll	$19, %ebp
	movl	%eax, %edx
	roll	$10, %edx
	xorl	%edi, %ebp
	xorl	%ebp, %edx
	movl	%eax, %ebp
	andl	%ebx, %ebp
	movl	%eax, %edi
	orl	%ebx, %edi
	andl	%r11d, %edi
	orl	%ebp, %edi
	movl	%r9d, %ebp
	roll	$15, %ebp
	addl	%edx, %edi
	movl	%r9d, %edx
	roll	$13, %edx
	xorl	%ebp, %edx
	shrl	$10, %r9d
	movq	-120(%rsp), %rsi                # 8-byte Reload
	leal	(%rsi,%r12), %ebp
	movl	%ebp, %esi
	roll	$26, %esi
	xorl	%edx, %r9d
	movl	%ebp, %edx
	roll	$21, %edx
	xorl	%esi, %edx
	xorl	%r15d, %ecx
	andl	%ebp, %ecx
	movl	%ebp, %esi
	roll	$7, %esi
	xorl	%edx, %esi
	xorl	%r15d, %ecx
	movq	-128(%rsp), %rdx                # 8-byte Reload
	addl	%r9d, %edx
	addl	%r10d, %edx
	addl	%ecx, %edx
	movq	%rdx, %rbp
	movq	-120(%rsp), %r9                 # 8-byte Reload
	leal	(%rdi,%r9), %ecx
	movl	%ecx, %edx
	roll	$30, %edx
	addl	%ebp, %esi
	addl	$-965641998, %esi               # imm = 0xC67178F2
	movl	%ecx, %ebp
	roll	$19, %ebp
	xorl	%edx, %ebp
	movl	%ecx, %edx
	roll	$10, %edx
	xorl	%ebp, %edx
	movl	%ecx, %ebp
	andl	%eax, %ebp
	orl	%eax, %ecx
	andl	%ebx, %ecx
	orl	%ebp, %ecx
	addl	%edx, %ecx
	leal	(%rcx,%rsi), %eax
	addl	$1779033703, %eax               # imm = 0x6A09E667
	bswapl	%eax
	movq	%rax, %rbp
	leal	-1150833019(%rdi,%r9), %eax
	movq	%r9, %r15
	bswapl	%eax
	movq	%rax, %rbx
	movq	%rax, -104(%rsp)                # 8-byte Spill
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	leal	1013904242(%r8,%rcx), %eax
	bswapl	%eax
	movq	%rax, %rdi
	movq	-56(%rsp), %rdx                 # 8-byte Reload
	leal	-1521486534(%r13,%rdx), %eax
	bswapl	%eax
	movq	%rax, %r10
	movq	%rax, -112(%rsp)                # 8-byte Spill
	leal	1359893119(%r11,%rsi), %r9d
	bswapl	%r9d
	movq	%r9, -128(%rsp)                 # 8-byte Spill
	leal	-1694144372(%r15,%r12), %esi
	bswapl	%esi
	leal	528734635(%rcx,%r14), %r13d
	leal	-829798910(%rbp), %ecx
	movq	%rbp, %r14
	movq	%rbp, -96(%rsp)                 # 8-byte Spill
	roll	$11, %ecx
	movq	-88(%rsp), %rax                 # 8-byte Reload
	leal	(%rdx,%rax), %r15d
	addl	$1541459225, %r15d              # imm = 0x5BE0CD19
	addl	$-1009589776, %ecx              # imm = 0xC3D2E1F0
	movl	%ecx, %eax
	xorl	$79581675, %eax                 # imm = 0x4BE51EB
	leal	(%rbx,%rax), %r12d
	addl	$-1009589776, %r12d             # imm = 0xC3D2E1F0
	roll	$14, %r12d
	addl	$271733878, %r12d               # imm = 0x10325476
	movl	%ecx, %eax
	xorl	%r12d, %eax
	xorl	$917383103, %eax                # imm = 0x36AE27BF
	leal	(%rdi,%rax), %r8d
	addl	$271733878, %r8d                # imm = 0x10325476
	movq	%rdi, %r11
	movq	%rdi, -80(%rsp)                 # 8-byte Spill
	roll	$15, %r8d
	addl	$-344720798, %r8d               # imm = 0xEB73FA62
	roll	$10, %ecx
	movl	%r12d, %eax
	xorl	%ecx, %eax
	xorl	%r8d, %eax
	addl	%eax, %r10d
	addl	$-344720798, %r10d              # imm = 0xEB73FA62
	roll	$12, %r10d
	addl	$917383103, %r10d               # imm = 0x36AE27BF
	roll	$10, %r12d
	movl	%r8d, %eax
	xorl	%r12d, %eax
	xorl	%r10d, %eax
	addl	%eax, %r9d
	addl	$917383103, %r9d                # imm = 0x36AE27BF
	roll	$5, %r9d
	bswapl	%r15d
	addl	%ecx, %r9d
	roll	$10, %r8d
	movl	%r10d, %eax
	xorl	%r8d, %eax
	xorl	%r9d, %eax
	movq	%rsi, -72(%rsp)                 # 8-byte Spill
	addl	%esi, %ecx
	addl	%eax, %ecx
	leal	-937819299(%rsi), %eax
	roll	$8, %eax
	addl	$-1009589776, %eax              # imm = 0xC3D2E1F0
	movl	%eax, %edi
	xorl	$-3297379, %edi                 # imm = 0xFFCDAF9D
	addl	$343240406, %edi                # imm = 0x14756ED6
	roll	$9, %edi
	addl	$271733878, %edi                # imm = 0x10325476
	movl	%eax, %esi
	orl	$-917383104, %esi               # imm = 0xC951D840
	xorl	%edi, %esi
	leal	(%r15,%rsi), %ebp
	addl	$1624563804, %ebp               # imm = 0x60D4E05C
	movq	%r15, -88(%rsp)                 # 8-byte Spill
	roll	$9, %ebp
	roll	$10, %eax
	addl	$-344720798, %ebp               # imm = 0xEB73FA62
	movl	%eax, %esi
	notl	%esi
	orl	%edi, %esi
	xorl	%ebp, %esi
	addl	%esi, %r14d
	addl	$1008109128, %r14d              # imm = 0x3C168648
	roll	$11, %r14d
	addl	$917383103, %r14d               # imm = 0x36AE27BF
	roll	$10, %edi
	movl	%edi, %esi
	notl	%esi
	orl	%ebp, %esi
	xorl	%r14d, %esi
	addl	$-2024754267, %esi              # imm = 0x8750B3A5
	roll	$13, %esi
	addl	%eax, %esi
	roll	$10, %ebp
	movl	%ebp, %ebx
	notl	%ebx
	orl	%r14d, %ebx
	xorl	%esi, %ebx
	addl	%r11d, %eax
	addl	%ebx, %eax
	addl	$1352829926, %eax               # imm = 0x50A28BE6
	movl	%r13d, %r11d
	bswapl	%r11d
	roll	$8, %ecx
	addl	%r12d, %ecx
	roll	$10, %r10d
	movl	%r9d, %ebx
	xorl	%r10d, %ebx
	xorl	%ecx, %ebx
	addl	%r11d, %r12d
	movl	%r11d, %r13d
	movl	%r11d, -120(%rsp)               # 4-byte Spill
	addl	%ebx, %r12d
	roll	$15, %eax
	roll	$10, %r14d
	roll	$7, %r12d
	addl	%edi, %eax
	movl	%r14d, %ebx
	notl	%ebx
	orl	%esi, %ebx
	xorl	%eax, %ebx
	addl	%ebx, %edi
	addl	$1352829926, %edi               # imm = 0x50A28BE6
	addl	%r8d, %r12d
	roll	$10, %r9d
	movl	%ecx, %edx
	xorl	%r9d, %edx
	xorl	%r12d, %edx
	addl	%r15d, %r8d
	addl	%edx, %r8d
	roll	$15, %edi
	roll	$10, %esi
	addl	%ebp, %edi
	movl	%esi, %edx
	notl	%edx
	orl	%eax, %edx
	xorl	%edi, %edx
	addl	-128(%rsp), %ebp                # 4-byte Folded Reload
	leal	(%rdx,%rbp), %ebx
	addl	$1352829926, %ebx               # imm = 0x50A28BE6
	roll	$9, %r8d
	addl	%r10d, %r8d
	roll	$10, %ecx
	movl	%r12d, %edx
	xorl	%ecx, %edx
	xorl	%r8d, %edx
	leal	(%r10,%rdx), %r11d
	addl	$128, %r11d
	roll	$5, %ebx
	roll	$10, %eax
	roll	$11, %r11d
	addl	%r14d, %ebx
	movl	%eax, %edx
	notl	%edx
	orl	%edi, %edx
	xorl	%ebx, %edx
	leal	(%r14,%rdx), %r15d
	addl	$1352829926, %r15d              # imm = 0x50A28BE6
	addl	%r9d, %r11d
	roll	$10, %r12d
	roll	$7, %r15d
	movl	%r8d, %ebp
	xorl	%r12d, %ebp
	xorl	%r11d, %ebp
	addl	%r9d, %ebp
	addl	%esi, %r15d
	roll	$10, %edi
	movl	%edi, %edx
	notl	%edx
	orl	%ebx, %edx
	xorl	%r15d, %edx
	addl	%r13d, %esi
	leal	(%rdx,%rsi), %r9d
	addl	$1352829926, %r9d               # imm = 0x50A28BE6
	roll	$13, %ebp
	addl	%ecx, %ebp
	roll	$10, %r8d
	movl	%r11d, %esi
	xorl	%r8d, %esi
	xorl	%ebp, %esi
	addl	%ecx, %esi
	roll	$7, %r9d
	addl	%eax, %r9d
	roll	$10, %ebx
	movl	%ebx, %ecx
	notl	%ecx
	orl	%r15d, %ecx
	xorl	%r9d, %ecx
	leal	(%rax,%rcx), %r14d
	addl	$1352829926, %r14d              # imm = 0x50A28BE6
	roll	$14, %esi
	addl	%r12d, %esi
	roll	$10, %r11d
	movl	%ebp, %eax
	xorl	%r11d, %eax
	xorl	%esi, %eax
	addl	%r12d, %eax
	roll	$8, %r14d
	roll	$10, %r15d
	roll	$15, %eax
	addl	%edi, %r14d
	movl	%r15d, %ecx
	notl	%ecx
	orl	%r9d, %ecx
	xorl	%r14d, %ecx
	leal	(%rdi,%rcx), %edx
	addl	$1352830054, %edx               # imm = 0x50A28C66
	addl	%r8d, %eax
	roll	$10, %ebp
	roll	$11, %edx
	movl	%esi, %edi
	xorl	%ebp, %edi
	xorl	%eax, %edi
	addl	%r8d, %edi
	addl	%ebx, %edx
	roll	$10, %r9d
	movl	%r9d, %ecx
	notl	%ecx
	orl	%r14d, %ecx
	xorl	%edx, %ecx
	addl	-104(%rsp), %ebx                # 4-byte Folded Reload
	leal	(%rcx,%rbx), %r10d
	addl	$1352829926, %r10d              # imm = 0x50A28BE6
	roll	$6, %edi
	addl	%r11d, %edi
	roll	$10, %esi
	movl	%eax, %ebx
	xorl	%esi, %ebx
	xorl	%edi, %ebx
	addl	%r11d, %ebx
	roll	$14, %r10d
	addl	%r15d, %r10d
	roll	$10, %r14d
	movl	%r14d, %ecx
	notl	%ecx
	orl	%edx, %ecx
	xorl	%r10d, %ecx
	leal	(%r15,%rcx), %r11d
	addl	$1352829926, %r11d              # imm = 0x50A28BE6
	roll	$7, %ebx
	addl	%ebp, %ebx
	roll	$10, %eax
	movl	%edi, %ecx
	xorl	%eax, %ecx
	xorl	%ebx, %ecx
	leal	(%rcx,%rbp), %r8d
	addl	$256, %r8d                      # imm = 0x100
	roll	$14, %r11d
	addl	%r9d, %r11d
	roll	$10, %edx
	movl	%edx, %ecx
	notl	%ecx
	orl	%r10d, %ecx
	xorl	%r11d, %ecx
	addl	-112(%rsp), %r9d                # 4-byte Folded Reload
	addl	%r9d, %ecx
	addl	$1352829926, %ecx               # imm = 0x50A28BE6
	roll	$9, %r8d
	roll	$10, %edi
	addl	%esi, %r8d
	movl	%ebx, %ebp
	xorl	%edi, %ebp
	xorl	%r8d, %ebp
	addl	%esi, %ebp
	roll	$12, %ecx
	roll	$10, %r10d
	addl	%r14d, %ecx
	movl	%r10d, %esi
	notl	%esi
	orl	%r11d, %esi
	xorl	%ecx, %esi
	leal	(%r14,%rsi), %r13d
	addl	$1352829926, %r13d              # imm = 0x50A28BE6
	roll	$8, %ebp
	addl	%eax, %ebp
	roll	$10, %ebx
	movl	%ebp, %esi
	notl	%esi
	andl	%ebx, %esi
	addl	-88(%rsp), %eax                 # 4-byte Folded Reload
	addl	%esi, %eax
	movl	%ebp, %esi
	andl	%r8d, %esi
	leal	(%rsi,%rax), %r9d
	addl	$1518500249, %r9d               # imm = 0x5A827999
	roll	$6, %r13d
	roll	$10, %r11d
	addl	%edx, %r13d
	movl	%r11d, %eax
	notl	%eax
	andl	%ecx, %eax
	addl	-120(%rsp), %edx                # 4-byte Folded Reload
	addl	%eax, %edx
	roll	$7, %r9d
	movl	%r13d, %eax
	andl	%r11d, %eax
	leal	(%rax,%rdx), %r15d
	addl	$1548603684, %r15d              # imm = 0x5C4DD124
	addl	%edi, %r9d
	roll	$10, %r8d
	movl	%r9d, %eax
	notl	%eax
	andl	%r8d, %eax
	addl	-128(%rsp), %edi                # 4-byte Folded Reload
	addl	%eax, %edi
	movl	%r9d, %eax
	andl	%ebp, %eax
	leal	(%rax,%rdi), %r14d
	addl	$1518500249, %r14d              # imm = 0x5A827999
	roll	$9, %r15d
	addl	%r10d, %r15d
	roll	$10, %ecx
	movl	%ecx, %eax
	notl	%eax
	andl	%r13d, %eax
	addl	%r10d, %eax
	movl	%r15d, %edx
	andl	%ecx, %edx
	addl	%edx, %eax
	addl	$1548603684, %eax               # imm = 0x5C4DD124
	roll	$6, %r14d
	roll	$10, %ebp
	addl	%ebx, %r14d
	movl	%r14d, %edx
	notl	%edx
	andl	%ebp, %edx
	addl	%ebx, %edx
	movl	%r14d, %esi
	andl	%r9d, %esi
	leal	(%rsi,%rdx), %r12d
	addl	$1518500249, %r12d              # imm = 0x5A827999
	roll	$13, %eax
	addl	%r11d, %eax
	roll	$10, %r13d
	movl	%r13d, %edx
	notl	%edx
	andl	%r15d, %edx
	addl	-112(%rsp), %r11d               # 4-byte Folded Reload
	addl	%edx, %r11d
	movl	%eax, %edx
	andl	%r13d, %edx
	addl	%edx, %r11d
	addl	$1548603684, %r11d              # imm = 0x5C4DD124
	roll	$8, %r12d
	addl	%r8d, %r12d
	roll	$10, %r9d
	movl	%r12d, %edx
	notl	%edx
	andl	%r9d, %edx
	addl	-104(%rsp), %r8d                # 4-byte Folded Reload
	addl	%edx, %r8d
	movl	%r12d, %edx
	andl	%r14d, %edx
	leal	(%rdx,%r8), %r10d
	addl	$1518500249, %r10d              # imm = 0x5A827999
	roll	$15, %r11d
	roll	$10, %r15d
	addl	%ecx, %r11d
	movl	%r15d, %edx
	notl	%edx
	andl	%eax, %edx
	addl	-88(%rsp), %ecx                 # 4-byte Folded Reload
	addl	%edx, %ecx
	roll	$13, %r10d
	movl	%r11d, %edx
	andl	%r15d, %edx
	leal	(%rdx,%rcx), %r8d
	addl	$1548603684, %r8d               # imm = 0x5C4DD124
	addl	%ebp, %r10d
	roll	$10, %r14d
	movl	%r10d, %ecx
	notl	%ecx
	andl	%r14d, %ecx
	addl	%ebp, %ecx
	movl	%r10d, %edx
	andl	%r12d, %edx
	leal	(%rdx,%rcx), %ebx
	addl	$1518500249, %ebx               # imm = 0x5A827999
	roll	$7, %r8d
	addl	%r13d, %r8d
	roll	$10, %eax
	movl	%eax, %ecx
	notl	%ecx
	andl	%r11d, %ecx
	addl	-96(%rsp), %r13d                # 4-byte Folded Reload
	addl	%ecx, %r13d
	movl	%r8d, %ecx
	andl	%eax, %ecx
	leal	(%rcx,%r13), %edx
	addl	$1548603684, %edx               # imm = 0x5C4DD124
	roll	$11, %ebx
	roll	$10, %r12d
	addl	%r9d, %ebx
	movl	%ebx, %ecx
	notl	%ecx
	andl	%r12d, %ecx
	addl	-120(%rsp), %r9d                # 4-byte Folded Reload
	addl	%ecx, %r9d
	roll	$12, %edx
	movl	%ebx, %ecx
	andl	%r10d, %ecx
	addl	%ecx, %r9d
	addl	$1518500249, %r9d               # imm = 0x5A827999
	addl	%r15d, %edx
	roll	$10, %r11d
	movl	%r11d, %ecx
	notl	%ecx
	andl	%r8d, %ecx
	addl	%r15d, %ecx
	movl	%edx, %esi
	andl	%r11d, %esi
	leal	(%rsi,%rcx), %r15d
	addl	$1548603684, %r15d              # imm = 0x5C4DD124
	roll	$9, %r9d
	addl	%r14d, %r9d
	roll	$10, %r10d
	movl	%r9d, %ecx
	notl	%ecx
	andl	%r10d, %ecx
	addl	%r14d, %ecx
	roll	$8, %r15d
	movl	%r9d, %esi
	andl	%ebx, %esi
	leal	(%rsi,%rcx), %edi
	addl	$1518500249, %edi               # imm = 0x5A827999
	addl	%eax, %r15d
	roll	$10, %r8d
	movl	%r8d, %ecx
	notl	%ecx
	andl	%edx, %ecx
	movq	-72(%rsp), %r13                 # 8-byte Reload
	addl	%r13d, %eax
	addl	%ecx, %eax
	movl	%r15d, %ecx
	andl	%r8d, %ecx
	leal	(%rcx,%rax), %r14d
	addl	$1548603684, %r14d              # imm = 0x5C4DD124
	roll	$7, %edi
	addl	%r12d, %edi
	roll	$10, %ebx
	movl	%edi, %eax
	notl	%eax
	andl	%ebx, %eax
	addl	-112(%rsp), %r12d               # 4-byte Folded Reload
	addl	%eax, %r12d
	movl	%edi, %eax
	andl	%r9d, %eax
	leal	(%rax,%r12), %ebp
	addl	$1518500249, %ebp               # imm = 0x5A827999
	roll	$9, %r14d
	addl	%r11d, %r14d
	roll	$10, %edx
	movl	%edx, %eax
	notl	%eax
	andl	%r15d, %eax
	addl	%r11d, %eax
	roll	$15, %ebp
	movl	%r14d, %ecx
	andl	%edx, %ecx
	addl	%eax, %ecx
	addl	$1548603684, %ecx               # imm = 0x5C4DD124
	addl	%r10d, %ebp
	roll	$10, %r9d
	movl	%ebp, %eax
	notl	%eax
	andl	%r9d, %eax
	addl	%r10d, %eax
	movl	%ebp, %esi
	andl	%edi, %esi
	leal	(%rsi,%rax), %r10d
	addl	$1518500249, %r10d              # imm = 0x5A827999
	roll	$11, %ecx
	addl	%r8d, %ecx
	roll	$10, %r15d
	movl	%r15d, %eax
	notl	%eax
	andl	%r14d, %eax
	addl	%r8d, %eax
	roll	$7, %r10d
	movl	%ecx, %esi
	andl	%r15d, %esi
	leal	(%rsi,%rax), %r8d
	addl	$1548603940, %r8d               # imm = 0x5C4DD224
	addl	%ebx, %r10d
	roll	$10, %edi
	movl	%r10d, %eax
	notl	%eax
	andl	%edi, %eax
	addl	-96(%rsp), %ebx                 # 4-byte Folded Reload
	addl	%eax, %ebx
	movl	%r10d, %eax
	andl	%ebp, %eax
	leal	(%rax,%rbx), %r11d
	addl	$1518500249, %r11d              # imm = 0x5A827999
	roll	$7, %r8d
	addl	%edx, %r8d
	roll	$10, %r14d
	movl	%r14d, %eax
	notl	%eax
	andl	%ecx, %eax
	addl	%edx, %eax
	movl	%r8d, %edx
	andl	%r14d, %edx
	addl	%eax, %edx
	addl	$1548603684, %edx               # imm = 0x5C4DD124
	roll	$12, %r11d
	roll	$10, %ebp
	addl	%r9d, %r11d
	movl	%r11d, %eax
	notl	%eax
	andl	%ebp, %eax
	addl	%r9d, %eax
	movl	%r11d, %esi
	andl	%r10d, %esi
	leal	(%rsi,%rax), %r9d
	addl	$1518500249, %r9d               # imm = 0x5A827999
	roll	$7, %edx
	addl	%r15d, %edx
	roll	$10, %ecx
	movl	%ecx, %eax
	notl	%eax
	andl	%r8d, %eax
	addl	%r15d, %eax
	movl	%edx, %esi
	andl	%ecx, %esi
	addl	%esi, %eax
	addl	$1548603812, %eax               # imm = 0x5C4DD1A4
	roll	$15, %r9d
	roll	$10, %r10d
	addl	%edi, %r9d
	movl	%r9d, %esi
	notl	%esi
	andl	%r10d, %esi
	addl	%r13d, %edi
	addl	%esi, %edi
	roll	$12, %eax
	movl	%r9d, %esi
	andl	%r11d, %esi
	addl	%esi, %edi
	addl	$1518500249, %edi               # imm = 0x5A827999
	addl	%r14d, %eax
	roll	$10, %r8d
	movl	%r8d, %esi
	notl	%esi
	andl	%edx, %esi
	addl	%r14d, %esi
	movl	%eax, %ebx
	andl	%r8d, %ebx
	leal	(%rbx,%rsi), %r14d
	addl	$1548603684, %r14d              # imm = 0x5C4DD124
	roll	$9, %edi
	addl	%ebp, %edi
	roll	$10, %r11d
	movl	%edi, %esi
	notl	%esi
	andl	%r11d, %esi
	movq	-80(%rsp), %r13                 # 8-byte Reload
	addl	%r13d, %ebp
	addl	%esi, %ebp
	movl	%edi, %esi
	andl	%r9d, %esi
	leal	(%rsi,%rbp), %r15d
	addl	$1518500249, %r15d              # imm = 0x5A827999
	roll	$7, %r14d
	roll	$10, %edx
	addl	%ecx, %r14d
	movl	%edx, %esi
	notl	%esi
	andl	%eax, %esi
	addl	-128(%rsp), %ecx                # 4-byte Folded Reload
	addl	%esi, %ecx
	roll	$11, %r15d
	movl	%r14d, %esi
	andl	%edx, %esi
	leal	(%rsi,%rcx), %ebx
	addl	$1548603684, %ebx               # imm = 0x5C4DD124
	addl	%r10d, %r15d
	roll	$10, %r9d
	movl	%r15d, %esi
	notl	%esi
	andl	%r9d, %esi
	addl	%r10d, %esi
	movl	%r15d, %ebp
	andl	%edi, %ebp
	leal	(%rsi,%rbp), %r10d
	addl	$1518500505, %r10d              # imm = 0x5A827A99
	roll	$6, %ebx
	addl	%r8d, %ebx
	roll	$10, %eax
	movl	%eax, %esi
	notl	%esi
	andl	%r14d, %esi
	addl	%r8d, %esi
	roll	$7, %r10d
	movl	%ebx, %ecx
	andl	%eax, %ecx
	leal	(%rcx,%rsi), %r12d
	addl	$1548603684, %r12d              # imm = 0x5C4DD124
	addl	%r11d, %r10d
	roll	$10, %edi
	movl	%r10d, %esi
	notl	%esi
	andl	%edi, %esi
	addl	%r11d, %esi
	movl	%r10d, %ebp
	andl	%r15d, %ebp
	leal	(%rsi,%rbp), %r11d
	addl	$1518500249, %r11d              # imm = 0x5A827999
	roll	$15, %r12d
	addl	%edx, %r12d
	roll	$10, %r14d
	movl	%r14d, %esi
	notl	%esi
	andl	%ebx, %esi
	addl	-104(%rsp), %edx                # 4-byte Folded Reload
	addl	%esi, %edx
	movl	%r12d, %esi
	andl	%r14d, %esi
	addl	%esi, %edx
	addl	$1548603684, %edx               # imm = 0x5C4DD124
	roll	$13, %r11d
	roll	$10, %r15d
	addl	%r9d, %r11d
	movl	%r11d, %esi
	notl	%esi
	movl	%r15d, %ebp
	andl	%esi, %ebp
	addl	%r9d, %ebp
	movl	%r11d, %ecx
	andl	%r10d, %ecx
	leal	(%rcx,%rbp), %r8d
	addl	$1518500377, %r8d               # imm = 0x5A827A19
	roll	$13, %edx
	addl	%eax, %edx
	roll	$10, %ebx
	movl	%ebx, %ecx
	notl	%ecx
	andl	%r12d, %ecx
	addl	%r13d, %eax
	addl	%ecx, %eax
	movl	%edx, %ecx
	andl	%ebx, %ecx
	leal	(%rcx,%rax), %ebp
	addl	$1548603684, %ebp               # imm = 0x5C4DD124
	roll	$12, %r8d
	addl	%edi, %r8d
	roll	$10, %r10d
	orl	%r8d, %esi
	xorl	%r10d, %esi
	addl	-112(%rsp), %edi                # 4-byte Folded Reload
	addl	%esi, %edi
	addl	$1859775393, %edi               # imm = 0x6ED9EBA1
	roll	$11, %ebp
	roll	$10, %r12d
	roll	$11, %edi
	addl	%r14d, %ebp
	movl	%edx, %eax
	notl	%eax
	orl	%ebp, %eax
	xorl	%r12d, %eax
	leal	(%r14,%rax), %r9d
	addl	$1836072691, %r9d               # imm = 0x6D703EF3
	addl	%r15d, %edi
	roll	$10, %r11d
	movl	%r8d, %eax
	notl	%eax
	orl	%edi, %eax
	xorl	%r11d, %eax
	leal	(%r15,%rax), %r14d
	addl	$1859775393, %r14d              # imm = 0x6ED9EBA1
	roll	$9, %r9d
	roll	$10, %edx
	addl	%ebx, %r9d
	movl	%ebp, %eax
	notl	%eax
	orl	%r9d, %eax
	xorl	%edx, %eax
	addl	-72(%rsp), %ebx                 # 4-byte Folded Reload
	leal	(%rax,%rbx), %esi
	addl	$1836072691, %esi               # imm = 0x6D703EF3
	roll	$13, %r14d
	roll	$10, %r8d
	roll	$7, %esi
	addl	%r10d, %r14d
	movl	%edi, %eax
	notl	%eax
	orl	%r14d, %eax
	xorl	%r8d, %eax
	addl	%r10d, %eax
	addl	$1859775649, %eax               # imm = 0x6ED9ECA1
	addl	%r12d, %esi
	roll	$10, %ebp
	movl	%r9d, %ecx
	notl	%ecx
	orl	%esi, %ecx
	xorl	%ebp, %ecx
	addl	-104(%rsp), %r12d               # 4-byte Folded Reload
	leal	(%rcx,%r12), %r10d
	addl	$1836072691, %r10d              # imm = 0x6D703EF3
	roll	$6, %eax
	addl	%r11d, %eax
	roll	$10, %edi
	roll	$15, %r10d
	movl	%r14d, %ecx
	notl	%ecx
	orl	%eax, %ecx
	xorl	%edi, %ecx
	addl	-128(%rsp), %r11d               # 4-byte Folded Reload
	leal	(%rcx,%r11), %ebx
	addl	$1859775393, %ebx               # imm = 0x6ED9EBA1
	addl	%edx, %r10d
	roll	$10, %r9d
	movl	%esi, %ecx
	notl	%ecx
	orl	%r10d, %ecx
	xorl	%r9d, %ecx
	addl	-112(%rsp), %edx                # 4-byte Folded Reload
	leal	(%rcx,%rdx), %r15d
	addl	$1836072691, %r15d              # imm = 0x6D703EF3
	roll	$7, %ebx
	addl	%r8d, %ebx
	roll	$10, %r14d
	movl	%eax, %ecx
	notl	%ecx
	orl	%ebx, %ecx
	xorl	%r14d, %ecx
	addl	%r8d, %ecx
	addl	$1859775393, %ecx               # imm = 0x6ED9EBA1
	roll	$11, %r15d
	addl	%ebp, %r15d
	roll	$10, %esi
	movl	%r10d, %edx
	notl	%edx
	orl	%r15d, %edx
	xorl	%esi, %edx
	movq	-88(%rsp), %r13                 # 8-byte Reload
	addl	%r13d, %ebp
	leal	(%rdx,%rbp), %r12d
	addl	$1836072691, %r12d              # imm = 0x6D703EF3
	roll	$14, %ecx
	roll	$10, %eax
	addl	%edi, %ecx
	movl	%ebx, %edx
	notl	%edx
	orl	%ecx, %edx
	xorl	%eax, %edx
	leal	(%rdi,%rdx), %ebp
	addl	$1859775393, %ebp               # imm = 0x6ED9EBA1
	roll	$8, %r12d
	addl	%r9d, %r12d
	roll	$10, %r10d
	movl	%r15d, %edx
	notl	%edx
	orl	%r12d, %edx
	xorl	%r10d, %edx
	leal	(%r9,%rdx), %r8d
	addl	$1836072947, %r8d               # imm = 0x6D703FF3
	roll	$9, %ebp
	roll	$10, %ebx
	roll	$6, %r8d
	addl	%r14d, %ebp
	movl	%ecx, %edx
	notl	%edx
	orl	%ebp, %edx
	xorl	%ebx, %edx
	leal	(%r14,%rdx), %edi
	addl	$1859775521, %edi               # imm = 0x6ED9EC21
	addl	%esi, %r8d
	roll	$10, %r15d
	movl	%r12d, %edx
	notl	%edx
	orl	%r8d, %edx
	xorl	%r15d, %edx
	addl	-120(%rsp), %esi                # 4-byte Folded Reload
	addl	%esi, %edx
	addl	$1836072691, %edx               # imm = 0x6D703EF3
	roll	$13, %edi
	addl	%eax, %edi
	roll	$10, %ecx
	roll	$6, %edx
	movl	%ebp, %esi
	notl	%esi
	orl	%edi, %esi
	xorl	%ecx, %esi
	addl	-104(%rsp), %eax                # 4-byte Folded Reload
	leal	(%rsi,%rax), %r11d
	addl	$1859775393, %r11d              # imm = 0x6ED9EBA1
	addl	%r10d, %edx
	roll	$10, %r12d
	movl	%r8d, %eax
	notl	%eax
	orl	%edx, %eax
	xorl	%r12d, %eax
	leal	(%r10,%rax), %r9d
	addl	$1836072691, %r9d               # imm = 0x6D703EF3
	roll	$15, %r11d
	roll	$10, %ebp
	addl	%ebx, %r11d
	movl	%edi, %eax
	notl	%eax
	orl	%r11d, %eax
	xorl	%ebp, %eax
	movq	-80(%rsp), %r14                 # 8-byte Reload
	addl	%r14d, %ebx
	leal	(%rax,%rbx), %r10d
	addl	$1859775393, %r10d              # imm = 0x6ED9EBA1
	roll	$14, %r9d
	roll	$10, %r8d
	roll	$14, %r10d
	addl	%r15d, %r9d
	movl	%edx, %eax
	notl	%eax
	orl	%r9d, %eax
	xorl	%r8d, %eax
	leal	(%r15,%rax), %ebx
	addl	$1836072691, %ebx               # imm = 0x6D703EF3
	addl	%ecx, %r10d
	roll	$10, %edi
	movl	%r11d, %eax
	notl	%eax
	orl	%r10d, %eax
	xorl	%edi, %eax
	addl	%r13d, %ecx
	leal	(%rax,%rcx), %esi
	addl	$1859775393, %esi               # imm = 0x6ED9EBA1
	roll	$12, %ebx
	addl	%r12d, %ebx
	roll	$10, %edx
	movl	%r9d, %eax
	notl	%eax
	orl	%ebx, %eax
	xorl	%edx, %eax
	addl	%r12d, %eax
	addl	$1836072819, %eax               # imm = 0x6D703F73
	roll	$8, %esi
	addl	%ebp, %esi
	roll	$10, %r11d
	movl	%r10d, %ecx
	notl	%ecx
	orl	%esi, %ecx
	xorl	%r11d, %ecx
	movq	-96(%rsp), %r13                 # 8-byte Reload
	addl	%r13d, %ebp
	leal	(%rcx,%rbp), %r15d
	addl	$1859775393, %r15d              # imm = 0x6ED9EBA1
	roll	$13, %eax
	roll	$10, %r9d
	addl	%r8d, %eax
	movl	%ebx, %ecx
	notl	%ecx
	orl	%eax, %ecx
	xorl	%r9d, %ecx
	addl	%ecx, %r8d
	addl	$1836072691, %r8d               # imm = 0x6D703EF3
	roll	$13, %r15d
	addl	%edi, %r15d
	roll	$10, %r10d
	roll	$5, %r8d
	movl	%esi, %ecx
	notl	%ecx
	orl	%r15d, %ecx
	xorl	%r10d, %ecx
	addl	-120(%rsp), %edi                # 4-byte Folded Reload
	addl	%ecx, %edi
	addl	$1859775393, %edi               # imm = 0x6ED9EBA1
	addl	%edx, %r8d
	roll	$10, %ebx
	movl	%eax, %ecx
	notl	%ecx
	orl	%r8d, %ecx
	xorl	%ebx, %ecx
	addl	%r14d, %edx
	leal	(%rcx,%rdx), %r12d
	addl	$1836072691, %r12d              # imm = 0x6D703EF3
	roll	$6, %edi
	addl	%r11d, %edi
	roll	$10, %esi
	movl	%r15d, %ecx
	notl	%ecx
	orl	%edi, %ecx
	xorl	%esi, %ecx
	addl	%ecx, %r11d
	addl	$1859775393, %r11d              # imm = 0x6ED9EBA1
	roll	$14, %r12d
	roll	$10, %eax
	roll	$5, %r11d
	addl	%r9d, %r12d
	movl	%r8d, %ecx
	notl	%ecx
	orl	%r12d, %ecx
	xorl	%eax, %ecx
	addl	%r9d, %ecx
	addl	$1836072691, %ecx               # imm = 0x6D703EF3
	addl	%r10d, %r11d
	roll	$10, %r15d
	movl	%edi, %edx
	notl	%edx
	orl	%r11d, %edx
	xorl	%r15d, %edx
	leal	(%r10,%rdx), %r14d
	addl	$1859775393, %r14d              # imm = 0x6ED9EBA1
	roll	$13, %ecx
	roll	$10, %r8d
	addl	%ebx, %ecx
	movl	%r12d, %edx
	notl	%edx
	orl	%ecx, %edx
	xorl	%r8d, %edx
	addl	%r13d, %ebx
	addl	%edx, %ebx
	addl	$1836072691, %ebx               # imm = 0x6D703EF3
	roll	$12, %r14d
	addl	%esi, %r14d
	roll	$10, %edi
	movl	%r11d, %edx
	notl	%edx
	orl	%r14d, %edx
	xorl	%edi, %edx
	addl	-72(%rsp), %esi                 # 4-byte Folded Reload
	leal	(%rdx,%rsi), %r10d
	addl	$1859775393, %r10d              # imm = 0x6ED9EBA1
	roll	$13, %ebx
	roll	$10, %r12d
	addl	%eax, %ebx
	movl	%ecx, %edx
	notl	%edx
	orl	%ebx, %edx
	xorl	%r12d, %edx
	addl	-128(%rsp), %eax                # 4-byte Folded Reload
	leal	(%rdx,%rax), %ebp
	addl	$1836072691, %ebp               # imm = 0x6D703EF3
	roll	$7, %r10d
	roll	$10, %r11d
	roll	$7, %ebp
	addl	%r15d, %r10d
	movl	%r14d, %eax
	notl	%eax
	orl	%r10d, %eax
	xorl	%r11d, %eax
	leal	(%r15,%rax), %edx
	addl	$1859775393, %edx               # imm = 0x6ED9EBA1
	addl	%r8d, %ebp
	roll	$10, %ecx
	movl	%ebx, %eax
	notl	%eax
	orl	%ebp, %eax
	xorl	%ecx, %eax
	leal	(%r8,%rax), %r15d
	addl	$1836072691, %r15d              # imm = 0x6D703EF3
	roll	$5, %edx
	roll	$10, %r14d
	addl	%edi, %edx
	movl	%r14d, %esi
	notl	%esi
	andl	%r10d, %esi
	addl	-104(%rsp), %edi                # 4-byte Folded Reload
	addl	%esi, %edi
	roll	$5, %r15d
	movl	%edx, %esi
	andl	%r14d, %esi
	leal	(%rsi,%rdi), %r13d
	addl	$-1894007588, %r13d             # imm = 0x8F1BBCDC
	addl	%r12d, %r15d
	roll	$10, %ebx
	movl	%r15d, %edi
	notl	%edi
	andl	%ebx, %edi
	addl	%r12d, %edi
	movl	%r15d, %eax
	andl	%ebp, %eax
	addl	%eax, %edi
	addl	$2053994345, %edi               # imm = 0x7A6D7769
	roll	$11, %r13d
	addl	%r11d, %r13d
	roll	$10, %r10d
	movl	%r10d, %eax
	notl	%eax
	andl	%edx, %eax
	addl	%r11d, %eax
	roll	$15, %edi
	movl	%r13d, %esi
	andl	%r10d, %esi
	leal	(%rsi,%rax), %r8d
	addl	$-1894007588, %r8d              # imm = 0x8F1BBCDC
	addl	%ecx, %edi
	roll	$10, %ebp
	movl	%edi, %eax
	notl	%eax
	andl	%ebp, %eax
	addl	-120(%rsp), %ecx                # 4-byte Folded Reload
	addl	%eax, %ecx
	movl	%edi, %eax
	andl	%r15d, %eax
	leal	(%rax,%rcx), %r9d
	addl	$2053994217, %r9d               # imm = 0x7A6D76E9
	roll	$12, %r8d
	addl	%r14d, %r8d
	roll	$10, %edx
	movl	%edx, %eax
	notl	%eax
	andl	%r13d, %eax
	addl	%r14d, %eax
	movl	%r8d, %ecx
	andl	%edx, %ecx
	addl	%eax, %ecx
	addl	$-1894007588, %ecx              # imm = 0x8F1BBCDC
	roll	$5, %r9d
	roll	$10, %r15d
	addl	%ebx, %r9d
	movl	%r9d, %eax
	notl	%eax
	andl	%r15d, %eax
	addl	-128(%rsp), %ebx                # 4-byte Folded Reload
	addl	%eax, %ebx
	roll	$14, %ecx
	movl	%r9d, %eax
	andl	%edi, %eax
	addl	%eax, %ebx
	addl	$2053994217, %ebx               # imm = 0x7A6D76E9
	addl	%r10d, %ecx
	roll	$10, %r13d
	movl	%r13d, %eax
	notl	%eax
	andl	%r8d, %eax
	addl	%r10d, %eax
	movl	%ecx, %esi
	andl	%r13d, %esi
	leal	(%rsi,%rax), %r14d
	addl	$-1894007588, %r14d             # imm = 0x8F1BBCDC
	roll	$8, %ebx
	addl	%ebp, %ebx
	roll	$10, %edi
	movl	%ebx, %eax
	notl	%eax
	andl	%edi, %eax
	addl	-104(%rsp), %ebp                # 4-byte Folded Reload
	addl	%eax, %ebp
	movl	%ebx, %eax
	andl	%r9d, %eax
	addl	%eax, %ebp
	addl	$2053994217, %ebp               # imm = 0x7A6D76E9
	roll	$15, %r14d
	roll	$10, %r8d
	addl	%edx, %r14d
	movl	%r8d, %eax
	notl	%eax
	andl	%ecx, %eax
	addl	-96(%rsp), %edx                 # 4-byte Folded Reload
	addl	%eax, %edx
	roll	$11, %ebp
	movl	%r14d, %eax
	andl	%r8d, %eax
	addl	%eax, %edx
	addl	$-1894007588, %edx              # imm = 0x8F1BBCDC
	addl	%r15d, %ebp
	roll	$10, %r9d
	movl	%ebp, %eax
	notl	%eax
	andl	%r9d, %eax
	addl	-112(%rsp), %r15d               # 4-byte Folded Reload
	addl	%eax, %r15d
	movl	%ebp, %eax
	andl	%ebx, %eax
	leal	(%rax,%r15), %r10d
	addl	$2053994217, %r10d              # imm = 0x7A6D76E9
	roll	$14, %edx
	addl	%r13d, %edx
	roll	$10, %ecx
	movl	%ecx, %eax
	notl	%eax
	andl	%r14d, %eax
	addl	%r13d, %eax
	movl	%edx, %esi
	andl	%ecx, %esi
	leal	(%rsi,%rax), %r15d
	addl	$-1894007460, %r15d             # imm = 0x8F1BBD5C
	roll	$14, %r10d
	roll	$10, %ebx
	addl	%edi, %r10d
	movl	%r10d, %eax
	notl	%eax
	andl	%ebx, %eax
	addl	%edi, %eax
	movl	%r10d, %edi
	andl	%ebp, %edi
	leal	(%rdi,%rax), %r12d
	addl	$2053994217, %r12d              # imm = 0x7A6D76E9
	roll	$15, %r15d
	addl	%r8d, %r15d
	roll	$10, %r14d
	movl	%r14d, %edi
	notl	%edi
	andl	%edx, %edi
	addl	%r8d, %edi
	movl	%r15d, %esi
	andl	%r14d, %esi
	addl	%esi, %edi
	addl	$-1894007588, %edi              # imm = 0x8F1BBCDC
	roll	$14, %r12d
	roll	$10, %ebp
	addl	%r9d, %r12d
	movl	%r12d, %esi
	notl	%esi
	andl	%ebp, %esi
	addl	%r9d, %esi
	movl	%r12d, %eax
	andl	%r10d, %eax
	leal	(%rax,%rsi), %r11d
	addl	$2053994217, %r11d              # imm = 0x7A6D76E9
	roll	$9, %edi
	addl	%ecx, %edi
	roll	$10, %edx
	movl	%edx, %eax
	notl	%eax
	andl	%r15d, %eax
	addl	-128(%rsp), %ecx                # 4-byte Folded Reload
	addl	%eax, %ecx
	movl	%edi, %eax
	andl	%edx, %eax
	addl	%eax, %ecx
	addl	$-1894007588, %ecx              # imm = 0x8F1BBCDC
	roll	$6, %r11d
	addl	%ebx, %r11d
	roll	$10, %r10d
	movl	%r11d, %eax
	notl	%eax
	andl	%r10d, %eax
	addl	-96(%rsp), %ebx                 # 4-byte Folded Reload
	addl	%eax, %ebx
	movl	%r11d, %eax
	andl	%r12d, %eax
	leal	(%rax,%rbx), %r9d
	addl	$2053994217, %r9d               # imm = 0x7A6D76E9
	roll	$8, %ecx
	roll	$10, %r15d
	addl	%r14d, %ecx
	movl	%ecx, %r8d
	movl	%r15d, %eax
	notl	%eax
	andl	%edi, %eax
	addl	%r14d, %eax
	andl	%r15d, %ecx
	addl	%eax, %ecx
	addl	$-1894007588, %ecx              # imm = 0x8F1BBCDC
	roll	$14, %r9d
	addl	%ebp, %r9d
	roll	$10, %r12d
	movl	%r9d, %eax
	notl	%eax
	andl	%r12d, %eax
	addl	-72(%rsp), %ebp                 # 4-byte Folded Reload
	addl	%eax, %ebp
	movl	%r9d, %eax
	andl	%r11d, %eax
	leal	(%rax,%rbp), %ebx
	addl	$2053994217, %ebx               # imm = 0x7A6D76E9
	roll	$9, %ecx
	addl	%edx, %ecx
	roll	$10, %edi
	movl	%edi, %eax
	notl	%eax
	andl	%r8d, %eax
	addl	-112(%rsp), %edx                # 4-byte Folded Reload
	addl	%eax, %edx
	movl	%ecx, %eax
	andl	%edi, %eax
	addl	%eax, %edx
	addl	$-1894007588, %edx              # imm = 0x8F1BBCDC
	roll	$6, %ebx
	roll	$10, %r11d
	addl	%r10d, %ebx
	movl	%ebx, %eax
	notl	%eax
	andl	%r11d, %eax
	addl	%r10d, %eax
	movl	%ebx, %esi
	andl	%r9d, %esi
	leal	(%rsi,%rax), %r14d
	addl	$2053994217, %r14d              # imm = 0x7A6D76E9
	roll	$14, %edx
	addl	%r15d, %edx
	roll	$10, %r8d
	movl	%r8d, %eax
	notl	%eax
	andl	%ecx, %eax
	addl	-88(%rsp), %r15d                # 4-byte Folded Reload
	addl	%eax, %r15d
	movl	%edx, %eax
	andl	%r8d, %eax
	movl	%r8d, %r10d
	leal	(%rax,%r15), %esi
	addl	$-1894007588, %esi              # imm = 0x8F1BBCDC
	roll	$9, %r14d
	addl	%r12d, %r14d
	roll	$10, %r9d
	movl	%r14d, %eax
	notl	%eax
	andl	%r9d, %eax
	addl	-80(%rsp), %r12d                # 4-byte Folded Reload
	addl	%eax, %r12d
	movl	%r14d, %eax
	andl	%ebx, %eax
	leal	(%rax,%r12), %r13d
	addl	$2053994217, %r13d              # imm = 0x7A6D76E9
	roll	$5, %esi
	roll	$10, %ecx
	addl	%edi, %esi
	movl	%ecx, %eax
	notl	%eax
	andl	%edx, %eax
	addl	%edi, %eax
	movl	%esi, %edi
	andl	%ecx, %edi
	leal	(%rdi,%rax), %r8d
	addl	$-1894007588, %r8d              # imm = 0x8F1BBCDC
	roll	$12, %r13d
	addl	%r11d, %r13d
	roll	$10, %ebx
	movl	%r13d, %eax
	notl	%eax
	andl	%ebx, %eax
	addl	%r11d, %eax
	movl	%r13d, %ebp
	andl	%r14d, %ebp
	leal	(%rax,%rbp), %r11d
	addl	$2053994217, %r11d              # imm = 0x7A6D76E9
	roll	$6, %r8d
	roll	$10, %edx
	addl	%r10d, %r8d
	movl	%edx, %eax
	notl	%eax
	andl	%esi, %eax
	addl	%r10d, %eax
	movl	%r8d, %edi
	andl	%edx, %edi
	leal	(%rdi,%rax), %ebp
	addl	$-1894007332, %ebp              # imm = 0x8F1BBDDC
	roll	$9, %r11d
	addl	%r9d, %r11d
	roll	$10, %r14d
	movl	%r11d, %eax
	notl	%eax
	andl	%r14d, %eax
	addl	%r9d, %eax
	movl	%r11d, %edi
	andl	%r13d, %edi
	leal	(%rdi,%rax), %r10d
	addl	$2053994217, %r10d              # imm = 0x7A6D76E9
	roll	$8, %ebp
	roll	$10, %esi
	addl	%ecx, %ebp
	movl	%esi, %eax
	notl	%eax
	andl	%r8d, %eax
	addl	-72(%rsp), %ecx                 # 4-byte Folded Reload
	addl	%eax, %ecx
	roll	$12, %r10d
	movl	%ebp, %eax
	andl	%esi, %eax
	leal	(%rax,%rcx), %r9d
	addl	$-1894007588, %r9d              # imm = 0x8F1BBCDC
	addl	%ebx, %r10d
	roll	$10, %r13d
	movl	%r10d, %eax
	notl	%eax
	andl	%r13d, %eax
	addl	-88(%rsp), %ebx                 # 4-byte Folded Reload
	addl	%eax, %ebx
	movl	%r10d, %eax
	andl	%r11d, %eax
	leal	(%rax,%rbx), %r15d
	addl	$2053994217, %r15d              # imm = 0x7A6D76E9
	roll	$6, %r9d
	addl	%edx, %r9d
	roll	$10, %r8d
	movl	%r8d, %eax
	notl	%eax
	andl	%ebp, %eax
	addl	-120(%rsp), %edx                # 4-byte Folded Reload
	addl	%eax, %edx
	movl	%r9d, %eax
	andl	%r8d, %eax
	leal	(%rax,%rdx), %r12d
	addl	$-1894007588, %r12d             # imm = 0x8F1BBCDC
	roll	$5, %r15d
	addl	%r14d, %r15d
	roll	$10, %r11d
	movl	%r15d, %eax
	notl	%eax
	andl	%r11d, %eax
	addl	%r14d, %eax
	roll	$5, %r12d
	movl	%r15d, %ecx
	andl	%r10d, %ecx
	addl	%ecx, %eax
	addl	$2053994217, %eax               # imm = 0x7A6D76E9
	addl	%esi, %r12d
	roll	$10, %ebp
	movl	%ebp, %ecx
	notl	%ecx
	andl	%r9d, %ecx
	addl	-80(%rsp), %esi                 # 4-byte Folded Reload
	addl	%ecx, %esi
	movl	%r12d, %ecx
	andl	%ebp, %ecx
	leal	(%rcx,%rsi), %r14d
	addl	$-1894007588, %r14d             # imm = 0x8F1BBCDC
	roll	$15, %eax
	addl	%r13d, %eax
	roll	$10, %r10d
	movl	%eax, %ecx
	notl	%ecx
	andl	%r10d, %ecx
	addl	%r13d, %ecx
	movl	%eax, %edx
	andl	%r15d, %edx
	leal	(%rdx,%rcx), %r13d
	addl	$2053994473, %r13d              # imm = 0x7A6D77E9
	roll	$12, %r14d
	roll	$10, %r9d
	addl	%r8d, %r14d
	movl	%r9d, %ecx
	notl	%ecx
	orl	%r12d, %ecx
	xorl	%r14d, %ecx
	addl	-128(%rsp), %r8d                # 4-byte Folded Reload
	addl	%ecx, %r8d
	addl	$-1454113458, %r8d              # imm = 0xA953FD4E
	roll	$8, %r13d
	addl	%r11d, %r13d
	roll	$10, %r15d
	movl	%eax, %ecx
	xorl	%r15d, %ecx
	xorl	%r13d, %ecx
	addl	%r11d, %ecx
	roll	$9, %r8d
	addl	%ebp, %r8d
	roll	$10, %r12d
	movl	%r12d, %edx
	notl	%edx
	orl	%r14d, %edx
	xorl	%r8d, %edx
	addl	-96(%rsp), %ebp                 # 4-byte Folded Reload
	leal	(%rdx,%rbp), %ebx
	addl	$-1454113458, %ebx              # imm = 0xA953FD4E
	roll	$8, %ecx
	roll	$10, %eax
	addl	%r10d, %ecx
	movl	%r13d, %esi
	xorl	%eax, %esi
	xorl	%ecx, %esi
	addl	%r10d, %esi
	roll	$15, %ebx
	roll	$10, %r14d
	addl	%r9d, %ebx
	movl	%r14d, %edi
	notl	%edi
	orl	%r8d, %edi
	xorl	%ebx, %edi
	movq	-72(%rsp), %r11                 # 8-byte Reload
	addl	%r11d, %r9d
	leal	(%rdi,%r9), %r10d
	addl	$-1454113458, %r10d             # imm = 0xA953FD4E
	roll	$5, %esi
	addl	%r15d, %esi
	roll	$10, %r13d
	movl	%ecx, %edi
	xorl	%r13d, %edi
	xorl	%esi, %edi
	addl	%r15d, %edi
	roll	$5, %r10d
	roll	$10, %r8d
	roll	$12, %edi
	addl	%r12d, %r10d
	movl	%r8d, %ebp
	notl	%ebp
	orl	%ebx, %ebp
	xorl	%r10d, %ebp
	leal	(%r12,%rbp), %r15d
	addl	$-1454113458, %r15d             # imm = 0xA953FD4E
	addl	%eax, %edi
	roll	$10, %ecx
	addl	-128(%rsp), %eax                # 4-byte Folded Reload
	movl	%esi, %ebp
	xorl	%ecx, %ebp
	xorl	%edi, %ebp
	addl	%ebp, %eax
	roll	$11, %r15d
	roll	$10, %ebx
	addl	%r14d, %r15d
	movl	%ebx, %edx
	notl	%edx
	orl	%r10d, %edx
	xorl	%r15d, %edx
	addl	-88(%rsp), %r14d                # 4-byte Folded Reload
	leal	(%rdx,%r14), %ebp
	addl	$-1454113458, %ebp              # imm = 0xA953FD4E
	roll	$9, %eax
	roll	$10, %esi
	roll	$6, %ebp
	addl	%r13d, %eax
	movl	%edi, %edx
	xorl	%esi, %edx
	xorl	%eax, %edx
	movq	-104(%rsp), %r14                # 8-byte Reload
	addl	%r14d, %r13d
	addl	%edx, %r13d
	addl	%r8d, %ebp
	roll	$10, %r10d
	movl	%r10d, %edx
	notl	%edx
	orl	%r15d, %edx
	xorl	%ebp, %edx
	leal	(%r8,%rdx), %r9d
	addl	$-1454113458, %r9d              # imm = 0xA953FD4E
	roll	$12, %r13d
	addl	%ecx, %r13d
	addl	%r11d, %ecx
	roll	$10, %edi
	movl	%eax, %edx
	xorl	%edi, %edx
	xorl	%r13d, %edx
	addl	%edx, %ecx
	roll	$8, %r9d
	addl	%ebx, %r9d
	roll	$10, %r15d
	roll	$5, %ecx
	movl	%r15d, %edx
	notl	%edx
	orl	%ebp, %edx
	xorl	%r9d, %edx
	movq	-80(%rsp), %r12                 # 8-byte Reload
	addl	%r12d, %ebx
	leal	(%rdx,%rbx), %r8d
	addl	$-1454113458, %r8d              # imm = 0xA953FD4E
	addl	%esi, %ecx
	roll	$10, %eax
	roll	$13, %r8d
	movl	%r13d, %edx
	xorl	%eax, %edx
	xorl	%ecx, %edx
	leal	(%rsi,%rdx), %r11d
	addl	$128, %r11d
	addl	%r10d, %r8d
	roll	$10, %ebp
	movl	%ebp, %edx
	notl	%edx
	orl	%r9d, %edx
	xorl	%r8d, %edx
	addl	%edx, %r10d
	addl	$-1454113458, %r10d             # imm = 0xA953FD4E
	roll	$14, %r11d
	addl	%edi, %r11d
	addl	-88(%rsp), %edi                 # 4-byte Folded Reload
	roll	$10, %r13d
	movl	%ecx, %edx
	xorl	%r13d, %edx
	xorl	%r11d, %edx
	addl	%edx, %edi
	roll	$12, %r10d
	addl	%r15d, %r10d
	roll	$10, %r9d
	movl	%r9d, %edx
	notl	%edx
	orl	%r8d, %edx
	xorl	%r10d, %edx
	leal	(%r15,%rdx), %ebx
	addl	$-1454113202, %ebx              # imm = 0xA953FE4E
	roll	$6, %edi
	roll	$10, %ecx
	roll	$5, %ebx
	addl	%eax, %edi
	movl	%r11d, %edx
	xorl	%ecx, %edx
	xorl	%edi, %edx
	addl	-120(%rsp), %eax                # 4-byte Folded Reload
	addl	%edx, %eax
	addl	%ebp, %ebx
	roll	$10, %r8d
	addl	%r14d, %ebp
	movl	%r8d, %edx
	notl	%edx
	orl	%r10d, %edx
	xorl	%ebx, %edx
	leal	(%rdx,%rbp), %r14d
	addl	$-1454113458, %r14d             # imm = 0xA953FD4E
	roll	$8, %eax
	addl	%r13d, %eax
	roll	$10, %r11d
	roll	$12, %r14d
	roll	$10, %r10d
	addl	%r12d, %r13d
	movl	%edi, %edx
	xorl	%r11d, %edx
	xorl	%eax, %edx
	addl	%edx, %r13d
	roll	$13, %r13d
	addl	%r9d, %r14d
	addl	%ecx, %r13d
	roll	$10, %edi
	movl	%r10d, %edx
	notl	%edx
	orl	%ebx, %edx
	xorl	%r14d, %edx
	movq	-112(%rsp), %r15                # 8-byte Reload
	addl	%r15d, %r9d
	addl	%edx, %r9d
	addl	$-1454113458, %r9d              # imm = 0xA953FD4E
	roll	$13, %r9d
	addl	%r8d, %r9d
	roll	$10, %ebx
	movl	%eax, %esi
	xorl	%edi, %esi
	xorl	%r13d, %esi
	addl	%ecx, %esi
	roll	$6, %esi
	addl	%r11d, %esi
	roll	$10, %eax
	movl	%ebx, %ecx
	notl	%ecx
	orl	%r14d, %ecx
	xorl	%r9d, %ecx
	addl	%ecx, %r8d
	addl	$-1454113330, %r8d              # imm = 0xA953FDCE
	roll	$14, %r8d
	addl	%r10d, %r8d
	roll	$10, %r14d
	movl	%r13d, %ecx
	xorl	%eax, %ecx
	xorl	%esi, %ecx
	addl	%ecx, %r11d
	addl	$256, %r11d                     # imm = 0x100
	movl	%r14d, %ebp
	notl	%ebp
	orl	%r9d, %ebp
	xorl	%r8d, %ebp
	addl	%ebp, %r10d
	addl	$-1454113458, %r10d             # imm = 0xA953FD4E
	roll	$5, %r11d
	roll	$10, %r13d
	roll	$11, %r10d
	addl	%edi, %r11d
	addl	-96(%rsp), %edi                 # 4-byte Folded Reload
	movl	%esi, %edx
	xorl	%r13d, %edx
	xorl	%r11d, %edx
	addl	%edx, %edi
	addl	%ebx, %r10d
	roll	$10, %r9d
	addl	-120(%rsp), %ebx                # 4-byte Folded Reload
	movl	%r9d, %edx
	notl	%edx
	orl	%r8d, %edx
	xorl	%r10d, %edx
	addl	%ebx, %edx
	addl	$-1454113458, %edx              # imm = 0xA953FD4E
	roll	$15, %edi
	addl	%eax, %edi
	roll	$10, %esi
	roll	$8, %edx
	roll	$10, %r8d
	addl	%r15d, %eax
	movl	%r11d, %ebp
	xorl	%esi, %ebp
	xorl	%edi, %ebp
	addl	%ebp, %eax
	roll	$13, %eax
	addl	%r14d, %edx
	addl	%r13d, %eax
	roll	$10, %r11d
	movl	%r8d, %ebp
	notl	%ebp
	orl	%r10d, %ebp
	xorl	%edx, %ebp
	leal	(%r14,%rbp), %ebx
	addl	$-1454113458, %ebx              # imm = 0xA953FD4E
	roll	$5, %ebx
	roll	$10, %r10d
	addl	%r9d, %ebx
	movl	%edi, %ebp
	xorl	%r11d, %ebp
	xorl	%eax, %ebp
	addl	%r13d, %ebp
	roll	$11, %ebp
	roll	$10, %edi
	addl	%esi, %ebp
	movl	%r10d, %ecx
	notl	%ecx
	orl	%edx, %ecx
	xorl	%ebx, %ecx
	addl	%ecx, %r9d
	addl	$-1454113458, %r9d              # imm = 0xA953FD4E
	movl	%eax, %ecx
	xorl	%edi, %ecx
	xorl	%ebp, %ecx
	addl	%esi, %ecx
	roll	$10, %eax
	leal	(%rax,%rbx), %esi
	addl	$-271733879, %esi               # imm = 0xEFCDAB89
	movq	80(%rsp), %rax                  # 8-byte Reload
	movl	%esi, (%rax)
	roll	$10, %edx
	addl	%edi, %edx
	addl	$-1732584194, %edx              # imm = 0x98BADCFE
	movl	%edx, 4(%rax)
	leal	(%r11,%r10), %edx
	addl	$271733878, %edx                # imm = 0x10325476
	movl	%edx, 8(%rax)
	roll	$11, %ecx
	addl	%r8d, %r11d
	addl	%r11d, %ecx
	addl	$-1009589776, %ecx              # imm = 0xC3D2E1F0
	movl	%ecx, 12(%rax)
	addl	%r8d, %ebp
	roll	$6, %r9d
	leal	(%r9,%rbp), %ecx
	addl	$1732584193, %ecx               # imm = 0x67452301
	movl	%ecx, 16(%rax)
	addq	$88, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end25:
	.size	hash160, .Lfunc_end25-hash160
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function main
.LCPI26_0:
	.long	1127219200                      # 0x43300000
	.long	1160773632                      # 0x45300000
	.long	0                               # 0x0
	.long	0                               # 0x0
.LCPI26_1:
	.quad	0x4330000000000000              # double 4503599627370496
	.quad	0x4530000000000000              # double 1.9342813113834067E+25
.LCPI26_6:
	.long	1779033703                      # 0x6a09e667
	.long	3144134277                      # 0xbb67ae85
	.long	1013904242                      # 0x3c6ef372
	.long	2773480762                      # 0xa54ff53a
.LCPI26_7:
	.long	1359893119                      # 0x510e527f
	.long	2600822924                      # 0x9b05688c
	.long	528734635                       # 0x1f83d9ab
	.long	1541459225                      # 0x5be0cd19
.LCPI26_8:
	.zero	16
	.section	.rodata.cst8,"aM",@progbits,8
	.p2align	3
.LCPI26_2:
	.quad	0x3fe0000000000000              # double 0.5
.LCPI26_3:
	.quad	0x3ff2d6abe44afc2e              # double 1.17741002251547
.LCPI26_4:
	.quad	0x3fdb000000000000              # double 0.421875
.LCPI26_5:
	.quad	0x43e0000000000000              # double 9.2233720368547758E+18
	.text
	.globl	main
	.p2align	4, 0x90
	.type	main,@function
main:                                   # @main
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset %rbp, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1528, %rsp                     # imm = 0x5F8
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	movq	%rsi, %r14
	movl	%edi, %r12d
	leaq	.L.str.21(%rip), %rdx
	leaq	main.long_options(%rip), %rcx
	leaq	-732(%rbp), %r8
	callq	getopt_long@PLT
	testl	%eax, %eax
	js	.LBB26_9
# %bb.1:                                # %.lr.ph.preheader
	leaq	.L.str.20(%rip), %rcx
	movq	%rcx, -376(%rbp)                # 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, -120(%rbp)                # 8-byte Spill
	movq	optarg@GOTPCREL(%rip), %r15
	leaq	.L.str.21(%rip), %r13
	leaq	main.long_options(%rip), %rbx
	movq	%r12, -48(%rbp)                 # 8-byte Spill
	jmp	.LBB26_3
	.p2align	4, 0x90
.LBB26_2:                               #   in Loop: Header=BB26_3 Depth=1
	movl	%r12d, %edi
	movq	%r14, %rsi
	movq	%r13, %rdx
	movq	%rbx, %rcx
	leaq	-732(%rbp), %r8
	callq	getopt_long@PLT
	testl	%eax, %eax
	js	.LBB26_10
.LBB26_3:                               # %.lr.ph
                                        # =>This Inner Loop Header: Depth=1
	cmpl	$3, %eax
	je	.LBB26_6
# %bb.4:                                # %.lr.ph
                                        #   in Loop: Header=BB26_3 Depth=1
	cmpl	$1, %eax
	jne	.LBB26_174
# %bb.5:                                #   in Loop: Header=BB26_3 Depth=1
	movq	(%r15), %rdi
	callq	strdup@PLT
	movq	%rax, %rcx
	movq	%rax, -120(%rbp)                # 8-byte Spill
	testq	%rax, %rax
	jne	.LBB26_2
	jmp	.LBB26_337
	.p2align	4, 0x90
.LBB26_6:                               #   in Loop: Header=BB26_3 Depth=1
	movq	%r14, %r12
	movq	%rbx, %r14
	movq	%r15, %rbx
	movq	(%r15), %r15
	movq	%r15, %rdi
	callq	strlen@PLT
	cmpq	$100, %rax
	jae	.LBB26_325
# %bb.7:                                #   in Loop: Header=BB26_3 Depth=1
	movq	%r15, %rdi
	callq	strdup@PLT
	movq	%rax, -376(%rbp)                # 8-byte Spill
	testq	%rax, %rax
	movq	%rbx, %r15
	movq	%r14, %rbx
	movq	%r12, %r14
	movq	-48(%rbp), %r12                 # 8-byte Reload
	jne	.LBB26_2
# %bb.8:
	leaq	.L.str.25(%rip), %rdi
	leaq	.L.str.23(%rip), %rsi
	leaq	.L__PRETTY_FUNCTION__.main(%rip), %rcx
	movl	$1258, %edx                     # imm = 0x4EA
	callq	__assert_fail@PLT
.LBB26_9:
	leaq	.L.str.20(%rip), %rax
	movq	%rax, -376(%rbp)                # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, -120(%rbp)                # 8-byte Spill
.LBB26_10:                              # %._crit_edge
	movq	optind@GOTPCREL(%rip), %rax
	movl	(%rax), %r15d
	leal	-1(%r12), %eax
	addl	$-2, %r12d
	cmpl	%eax, %r15d
	je	.LBB26_12
# %bb.11:                               # %._crit_edge
	cmpl	%r12d, %r15d
	jne	.LBB26_174
.LBB26_12:
	movslq	%r15d, %rbx
	movq	(%r14,%rbx,8), %rdi
	xorl	%esi, %esi
	movl	$10, %edx
	callq	strtol@PLT
	movslq	%eax, %rcx
	movq	%rcx, -48(%rbp)                 # 8-byte Spill
	addl	$-161, %ecx
	cmpl	$-130, %ecx
	jbe	.LBB26_177
# %bb.13:
	movq	%rax, -840(%rbp)                # 8-byte Spill
	cmpl	%r12d, %r15d
	jne	.LBB26_17
# %bb.14:
	movq	8(%r14,%rbx,8), %rdi
	xorl	%esi, %esi
	movl	$10, %edx
	callq	strtol@PLT
	movslq	%eax, %rcx
	shlq	$32, %rax
	movabsq	$-4294967296, %rdx              # imm = 0xFFFFFFFF00000000
	cmpq	%rdx, %rax
	sete	%r14b
	je	.LBB26_16
# %bb.15:
	cmpq	-48(%rbp), %rcx                 # 8-byte Folded Reload
	jae	.LBB26_324
.LBB26_16:
	movq	%rcx, %r15
	jmp	.LBB26_18
.LBB26_17:
	movq	$-1, %r15
	movb	$1, %r14b
.LBB26_18:                              # %.thread
	leaq	.L.str.34(%rip), %rdi
	movq	-48(%rbp), %rbx                 # 8-byte Reload
	movq	%rbx, %rsi
	xorl	%eax, %eax
	callq	printf@PLT
	movq	%rbx, %xmm0
	punpckldq	.LCPI26_0(%rip), %xmm0  # xmm0 = xmm0[0],mem[0],xmm0[1],mem[1]
	subpd	.LCPI26_1(%rip), %xmm0
	movapd	%xmm0, %xmm1
	unpckhpd	%xmm0, %xmm1                    # xmm1 = xmm1[1],xmm0[1]
	addsd	%xmm0, %xmm1
	movsd	.LCPI26_2(%rip), %xmm0          # xmm0 = mem[0],zero
	movapd	%xmm1, -112(%rbp)               # 16-byte Spill
	mulsd	%xmm1, %xmm0
	callq	exp2@PLT
	mulsd	.LCPI26_3(%rip), %xmm0
	callq	round@PLT
	leaq	.L.str.35(%rip), %rdi
	movb	$1, %al
	callq	printf@PLT
	testb	%r14b, %r14b
	movq	%r15, %rbx
	je	.LBB26_20
# %bb.19:
	movapd	-112(%rbp), %xmm0               # 16-byte Reload
	mulsd	.LCPI26_4(%rip), %xmm0
	callq	round@PLT
	cvttsd2si	%xmm0, %rax
	movq	%rax, %rcx
	subsd	.LCPI26_5(%rip), %xmm0
	cvttsd2si	%xmm0, %rbx
	sarq	$63, %rcx
	andq	%rcx, %rbx
	orq	%rax, %rbx
	addq	$-6, %rbx
.LBB26_20:
	leaq	.L.str.36(%rip), %rdi
	movq	%rbx, %rsi
	xorl	%eax, %eax
	callq	printf@PLT
	leaq	bases(%rip), %rdi
	movl	$8388480, %edx                  # imm = 0x7FFF80
	xorl	%esi, %esi
	callq	memset@PLT
	leaq	offsets(%rip), %r12
	movl	$51903720, %edx                 # imm = 0x317FCE8
	movq	%r12, %rdi
	xorl	%esi, %esi
	callq	memset@PLT
	leaq	priv_bases(%rip), %rdi
	movl	$2097120, %edx                  # imm = 0x1FFFE0
	movl	$128, %esi
	callq	memset@PLT
	leaq	priv_offsets(%rip), %rdi
	movl	$18874080, %edx                 # imm = 0x11FFEE0
	movl	$128, %esi
	callq	memset@PLT
	movl	$3, %edi
	callq	secp256k1_context_create
	movq	%rax, cxt(%rip)
	leaq	table_lock(%rip), %rdi
	xorl	%esi, %esi
	callq	pthread_mutex_init@PLT
	testl	%eax, %eax
	movq	-120(%rbp), %r14                # 8-byte Reload
	jne	.LBB26_341
# %bb.21:                               # %mutex_init.exit
	movq	%rbx, -80(%rbp)                 # 8-byte Spill
	movl	$84, %edi
	callq	sysconf@PLT
	movq	%rax, %rbx
	leaq	.L.str.37(%rip), %rdi
	movq	%rax, %rsi
	xorl	%eax, %eax
	callq	printf@PLT
	leaq	-640(%rbp), %rsi
	movl	$1, %edi
	callq	clock_gettime@PLT
	movq	-640(%rbp), %rax
	movq	%rax, -848(%rbp)                # 8-byte Spill
	movabsq	$-4835703278458516699, %rax     # imm = 0xBCE4217D2849CB25
	imulq	-632(%rbp)
	movq	%rdx, %r13
	leaq	.L.str.38(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movq	stdout@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	callq	fflush@PLT
	testq	%r14, %r14
	movq	%r13, -856(%rbp)                # 8-byte Spill
	movq	%rbx, -368(%rbp)                # 8-byte Spill
	je	.LBB26_59
# %bb.22:
	movq	%r14, %rdi
	callq	strlen@PLT
	movq	%rax, %rbx
	leaq	8(%rax), %rdi
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB26_340
# %bb.23:
	movq	%rax, %r15
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%rbx, %rdx
	callq	memcpy@PLT
	movl	$1651863598, (%r15,%rbx)        # imm = 0x6275702E
	movl	$1667853410, 3(%r15,%rbx)       # imm = 0x63696C62
	movb	$0, 7(%r15,%rbx)
	movq	%r14, %rdi
	callq	strlen@PLT
	movq	%rax, %rbx
	leaq	8(%rax), %rdi
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB26_340
# %bb.24:
	movq	%r14, %rsi
	movq	%rax, %r14
	movq	%rax, %rdi
	movq	%rbx, %rdx
	callq	memcpy@PLT
	movl	$1667593006, (%r14,%rbx)        # imm = 0x6365732E
	movl	$1952805475, 3(%r14,%rbx)       # imm = 0x74657263
	movb	$0, 7(%r14,%rbx)
	leaq	.L.str.44(%rip), %rsi
	movq	%r15, %rdi
	callq	fopen@PLT
	testq	%rax, %rax
	movq	%r14, -344(%rbp)                # 8-byte Spill
	je	.LBB26_60
# %bb.25:
	movq	%rax, %rbx
	movq	%r15, -192(%rbp)                # 8-byte Spill
	movl	$21626582, %edi                 # imm = 0x149FED6
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB26_344
# %bb.26:
	movq	%rax, %r15
	movl	$21626582, %esi                 # imm = 0x149FED6
	movl	$1, %edx
	movq	%rax, %rdi
	movq	%rbx, -72(%rbp)                 # 8-byte Spill
	movq	%rbx, %rcx
	callq	fread@PLT
	cmpq	$1, %rax
	jne	.LBB26_328
# %bb.27:
	movaps	.LCPI26_6(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	%xmm0, -640(%rbp)
	movaps	.LCPI26_7(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movaps	%xmm0, -624(%rbp)
	movq	$0, -448(%rbp)
	leaq	-640(%rbp), %r14
	movl	$21626550, %edx                 # imm = 0x149FEB6
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	secp256k1_sha256_write
	movq	-448(%rbp), %rax
	movq	%rax, %rcx
	movq	%rax, %rsi
	movq	%rax, %r8
	movl	%eax, %r9d
	movl	%eax, %edi
	movl	%eax, %ebx
	movl	%eax, %r10d
	movl	$55, %edx
	subl	%eax, %edx
	shrq	$29, %rax
	shll	$24, %eax
	shrq	$21, %rcx
	andl	$16711680, %ecx                 # imm = 0xFF0000
	orl	%eax, %ecx
	shrq	$37, %rsi
	andl	$65280, %esi                    # imm = 0xFF00
	orl	%ecx, %esi
	shrq	$53, %r8
	movzbl	%r8b, %eax
	orl	%esi, %eax
	movl	%eax, -336(%rbp)
	shll	$27, %r9d
	shll	$11, %edi
	andl	$16711680, %edi                 # imm = 0xFF0000
	orl	%r9d, %edi
	shrl	$5, %ebx
	andl	$65280, %ebx                    # imm = 0xFF00
	orl	%edi, %ebx
	shrl	$21, %r10d
	movzbl	%r10b, %eax
	orl	%ebx, %eax
	movl	%eax, -332(%rbp)
	andl	$63, %edx
	addq	$1, %rdx
	leaq	secp256k1_sha256_finalize.pad(%rip), %rsi
	movq	%r14, %rdi
	callq	secp256k1_sha256_write
	leaq	-336(%rbp), %rsi
	movl	$8, %edx
	movq	%r14, %rdi
	callq	secp256k1_sha256_write
	pxor	%xmm3, %xmm3
	movdqa	-640(%rbp), %xmm0
	movdqa	-624(%rbp), %xmm1
	movdqa	%xmm0, %xmm2
	punpckhbw	%xmm3, %xmm2            # xmm2 = xmm2[8],xmm3[8],xmm2[9],xmm3[9],xmm2[10],xmm3[10],xmm2[11],xmm3[11],xmm2[12],xmm3[12],xmm2[13],xmm3[13],xmm2[14],xmm3[14],xmm2[15],xmm3[15]
	pshuflw	$27, %xmm2, %xmm2               # xmm2 = xmm2[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm2, %xmm2               # xmm2 = xmm2[0,1,2,3,7,6,5,4]
	punpcklbw	%xmm3, %xmm0            # xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1],xmm0[2],xmm3[2],xmm0[3],xmm3[3],xmm0[4],xmm3[4],xmm0[5],xmm3[5],xmm0[6],xmm3[6],xmm0[7],xmm3[7]
	pshuflw	$27, %xmm0, %xmm0               # xmm0 = xmm0[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm0, %xmm0               # xmm0 = xmm0[0,1,2,3,7,6,5,4]
	packuswb	%xmm2, %xmm0
	movdqa	%xmm1, %xmm2
	punpckhbw	%xmm3, %xmm2            # xmm2 = xmm2[8],xmm3[8],xmm2[9],xmm3[9],xmm2[10],xmm3[10],xmm2[11],xmm3[11],xmm2[12],xmm3[12],xmm2[13],xmm3[13],xmm2[14],xmm3[14],xmm2[15],xmm3[15]
	pshuflw	$27, %xmm2, %xmm2               # xmm2 = xmm2[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm2, %xmm2               # xmm2 = xmm2[0,1,2,3,7,6,5,4]
	punpcklbw	%xmm3, %xmm1            # xmm1 = xmm1[0],xmm3[0],xmm1[1],xmm3[1],xmm1[2],xmm3[2],xmm1[3],xmm3[3],xmm1[4],xmm3[4],xmm1[5],xmm3[5],xmm1[6],xmm3[6],xmm1[7],xmm3[7]
	pshuflw	$27, %xmm1, %xmm1               # xmm1 = xmm1[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm1, %xmm1               # xmm1 = xmm1[0,1,2,3,7,6,5,4]
	packuswb	%xmm2, %xmm1
	movdqa	%xmm0, -720(%rbp)
	movdqa	%xmm1, -704(%rbp)
	movdqu	21626550(%r15), %xmm2
	movq	%r15, -112(%rbp)                # 8-byte Spill
	movdqu	21626566(%r15), %xmm3
	pcmpeqb	%xmm1, %xmm3
	pcmpeqb	%xmm0, %xmm2
	pand	%xmm3, %xmm2
	pmovmskb	%xmm2, %eax
	cmpl	$65535, %eax                    # imm = 0xFFFF
	jne	.LBB26_328
# %bb.28:                               # %.preheader62.i.i
	movl	$120, %r14d
	leaq	-600(%rbp), %r15
	leaq	-640(%rbp), %r13
	movq	-112(%rbp), %rbx                # 8-byte Reload
	.p2align	4, 0x90
.LBB26_29:                              # =>This Inner Loop Header: Depth=1
	movq	%r13, %rdi
	movq	%rbx, %rsi
	movl	$33, %edx
	callq	secp256k1_eckey_pubkey_parse
	testl	%eax, %eax
	je	.LBB26_328
# %bb.30:                               #   in Loop: Header=BB26_29 Depth=1
	movl	-560(%rbp), %eax
	leaq	bases(%rip), %rcx
	movl	%eax, (%r14,%rcx)
	movups	-640(%rbp), %xmm0
	movups	-624(%rbp), %xmm1
	movups	%xmm0, -120(%r14,%rcx)
	movups	%xmm1, -104(%r14,%rcx)
	movq	-608(%rbp), %rax
	movq	%rax, -88(%r14,%rcx)
	movdqu	(%r15), %xmm0
	movdqu	16(%r15), %xmm1
	movdqu	%xmm0, -80(%r14,%rcx)
	movdqu	%xmm1, -64(%r14,%rcx)
	movq	32(%r15), %rax
	movq	%rax, -48(%r14,%rcx)
	movq	$1, -40(%r14,%rcx)
	pxor	%xmm0, %xmm0
	movdqu	%xmm0, -32(%r14,%rcx)
	movdqu	%xmm0, -16(%r14,%rcx)
	addq	$33, %rbx
	subq	$-128, %r14
	cmpq	$8388600, %r14                  # imm = 0x7FFFF8
	jne	.LBB26_29
# %bb.31:                               # %.preheader.preheader.i.i.preheader
	xorl	%r14d, %r14d
	movq	stdout@GOTPCREL(%rip), %r15
	.p2align	4, 0x90
.LBB26_32:                              # %.preheader.preheader.i.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%rbx,%r14), %rsi
	movq	%r12, %rdi
	movl	$33, %edx
	callq	secp256k1_eckey_pubkey_parse
	testl	%eax, %eax
	je	.LBB26_328
# %bb.33:                               #   in Loop: Header=BB26_32 Depth=1
	addq	$33, %r14
	addq	$792, %r12                      # imm = 0x318
	cmpq	$2162655, %r14                  # imm = 0x20FFDF
	jne	.LBB26_32
# %bb.34:                               # %.preheader.1.i.i.preheader
	addq	%r14, %rbx
	movl	$88, %r14d
	leaq	offsets(%rip), %r12
	.p2align	4, 0x90
.LBB26_35:                              # %.preheader.1.i.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r12,%r14), %rdi
	movq	%rbx, %rsi
	movl	$33, %edx
	callq	secp256k1_eckey_pubkey_parse
	testl	%eax, %eax
	je	.LBB26_328
# %bb.36:                               #   in Loop: Header=BB26_35 Depth=1
	addq	$33, %rbx
	addq	$792, %r14                      # imm = 0x318
	cmpq	$51903808, %r14                 # imm = 0x317FD40
	jne	.LBB26_35
# %bb.37:                               # %.preheader.2.i.i.preheader
	movl	$176, %r14d
	.p2align	4, 0x90
.LBB26_38:                              # %.preheader.2.i.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r12,%r14), %rdi
	movq	%rbx, %rsi
	movl	$33, %edx
	callq	secp256k1_eckey_pubkey_parse
	testl	%eax, %eax
	je	.LBB26_328
# %bb.39:                               #   in Loop: Header=BB26_38 Depth=1
	addq	$33, %rbx
	addq	$792, %r14                      # imm = 0x318
	cmpq	$51903896, %r14                 # imm = 0x317FD98
	jne	.LBB26_38
# %bb.40:                               # %.preheader.3.i.i.preheader
	movl	$264, %r14d                     # imm = 0x108
	.p2align	4, 0x90
.LBB26_41:                              # %.preheader.3.i.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r12,%r14), %rdi
	movq	%rbx, %rsi
	movl	$33, %edx
	callq	secp256k1_eckey_pubkey_parse
	testl	%eax, %eax
	je	.LBB26_328
# %bb.42:                               #   in Loop: Header=BB26_41 Depth=1
	addq	$33, %rbx
	addq	$792, %r14                      # imm = 0x318
	cmpq	$51903984, %r14                 # imm = 0x317FDF0
	jne	.LBB26_41
# %bb.43:                               # %.preheader.4.i.i.preheader
	movl	$352, %r14d                     # imm = 0x160
	.p2align	4, 0x90
.LBB26_44:                              # %.preheader.4.i.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r12,%r14), %rdi
	movq	%rbx, %rsi
	movl	$33, %edx
	callq	secp256k1_eckey_pubkey_parse
	testl	%eax, %eax
	je	.LBB26_328
# %bb.45:                               #   in Loop: Header=BB26_44 Depth=1
	addq	$33, %rbx
	addq	$792, %r14                      # imm = 0x318
	cmpq	$51904072, %r14                 # imm = 0x317FE48
	jne	.LBB26_44
# %bb.46:                               # %.preheader.5.i.i.preheader
	movl	$440, %r14d                     # imm = 0x1B8
	.p2align	4, 0x90
.LBB26_47:                              # %.preheader.5.i.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r12,%r14), %rdi
	movq	%rbx, %rsi
	movl	$33, %edx
	callq	secp256k1_eckey_pubkey_parse
	testl	%eax, %eax
	je	.LBB26_328
# %bb.48:                               #   in Loop: Header=BB26_47 Depth=1
	addq	$33, %rbx
	addq	$792, %r14                      # imm = 0x318
	cmpq	$51904160, %r14                 # imm = 0x317FEA0
	jne	.LBB26_47
# %bb.49:                               # %.preheader.6.i.i.preheader
	movl	$528, %r14d                     # imm = 0x210
	.p2align	4, 0x90
.LBB26_50:                              # %.preheader.6.i.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r12,%r14), %rdi
	movq	%rbx, %rsi
	movl	$33, %edx
	callq	secp256k1_eckey_pubkey_parse
	testl	%eax, %eax
	je	.LBB26_328
# %bb.51:                               #   in Loop: Header=BB26_50 Depth=1
	addq	$33, %rbx
	addq	$792, %r14                      # imm = 0x318
	cmpq	$51904248, %r14                 # imm = 0x317FEF8
	jne	.LBB26_50
# %bb.52:                               # %.preheader.7.i.i.preheader
	movl	$616, %r14d                     # imm = 0x268
	.p2align	4, 0x90
.LBB26_53:                              # %.preheader.7.i.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r12,%r14), %rdi
	movq	%rbx, %rsi
	movl	$33, %edx
	callq	secp256k1_eckey_pubkey_parse
	testl	%eax, %eax
	je	.LBB26_328
# %bb.54:                               #   in Loop: Header=BB26_53 Depth=1
	addq	$33, %rbx
	addq	$792, %r14                      # imm = 0x318
	cmpq	$51904336, %r14                 # imm = 0x317FF50
	jne	.LBB26_53
# %bb.55:                               # %.preheader.8.i.i.preheader
	movl	$704, %r14d                     # imm = 0x2C0
	.p2align	4, 0x90
.LBB26_56:                              # %.preheader.8.i.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r12,%r14), %rdi
	movq	%rbx, %rsi
	movl	$33, %edx
	callq	secp256k1_eckey_pubkey_parse
	testl	%eax, %eax
	je	.LBB26_328
# %bb.57:                               #   in Loop: Header=BB26_56 Depth=1
	addq	$33, %rbx
	addq	$792, %r14                      # imm = 0x318
	cmpq	$51904424, %r14                 # imm = 0x317FFA8
	jne	.LBB26_56
# %bb.58:
	movq	-112(%rbp), %rdi                # 8-byte Reload
	callq	free@PLT
	movq	-72(%rbp), %rdi                 # 8-byte Reload
	callq	fclose@PLT
	movq	(%r15), %rcx
	leaq	.L.str.83(%rip), %rdi
	movl	$10, %esi
	movl	$1, %edx
	callq	fwrite@PLT
	jmp	.LBB26_120
.LBB26_59:                              # %.thread.i
	movq	-48(%rbp), %rax                 # 8-byte Reload
	leaq	-1(%rax), %rcx
	addq	$14, %rax
	testq	%rcx, %rcx
	cmovsq	%rax, %rcx
	sarq	$4, %rcx
	movq	%rcx, -72(%rbp)                 # 8-byte Spill
	movb	$1, %dl
	xorl	%ecx, %ecx
	movq	%rcx, -344(%rbp)                # 8-byte Spill
	movl	$0, %r15d
	cmpq	$31, %rax
	jae	.LBB26_61
	jmp	.LBB26_67
.LBB26_60:                              # %.thread.thread.i
	movl	$9, %eax
	movq	%rax, -72(%rbp)                 # 8-byte Spill
.LBB26_61:                              # %.lr.ph.i
	movq	%r15, %r13
	movq	-72(%rbp), %rax                 # 8-byte Reload
	cmpq	$2, %rax
	movl	$1, %r12d
	cmovaeq	%rax, %r12
	xorl	%ebx, %ebx
	leaq	init_worker(%rip), %r14
	leaq	-640(%rbp), %r15
	.p2align	4, 0x90
.LBB26_62:                              # =>This Inner Loop Header: Depth=1
	movq	%r15, %rdi
	xorl	%esi, %esi
	movq	%r14, %rdx
	movq	%rbx, %rcx
	callq	pthread_create@PLT
	movl	$0, %ecx
	testl	%eax, %eax
	jne	.LBB26_64
# %bb.63:                               #   in Loop: Header=BB26_62 Depth=1
	movq	-640(%rbp), %rcx
.LBB26_64:                              #   in Loop: Header=BB26_62 Depth=1
	movq	%rcx, -720(%rbp,%rbx,8)
	testq	%rcx, %rcx
	je	.LBB26_336
# %bb.65:                               #   in Loop: Header=BB26_62 Depth=1
	addq	$1, %rbx
	cmpq	%rbx, %r12
	jne	.LBB26_62
# %bb.66:
	xorl	%edx, %edx
	movq	%r13, %r15
.LBB26_67:                              # %._crit_edge.i
	movl	%edx, -88(%rbp)                 # 4-byte Spill
	movq	%r15, -192(%rbp)                # 8-byte Spill
	xorl	%ebx, %ebx
	callq	make_seed
	movq	%rax, %r12
	leaq	-640(%rbp), %r14
	.p2align	4, 0x90
.LBB26_68:                              # %.preheader.i
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB26_69 Depth 2
	movq	%rbx, -112(%rbp)                # 8-byte Spill
	movq	%rbx, %rax
	shlq	$5, %rax
	leaq	priv_bases(%rip), %rcx
	leaq	(%rcx,%rax), %r13
	leaq	(%rax,%rcx), %r15
	addq	$8, %r15
	.p2align	4, 0x90
.LBB26_69:                              #   Parent Loop BB26_68 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	addq	$1, 32(%r12)
	adcq	$0, 40(%r12)
	movaps	.LCPI26_6(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	%xmm0, -640(%rbp)
	movapd	.LCPI26_7(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movapd	%xmm0, -624(%rbp)
	movq	$0, -448(%rbp)
	movl	$48, %edx
	movq	%r14, %rdi
	movq	%r12, %rsi
	callq	secp256k1_sha256_write
	movq	-448(%rbp), %rax
	movq	%rax, %rcx
	shrq	$29, %rcx
	shll	$24, %ecx
	movq	%rax, %rdx
	shrq	$21, %rdx
	andl	$16711680, %edx                 # imm = 0xFF0000
	orl	%ecx, %edx
	movq	%rax, %rcx
	shrq	$37, %rcx
	andl	$65280, %ecx                    # imm = 0xFF00
	orl	%edx, %ecx
	movq	%rax, %rdx
	shrq	$53, %rdx
	movzbl	%dl, %edx
	orl	%ecx, %edx
	movl	%edx, -336(%rbp)
	movl	%eax, %ecx
	shll	$27, %ecx
	movl	%eax, %edx
	shll	$11, %edx
	andl	$16711680, %edx                 # imm = 0xFF0000
	orl	%ecx, %edx
	movl	%eax, %ecx
	shrl	$5, %ecx
	andl	$65280, %ecx                    # imm = 0xFF00
	orl	%edx, %ecx
	movl	$55, %edx
	subl	%eax, %edx
	shrl	$21, %eax
	movzbl	%al, %eax
	orl	%ecx, %eax
	movl	%eax, -332(%rbp)
	andl	$63, %edx
	addq	$1, %rdx
	movq	%r14, %rdi
	leaq	secp256k1_sha256_finalize.pad(%rip), %rsi
	callq	secp256k1_sha256_write
	movl	$8, %edx
	movq	%r14, %rdi
	leaq	-336(%rbp), %rsi
	callq	secp256k1_sha256_write
	movl	-636(%rbp), %r8d
	bswapl	%r8d
	movl	-628(%rbp), %esi
	bswapl	%esi
	movl	-620(%rbp), %edi
	bswapl	%edi
	movl	-612(%rbp), %eax
	bswapl	%eax
	movl	%eax, %edx
	shrl	$24, %edx
	movl	%eax, %ebx
	movl	%eax, %ecx
	shll	$24, %eax
	orl	%edx, %eax
	movl	-616(%rbp), %edx
	bswapl	%edx
	shrl	$8, %ebx
	andl	$65280, %ebx                    # imm = 0xFF00
	orl	%ebx, %eax
	movl	%edi, %r10d
	shrl	$24, %r10d
	shll	$8, %ecx
	andl	$16711680, %ecx                 # imm = 0xFF0000
	orl	%ecx, %eax
	movq	%rdx, %rbx
	movl	%ebx, %ecx
	shrl	$24, %ecx
	shlq	$32, %rcx
	orq	%rax, %rcx
	movl	%ebx, %eax
	shrl	$16, %eax
	movzbl	%al, %eax
	shlq	$40, %rax
	orq	%rcx, %rax
	movzbl	%bh, %ecx
	shlq	$48, %rcx
	orq	%rax, %rcx
	shlq	$56, %rbx
	orq	%rcx, %rbx
	movq	%rbx, %r9
	movl	%edi, %eax
	movl	%edi, %ecx
	shll	$24, %edi
	orl	%r10d, %edi
	movl	-624(%rbp), %edx
	bswapl	%edx
	shrl	$8, %eax
	andl	$65280, %eax                    # imm = 0xFF00
	orl	%eax, %edi
	movl	%esi, %eax
	shrl	$24, %eax
	shll	$8, %ecx
	andl	$16711680, %ecx                 # imm = 0xFF0000
	orl	%ecx, %edi
	movl	%edx, %ecx
	shrl	$24, %ecx
	shlq	$32, %rcx
	orq	%rdi, %rcx
	movl	%edx, %edi
	shrl	$16, %edi
	movzbl	%dil, %edi
	shlq	$40, %rdi
	orq	%rcx, %rdi
	movzbl	%dh, %ecx
	shlq	$48, %rcx
	orq	%rdi, %rcx
	shlq	$56, %rdx
	orq	%rcx, %rdx
	movq	%rdx, %r10
	movl	%esi, %ecx
	movl	%esi, %edi
	shll	$24, %esi
	orl	%eax, %esi
	movl	-632(%rbp), %ebx
	bswapl	%ebx
	shrl	$8, %ecx
	andl	$65280, %ecx                    # imm = 0xFF00
	orl	%ecx, %esi
	movl	%r8d, %eax
	shrl	$24, %eax
	shll	$8, %edi
	andl	$16711680, %edi                 # imm = 0xFF0000
	orl	%edi, %esi
	movl	%ebx, %ecx
	shrl	$24, %ecx
	shlq	$32, %rcx
	orq	%rsi, %rcx
	movl	%ebx, %esi
	shrl	$16, %esi
	movzbl	%sil, %esi
	shlq	$40, %rsi
	orq	%rcx, %rsi
	movzbl	%bh, %ecx
	shlq	$48, %rcx
	orq	%rsi, %rcx
	shlq	$56, %rbx
	orq	%rcx, %rbx
	movl	%r8d, %ecx
	movl	%r8d, %esi
	shll	$24, %r8d
	orl	%eax, %r8d
	movl	-640(%rbp), %eax
	bswapl	%eax
	shrl	$8, %ecx
	andl	$65280, %ecx                    # imm = 0xFF00
	orl	%ecx, %r8d
	movl	%eax, %ecx
	shrl	$16, %ecx
	shll	$8, %esi
	andl	$16711680, %esi                 # imm = 0xFF0000
	orl	%esi, %r8d
	movl	%eax, %esi
	shrl	$24, %esi
	shlq	$32, %rsi
	orq	%r8, %rsi
	movzbl	%cl, %ecx
	shlq	$40, %rcx
	orq	%rsi, %rcx
	movzbl	%ah, %esi
	shlq	$48, %rsi
	orq	%rcx, %rsi
	shlq	$56, %rax
	orq	%rsi, %rax
	cmpq	$-1, %rax
	setne	%cl
	cmpq	$-2, %rbx
	setb	%dl
	orb	%cl, %dl
	movzbl	%dl, %ecx
	xorl	%edx, %edx
	cmpq	$-1, %rbx
	sete	%dl
	movl	%ecx, %esi
	notl	%esi
	andl	%edx, %esi
	xorl	%edx, %edx
	movabsq	$-4994812053365940165, %rdi     # imm = 0xBAAEDCE6AF48A03B
	cmpq	%rdi, %r10
	setb	%dil
	seta	%dl
	orb	%dil, %cl
	movzbl	%cl, %ecx
	notl	%ecx
	andl	%ecx, %edx
	orl	%esi, %edx
	xorl	%esi, %esi
	movabsq	$-4624529908474429120, %rdi     # imm = 0xBFD25E8CD0364140
	cmpq	%rdi, %r9
	seta	%sil
	andl	%ecx, %esi
	orl	%edx, %esi
	movq	%rsi, %rcx
	movabsq	$4624529908474429119, %rdx      # imm = 0x402DA1732FC9BEBF
	imulq	%rdx, %rcx
	movq	%rsi, %rdx
	movabsq	$4994812053365940164, %rdi      # imm = 0x4551231950B75FC4
	imulq	%rdi, %rdx
	addq	%r9, %rcx
	adcq	%r10, %rdx
	movq	%rcx, (%r13)
	movq	%rdx, (%r15)
	adcq	%rsi, %rbx
	movq	%rbx, 8(%r15)
	adcq	$0, %rax
	movq	%rax, 16(%r15)
	testl	%esi, %esi
	jne	.LBB26_69
# %bb.70:                               #   in Loop: Header=BB26_68 Depth=1
	movq	cxt(%rip), %rdi
	addq	$8, %rdi
	movq	-112(%rbp), %rbx                # 8-byte Reload
	movq	%rbx, %rsi
	shlq	$7, %rsi
	leaq	bases(%rip), %rax
	addq	%rax, %rsi
	movq	%r13, %rdx
	callq	secp256k1_ecmult_gen
	addq	$1, %rbx
	cmpq	$65535, %rbx                    # imm = 0xFFFF
	jne	.LBB26_68
# %bb.71:
	movq	%r12, %rdi
	callq	free@PLT
	movq	stdout@GOTPCREL(%rip), %r15
	movq	(%r15), %rsi
	movl	$46, %edi
	callq	putc@PLT
	movq	(%r15), %rdi
	callq	fflush@PLT
	cmpb	$0, -88(%rbp)                   # 1-byte Folded Reload
	jne	.LBB26_74
# %bb.72:                               # %.lr.ph146.preheader.i
	movq	-72(%rbp), %rax                 # 8-byte Reload
	cmpq	$2, %rax
	movl	$1, %r14d
	cmovaeq	%rax, %r14
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB26_73:                              # %.lr.ph146.i
                                        # =>This Inner Loop Header: Depth=1
	movq	-720(%rbp,%rbx,8), %rdi
	xorl	%esi, %esi
	callq	pthread_join@PLT
	addq	$1, %rbx
	cmpq	%rbx, %r14
	jne	.LBB26_73
.LBB26_74:                              # %._crit_edge147.i
	movq	-120(%rbp), %r14                # 8-byte Reload
	testq	%r14, %r14
	movq	-80(%rbp), %rbx                 # 8-byte Reload
	movq	-192(%rbp), %r12                # 8-byte Reload
	je	.LBB26_121
# %bb.75:
	leaq	.L.str.85(%rip), %rsi
	movq	%r12, %rdi
	callq	fopen@PLT
	movq	%rax, -360(%rbp)                # 8-byte Spill
	testq	%rax, %rax
	je	.LBB26_330
# %bb.76:
	movl	$21626582, %edi                 # imm = 0x149FED6
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB26_345
# %bb.77:
	movl	$21626550, %r14d                # imm = 0x149FEB6
	xorl	%ebx, %ebx
	movq	%rax, -744(%rbp)                # 8-byte Spill
	movq	%rax, %r12
	.p2align	4, 0x90
.LBB26_78:                              # =>This Inner Loop Header: Depth=1
	movq	%rbx, -112(%rbp)                # 8-byte Spill
	movq	%r14, -88(%rbp)                 # 8-byte Spill
	movq	%r12, -72(%rbp)                 # 8-byte Spill
	leaq	bases(%rip), %rcx
	leaq	(%rcx,%rbx), %rax
	movq	%rax, -296(%rbp)                # 8-byte Spill
	movl	120(%rbx,%rcx), %eax
	movl	%eax, -560(%rbp)
	leaq	80(%rbx,%rcx), %rbx
	movq	%rbx, -352(%rbp)                # 8-byte Spill
	movq	%rbx, %rdi
	movq	%rbx, %rsi
	callq	secp256k1_fe_inv_var
	leaq	-336(%rbp), %rdi
	movq	%rbx, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -896(%rbp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -728(%rbp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -64(%rbp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-728(%rbp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-64(%rbp), %rsi
	movq	-896(%rbp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	-336(%rbp), %rbx
	leaq	-896(%rbp), %rdi
	movq	-352(%rbp), %rsi                # 8-byte Reload
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -64(%rbp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -752(%rbp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -728(%rbp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-752(%rbp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-728(%rbp), %rsi
	movq	-64(%rbp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	-336(%rbp), %rbx
	movq	-296(%rbp), %rdi                # 8-byte Reload
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -64(%rbp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -752(%rbp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -728(%rbp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-752(%rbp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-728(%rbp), %rsi
	movq	-64(%rbp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	-112(%rbp), %rax                # 8-byte Reload
	leaq	bases(%rip), %rcx
	leaq	(%rax,%rcx), %rdi
	addq	$40, %rdi
	leaq	-896(%rbp), %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -64(%rbp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -752(%rbp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -728(%rbp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-752(%rbp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-728(%rbp), %rsi
	movq	-64(%rbp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	-112(%rbp), %rbx                # 8-byte Reload
	movq	-88(%rbp), %r14                 # 8-byte Reload
	movq	-72(%rbp), %r12                 # 8-byte Reload
	leaq	bases(%rip), %rdx
	movq	$1, 80(%rbx,%rdx)
	xorpd	%xmm0, %xmm0
	movupd	%xmm0, 88(%rbx,%rdx)
	movupd	%xmm0, 104(%rbx,%rdx)
	movups	(%rbx,%rdx), %xmm0
	movupd	16(%rbx,%rdx), %xmm1
	movaps	%xmm0, -640(%rbp)
	movapd	%xmm1, -624(%rbp)
	movq	32(%rbx,%rdx), %rax
	movq	%rax, -608(%rbp)
	movq	72(%rbx,%rdx), %rax
	leaq	-600(%rbp), %rcx
	movq	%rax, 32(%rcx)
	movups	56(%rbx,%rdx), %xmm0
	movups	%xmm0, 16(%rcx)
	movupd	40(%rbx,%rdx), %xmm0
	movupd	%xmm0, (%rcx)
	leaq	-640(%rbp), %rdi
	movq	%r12, %rsi
	leaq	-52(%rbp), %rdx
	movl	$1, %ecx
	callq	secp256k1_eckey_pubkey_serialize
	testl	%eax, %eax
	je	.LBB26_329
# %bb.79:                               #   in Loop: Header=BB26_78 Depth=1
	addq	$33, %r12
	addq	$-33, %r14
	subq	$-128, %rbx
	cmpq	$8388480, %rbx                  # imm = 0x7FFF80
	jne	.LBB26_78
# %bb.80:                               # %.preheader.preheader.i76.i.preheader
	leaq	offsets(%rip), %r15
	xorl	%r13d, %r13d
	leaq	-52(%rbp), %rbx
	.p2align	4, 0x90
.LBB26_81:                              # %.preheader.preheader.i76.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r12,%r13), %rsi
	movq	%r15, %rdi
	movq	%rbx, %rdx
	movl	$1, %ecx
	callq	secp256k1_eckey_pubkey_serialize
	testl	%eax, %eax
	je	.LBB26_329
# %bb.82:                               #   in Loop: Header=BB26_81 Depth=1
	addq	$-33, %r14
	addq	$33, %r13
	addq	$792, %r15                      # imm = 0x318
	cmpq	$2162655, %r13                  # imm = 0x20FFDF
	jne	.LBB26_81
# %bb.83:                               # %.preheader.1.i77.i.preheader
	addq	%r13, %r12
	movl	$88, %r13d
	leaq	offsets(%rip), %r15
	leaq	-52(%rbp), %rbx
	.p2align	4, 0x90
.LBB26_84:                              # %.preheader.1.i77.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r15,%r13), %rdi
	movq	%r12, %rsi
	movq	%rbx, %rdx
	movl	$1, %ecx
	callq	secp256k1_eckey_pubkey_serialize
	testl	%eax, %eax
	je	.LBB26_329
# %bb.85:                               #   in Loop: Header=BB26_84 Depth=1
	addq	$33, %r12
	addq	$-33, %r14
	addq	$792, %r13                      # imm = 0x318
	cmpq	$51903808, %r13                 # imm = 0x317FD40
	jne	.LBB26_84
# %bb.86:                               # %.preheader.2.i78.i.preheader
	movl	$176, %r13d
	leaq	-52(%rbp), %rbx
	.p2align	4, 0x90
.LBB26_87:                              # %.preheader.2.i78.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r15,%r13), %rdi
	movq	%r12, %rsi
	movq	%rbx, %rdx
	movl	$1, %ecx
	callq	secp256k1_eckey_pubkey_serialize
	testl	%eax, %eax
	je	.LBB26_329
# %bb.88:                               #   in Loop: Header=BB26_87 Depth=1
	addq	$33, %r12
	addq	$-33, %r14
	addq	$792, %r13                      # imm = 0x318
	cmpq	$51903896, %r13                 # imm = 0x317FD98
	jne	.LBB26_87
# %bb.89:                               # %.preheader.3.i79.i.preheader
	movl	$264, %r13d                     # imm = 0x108
	leaq	-52(%rbp), %rbx
	.p2align	4, 0x90
.LBB26_90:                              # %.preheader.3.i79.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r15,%r13), %rdi
	movq	%r12, %rsi
	movq	%rbx, %rdx
	movl	$1, %ecx
	callq	secp256k1_eckey_pubkey_serialize
	testl	%eax, %eax
	je	.LBB26_329
# %bb.91:                               #   in Loop: Header=BB26_90 Depth=1
	addq	$33, %r12
	addq	$-33, %r14
	addq	$792, %r13                      # imm = 0x318
	cmpq	$51903984, %r13                 # imm = 0x317FDF0
	jne	.LBB26_90
# %bb.92:                               # %.preheader.4.i80.i.preheader
	movl	$352, %r13d                     # imm = 0x160
	leaq	-52(%rbp), %rbx
	.p2align	4, 0x90
.LBB26_93:                              # %.preheader.4.i80.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r15,%r13), %rdi
	movq	%r12, %rsi
	movq	%rbx, %rdx
	movl	$1, %ecx
	callq	secp256k1_eckey_pubkey_serialize
	testl	%eax, %eax
	je	.LBB26_329
# %bb.94:                               #   in Loop: Header=BB26_93 Depth=1
	addq	$33, %r12
	addq	$-33, %r14
	addq	$792, %r13                      # imm = 0x318
	cmpq	$51904072, %r13                 # imm = 0x317FE48
	jne	.LBB26_93
# %bb.95:                               # %.preheader.5.i81.i.preheader
	movl	$440, %r13d                     # imm = 0x1B8
	leaq	-52(%rbp), %rbx
	.p2align	4, 0x90
.LBB26_96:                              # %.preheader.5.i81.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r15,%r13), %rdi
	movq	%r12, %rsi
	movq	%rbx, %rdx
	movl	$1, %ecx
	callq	secp256k1_eckey_pubkey_serialize
	testl	%eax, %eax
	je	.LBB26_329
# %bb.97:                               #   in Loop: Header=BB26_96 Depth=1
	addq	$33, %r12
	addq	$-33, %r14
	addq	$792, %r13                      # imm = 0x318
	cmpq	$51904160, %r13                 # imm = 0x317FEA0
	jne	.LBB26_96
# %bb.98:                               # %.preheader.6.i82.i.preheader
	movl	$528, %r13d                     # imm = 0x210
	leaq	-52(%rbp), %rbx
	.p2align	4, 0x90
.LBB26_99:                              # %.preheader.6.i82.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r15,%r13), %rdi
	movq	%r12, %rsi
	movq	%rbx, %rdx
	movl	$1, %ecx
	callq	secp256k1_eckey_pubkey_serialize
	testl	%eax, %eax
	je	.LBB26_329
# %bb.100:                              #   in Loop: Header=BB26_99 Depth=1
	addq	$33, %r12
	addq	$-33, %r14
	addq	$792, %r13                      # imm = 0x318
	cmpq	$51904248, %r13                 # imm = 0x317FEF8
	jne	.LBB26_99
# %bb.101:                              # %.preheader.7.i83.i.preheader
	movl	$616, %r13d                     # imm = 0x268
	leaq	-52(%rbp), %rbx
	.p2align	4, 0x90
.LBB26_102:                             # %.preheader.7.i83.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r15,%r13), %rdi
	movq	%r12, %rsi
	movq	%rbx, %rdx
	movl	$1, %ecx
	callq	secp256k1_eckey_pubkey_serialize
	testl	%eax, %eax
	je	.LBB26_329
# %bb.103:                              #   in Loop: Header=BB26_102 Depth=1
	addq	$33, %r12
	addq	$-33, %r14
	addq	$792, %r13                      # imm = 0x318
	cmpq	$51904336, %r13                 # imm = 0x317FF50
	jne	.LBB26_102
# %bb.104:                              # %.preheader.8.i84.i.preheader
	movl	$704, %r13d                     # imm = 0x2C0
	leaq	-52(%rbp), %rbx
	.p2align	4, 0x90
.LBB26_105:                             # %.preheader.8.i84.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r15,%r13), %rdi
	movq	%r12, %rsi
	movq	%rbx, %rdx
	movl	$1, %ecx
	callq	secp256k1_eckey_pubkey_serialize
	testl	%eax, %eax
	je	.LBB26_329
# %bb.106:                              #   in Loop: Header=BB26_105 Depth=1
	addq	$33, %r12
	addq	$792, %r13                      # imm = 0x318
	addq	$-33, %r14
	cmpq	$51904424, %r13                 # imm = 0x317FFA8
	jne	.LBB26_105
# %bb.107:
	testq	%r14, %r14
	jne	.LBB26_346
# %bb.108:
	movaps	.LCPI26_6(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	%xmm0, -640(%rbp)
	movdqa	.LCPI26_7(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movdqa	%xmm0, -624(%rbp)
	movq	$0, -448(%rbp)
	leaq	-640(%rbp), %r14
	movl	$21626550, %edx                 # imm = 0x149FEB6
	movq	%r14, %rdi
	movq	-744(%rbp), %r15                # 8-byte Reload
	movq	%r15, %rsi
	callq	secp256k1_sha256_write
	movq	-448(%rbp), %rax
	movq	%rax, %rcx
	movq	%rax, %rsi
	movq	%rax, %r8
	movl	%eax, %r9d
	movl	%eax, %edi
	movl	%eax, %ebx
	movl	%eax, %r10d
	movl	$55, %edx
	subl	%eax, %edx
	shrq	$29, %rax
	shll	$24, %eax
	shrq	$21, %rcx
	andl	$16711680, %ecx                 # imm = 0xFF0000
	orl	%eax, %ecx
	shrq	$37, %rsi
	andl	$65280, %esi                    # imm = 0xFF00
	orl	%ecx, %esi
	shrq	$53, %r8
	movzbl	%r8b, %eax
	orl	%esi, %eax
	movl	%eax, -336(%rbp)
	shll	$27, %r9d
	shll	$11, %edi
	andl	$16711680, %edi                 # imm = 0xFF0000
	orl	%r9d, %edi
	shrl	$5, %ebx
	andl	$65280, %ebx                    # imm = 0xFF00
	orl	%edi, %ebx
	shrl	$21, %r10d
	movzbl	%r10b, %eax
	orl	%ebx, %eax
	movl	%eax, -332(%rbp)
	andl	$63, %edx
	addq	$1, %rdx
	leaq	secp256k1_sha256_finalize.pad(%rip), %rsi
	movq	%r14, %rdi
	callq	secp256k1_sha256_write
	leaq	-336(%rbp), %rsi
	movl	$8, %edx
	movq	%r14, %rdi
	callq	secp256k1_sha256_write
	pxor	%xmm0, %xmm0
	movdqa	-640(%rbp), %xmm1
	movdqa	-624(%rbp), %xmm2
	movdqa	%xmm1, %xmm3
	punpckhbw	%xmm0, %xmm3            # xmm3 = xmm3[8],xmm0[8],xmm3[9],xmm0[9],xmm3[10],xmm0[10],xmm3[11],xmm0[11],xmm3[12],xmm0[12],xmm3[13],xmm0[13],xmm3[14],xmm0[14],xmm3[15],xmm0[15]
	pshuflw	$27, %xmm3, %xmm3               # xmm3 = xmm3[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm3, %xmm3               # xmm3 = xmm3[0,1,2,3,7,6,5,4]
	punpcklbw	%xmm0, %xmm1            # xmm1 = xmm1[0],xmm0[0],xmm1[1],xmm0[1],xmm1[2],xmm0[2],xmm1[3],xmm0[3],xmm1[4],xmm0[4],xmm1[5],xmm0[5],xmm1[6],xmm0[6],xmm1[7],xmm0[7]
	pshuflw	$27, %xmm1, %xmm1               # xmm1 = xmm1[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm1, %xmm1               # xmm1 = xmm1[0,1,2,3,7,6,5,4]
	packuswb	%xmm3, %xmm1
	movdqa	%xmm2, %xmm3
	punpckhbw	%xmm0, %xmm3            # xmm3 = xmm3[8],xmm0[8],xmm3[9],xmm0[9],xmm3[10],xmm0[10],xmm3[11],xmm0[11],xmm3[12],xmm0[12],xmm3[13],xmm0[13],xmm3[14],xmm0[14],xmm3[15],xmm0[15]
	pshuflw	$27, %xmm3, %xmm3               # xmm3 = xmm3[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm3, %xmm3               # xmm3 = xmm3[0,1,2,3,7,6,5,4]
	punpcklbw	%xmm0, %xmm2            # xmm2 = xmm2[0],xmm0[0],xmm2[1],xmm0[1],xmm2[2],xmm0[2],xmm2[3],xmm0[3],xmm2[4],xmm0[4],xmm2[5],xmm0[5],xmm2[6],xmm0[6],xmm2[7],xmm0[7]
	pshuflw	$27, %xmm2, %xmm0               # xmm0 = xmm2[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm0, %xmm0               # xmm0 = xmm0[0,1,2,3,7,6,5,4]
	packuswb	%xmm3, %xmm0
	movdqu	%xmm1, (%r12)
	movdqu	%xmm0, 16(%r12)
	movl	$21626582, %esi                 # imm = 0x149FED6
	movl	$1, %edx
	movq	%r15, %rdi
	movq	-360(%rbp), %rcx                # 8-byte Reload
	callq	fwrite@PLT
	cmpq	$1, %rax
	jne	.LBB26_329
# %bb.109:
	movq	-744(%rbp), %rdi                # 8-byte Reload
	callq	free@PLT
	movq	-360(%rbp), %rdi                # 8-byte Reload
	callq	fclose@PLT
	leaq	.L.str.85(%rip), %rsi
	movq	-344(%rbp), %rdi                # 8-byte Reload
	callq	fopen@PLT
	testq	%rax, %rax
	je	.LBB26_342
# %bb.110:
	movq	%rax, %r14
	movl	$20971232, %edi                 # imm = 0x13FFEE0
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB26_347
# %bb.111:                              # %.preheader46.i.i.preheader
	movq	%rax, %r12
	xorl	%edx, %edx
	movdqa	.LCPI26_6(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	.LCPI26_7(%rip), %xmm1          # xmm1 = [1359893119,2600822924,528734635,1541459225]
	leaq	priv_bases(%rip), %rsi
	.p2align	4, 0x90
.LBB26_112:                             # %.preheader46.i.i
                                        # =>This Inner Loop Header: Depth=1
	movq	24(%rdx,%rsi), %rax
	bswapq	%rax
	movq	16(%rdx,%rsi), %rcx
	bswapq	%rcx
	movq	%rax, (%r12,%rdx)
	movq	8(%rdx,%rsi), %rax
	bswapq	%rax
	movq	%rcx, 8(%r12,%rdx)
	movq	(%rdx,%rsi), %rcx
	bswapq	%rcx
	movq	%rax, 16(%r12,%rdx)
	movq	%rcx, 24(%r12,%rdx)
	addq	$32, %rdx
	cmpq	$2097120, %rdx                  # imm = 0x1FFFE0
	jne	.LBB26_112
# %bb.113:                              # %.preheader.i.i.preheader
	addq	%r12, %rdx
	leaq	priv_offsets+24(%rip), %rax
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB26_114:                             # %.preheader.i.i
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB26_115 Depth 2
	movq	%rdx, %r13
	movq	%rax, %rdx
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB26_115:                             #   Parent Loop BB26_114 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movzbl	7(%rdx), %ecx
	movb	%cl, (%r13,%rbx)
	movzbl	6(%rdx), %ecx
	movb	%cl, 1(%r13,%rbx)
	movzbl	5(%rdx), %ecx
	movb	%cl, 2(%r13,%rbx)
	movzbl	4(%rdx), %ecx
	movb	%cl, 3(%r13,%rbx)
	movzbl	3(%rdx), %ecx
	movb	%cl, 4(%r13,%rbx)
	movzbl	2(%rdx), %ecx
	movb	%cl, 5(%r13,%rbx)
	movzbl	1(%rdx), %ecx
	movb	%cl, 6(%r13,%rbx)
	movzbl	(%rdx), %ecx
	movb	%cl, 7(%r13,%rbx)
	movzbl	-1(%rdx), %ecx
	movb	%cl, 8(%r13,%rbx)
	movzbl	-2(%rdx), %ecx
	movb	%cl, 9(%r13,%rbx)
	movzbl	-3(%rdx), %ecx
	movb	%cl, 10(%r13,%rbx)
	movzbl	-4(%rdx), %ecx
	movb	%cl, 11(%r13,%rbx)
	movzbl	-5(%rdx), %ecx
	movb	%cl, 12(%r13,%rbx)
	movzbl	-6(%rdx), %ecx
	movb	%cl, 13(%r13,%rbx)
	movzbl	-7(%rdx), %ecx
	movb	%cl, 14(%r13,%rbx)
	movzbl	-8(%rdx), %ecx
	movb	%cl, 15(%r13,%rbx)
	movzbl	-9(%rdx), %ecx
	movb	%cl, 16(%r13,%rbx)
	movzbl	-10(%rdx), %ecx
	movb	%cl, 17(%r13,%rbx)
	movzbl	-11(%rdx), %ecx
	movb	%cl, 18(%r13,%rbx)
	movzbl	-12(%rdx), %ecx
	movb	%cl, 19(%r13,%rbx)
	movzbl	-13(%rdx), %ecx
	movb	%cl, 20(%r13,%rbx)
	movzbl	-14(%rdx), %ecx
	movb	%cl, 21(%r13,%rbx)
	movzbl	-15(%rdx), %ecx
	movb	%cl, 22(%r13,%rbx)
	movzbl	-16(%rdx), %ecx
	movb	%cl, 23(%r13,%rbx)
	movzbl	-17(%rdx), %ecx
	movb	%cl, 24(%r13,%rbx)
	movzbl	-18(%rdx), %ecx
	movb	%cl, 25(%r13,%rbx)
	movzbl	-19(%rdx), %ecx
	movb	%cl, 26(%r13,%rbx)
	movzbl	-20(%rdx), %ecx
	movb	%cl, 27(%r13,%rbx)
	movzbl	-21(%rdx), %ecx
	movb	%cl, 28(%r13,%rbx)
	movzbl	-22(%rdx), %ecx
	movb	%cl, 29(%r13,%rbx)
	movzbl	-23(%rdx), %ecx
	movb	%cl, 30(%r13,%rbx)
	movzbl	-24(%rdx), %ecx
	movb	%cl, 31(%r13,%rbx)
	addq	$32, %rbx
	addq	$288, %rdx                      # imm = 0x120
	cmpq	$2097120, %rbx                  # imm = 0x1FFFE0
	jne	.LBB26_115
# %bb.116:                              #   in Loop: Header=BB26_114 Depth=1
	addq	$1, %rsi
	addq	$32, %rax
	leaq	(%rbx,%r13), %rdx
	cmpq	$9, %rsi
	jne	.LBB26_114
# %bb.117:
	movq	%r12, %rax
	subq	%r13, %rax
	addq	$20971200, %rax                 # imm = 0x13FFEC0
	cmpq	%rbx, %rax
	jne	.LBB26_348
# %bb.118:
	movdqa	%xmm0, -640(%rbp)
	movaps	%xmm1, -624(%rbp)
	movq	$0, -448(%rbp)
	leaq	-640(%rbp), %r15
	movl	$20971200, %edx                 # imm = 0x13FFEC0
	movq	%r15, %rdi
	movq	%r12, %rsi
	callq	secp256k1_sha256_write
	movq	-448(%rbp), %rax
	movq	%rax, %r10
	movq	%rax, %rsi
	movq	%rax, %r8
	movl	%eax, %r9d
	movl	%eax, %edi
	movl	%eax, %ecx
	movl	%eax, %r11d
	movl	$55, %edx
	subl	%eax, %edx
	shrq	$29, %rax
	shll	$24, %eax
	shrq	$21, %r10
	andl	$16711680, %r10d                # imm = 0xFF0000
	orl	%eax, %r10d
	shrq	$37, %rsi
	andl	$65280, %esi                    # imm = 0xFF00
	orl	%r10d, %esi
	shrq	$53, %r8
	movzbl	%r8b, %eax
	orl	%esi, %eax
	movl	%eax, -336(%rbp)
	shll	$27, %r9d
	shll	$11, %edi
	andl	$16711680, %edi                 # imm = 0xFF0000
	orl	%r9d, %edi
	shrl	$5, %ecx
	andl	$65280, %ecx                    # imm = 0xFF00
	orl	%edi, %ecx
	shrl	$21, %r11d
	movzbl	%r11b, %eax
	orl	%ecx, %eax
	movl	%eax, -332(%rbp)
	andl	$63, %edx
	addq	$1, %rdx
	leaq	secp256k1_sha256_finalize.pad(%rip), %rsi
	movq	%r15, %rdi
	callq	secp256k1_sha256_write
	leaq	-336(%rbp), %rsi
	movl	$8, %edx
	movq	%r15, %rdi
	callq	secp256k1_sha256_write
	pxor	%xmm0, %xmm0
	movdqa	-640(%rbp), %xmm1
	movdqa	-624(%rbp), %xmm2
	movdqa	%xmm1, %xmm3
	punpckhbw	%xmm0, %xmm3            # xmm3 = xmm3[8],xmm0[8],xmm3[9],xmm0[9],xmm3[10],xmm0[10],xmm3[11],xmm0[11],xmm3[12],xmm0[12],xmm3[13],xmm0[13],xmm3[14],xmm0[14],xmm3[15],xmm0[15]
	pshuflw	$27, %xmm3, %xmm3               # xmm3 = xmm3[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm3, %xmm3               # xmm3 = xmm3[0,1,2,3,7,6,5,4]
	punpcklbw	%xmm0, %xmm1            # xmm1 = xmm1[0],xmm0[0],xmm1[1],xmm0[1],xmm1[2],xmm0[2],xmm1[3],xmm0[3],xmm1[4],xmm0[4],xmm1[5],xmm0[5],xmm1[6],xmm0[6],xmm1[7],xmm0[7]
	pshuflw	$27, %xmm1, %xmm1               # xmm1 = xmm1[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm1, %xmm1               # xmm1 = xmm1[0,1,2,3,7,6,5,4]
	packuswb	%xmm3, %xmm1
	movdqa	%xmm2, %xmm3
	punpckhbw	%xmm0, %xmm3            # xmm3 = xmm3[8],xmm0[8],xmm3[9],xmm0[9],xmm3[10],xmm0[10],xmm3[11],xmm0[11],xmm3[12],xmm0[12],xmm3[13],xmm0[13],xmm3[14],xmm0[14],xmm3[15],xmm0[15]
	pshuflw	$27, %xmm3, %xmm3               # xmm3 = xmm3[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm3, %xmm3               # xmm3 = xmm3[0,1,2,3,7,6,5,4]
	punpcklbw	%xmm0, %xmm2            # xmm2 = xmm2[0],xmm0[0],xmm2[1],xmm0[1],xmm2[2],xmm0[2],xmm2[3],xmm0[3],xmm2[4],xmm0[4],xmm2[5],xmm0[5],xmm2[6],xmm0[6],xmm2[7],xmm0[7]
	pshuflw	$27, %xmm2, %xmm0               # xmm0 = xmm2[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm0, %xmm0               # xmm0 = xmm0[0,1,2,3,7,6,5,4]
	packuswb	%xmm3, %xmm0
	movdqu	%xmm1, (%r13,%rbx)
	movdqu	%xmm0, 16(%r13,%rbx)
	movl	$20971232, %esi                 # imm = 0x13FFEE0
	movl	$1, %edx
	movq	%r12, %rdi
	movq	%r14, %rcx
	callq	fwrite@PLT
	cmpq	$1, %rax
	jne	.LBB26_342
# %bb.119:
	movq	%r12, %rdi
	callq	free@PLT
	movq	%r14, %rdi
	callq	fclose@PLT
	movq	stdout@GOTPCREL(%rip), %r15
.LBB26_120:                             # %init.exit
	movq	-80(%rbp), %rbx                 # 8-byte Reload
	movq	-120(%rbp), %r14                # 8-byte Reload
.LBB26_121:                             # %init.exit
	leaq	.Lstr.6(%rip), %rdi
	callq	puts@PLT
	leaq	.L.str.42(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movq	(%r15), %rdi
	callq	fflush@PLT
	testq	%r14, %r14
	je	.LBB26_172
# %bb.122:
	movq	%r14, %rdi
	callq	strlen@PLT
	movq	%rax, %rbx
	leaq	6(%rax), %rdi
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB26_340
# %bb.123:
	movq	%rax, %r12
	movq	%rax, %rdi
	movq	%r14, %rsi
	movq	%rbx, %rdx
	callq	memcpy@PLT
	movl	$1919907630, (%r12,%rbx)        # imm = 0x726F772E
	movw	$107, 4(%r12,%rbx)
	movb	$0, stop(%rip)
	leaq	.L.str.44(%rip), %rsi
	movq	%r12, %rdi
	callq	fopen@PLT
	testq	%rax, %rax
	je	.LBB26_180
# %bb.124:
	movq	%rax, %r13
	movq	-80(%rbp), %rax                 # 8-byte Reload
	leaq	-1(%rax), %rcx
	shrq	$4, %rcx
	movq	%rcx, -352(%rbp)                # 8-byte Spill
	addq	$1, %rcx
	movq	%rcx, -120(%rbp)                # 8-byte Spill
	movl	$16, %ecx
	subl	%eax, %ecx
	movq	%rcx, -192(%rbp)                # 8-byte Spill
	movq	%r12, -88(%rbp)                 # 8-byte Spill
	movq	%r13, -296(%rbp)                # 8-byte Spill
	.p2align	4, 0x90
.LBB26_125:                             # %.loopexit.i
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB26_144 Depth 2
                                        #     Child Loop BB26_139 Depth 2
                                        #     Child Loop BB26_151 Depth 2
                                        #     Child Loop BB26_160 Depth 2
                                        #     Child Loop BB26_165 Depth 2
	movq	%r13, %rdi
	callq	getc@PLT
	cmpb	$87, %al
	je	.LBB26_127
# %bb.126:                              # %.loopexit.i
                                        #   in Loop: Header=BB26_125 Depth=1
	movsbl	%al, %eax
	cmpl	$35, %eax
	je	.LBB26_144
	jmp	.LBB26_173
	.p2align	4, 0x90
.LBB26_127:                             #   in Loop: Header=BB26_125 Depth=1
	movq	%r13, %rdi
	callq	getc@PLT
                                        # kill: def $eax killed $eax def $rax
	movq	%rax, -72(%rbp)                 # 8-byte Spill
	movb	%al, -336(%rbp)
	movq	%r13, %rdi
	callq	getc@PLT
	movq	%r13, %rbx
	movl	%eax, %r13d
	movb	%r13b, -335(%rbp)
	movq	%rbx, %rdi
	callq	getc@PLT
	movl	%eax, %r14d
	movb	%r14b, -334(%rbp)
	movb	$0, -333(%rbp)
	movq	%rbx, %rdi
	callq	getc@PLT
	movl	%eax, %r12d
	movb	%r12b, -896(%rbp)
	movq	%rbx, %rdi
	callq	getc@PLT
	movl	%eax, %r15d
	movb	%r15b, -895(%rbp)
	movq	%rbx, %rdi
	callq	getc@PLT
	movl	%eax, %ebx
	movb	%bl, -894(%rbp)
	movb	$0, -893(%rbp)
	callq	__ctype_b_loc@PLT
	movq	%rax, -112(%rbp)                # 8-byte Spill
	movq	(%rax), %rax
	movsbq	-72(%rbp), %rcx                 # 1-byte Folded Reload
	testb	$8, 1(%rax,%rcx,2)
	je	.LBB26_175
# %bb.128:                              #   in Loop: Header=BB26_125 Depth=1
	movsbq	%r13b, %rcx
	testb	$8, 1(%rax,%rcx,2)
	je	.LBB26_175
# %bb.129:                              #   in Loop: Header=BB26_125 Depth=1
	movsbq	%r14b, %rcx
	testb	$8, 1(%rax,%rcx,2)
	movq	-296(%rbp), %r13                # 8-byte Reload
	je	.LBB26_176
# %bb.130:                              #   in Loop: Header=BB26_125 Depth=1
	movsbq	%r12b, %rcx
	testb	$8, 1(%rax,%rcx,2)
	je	.LBB26_176
# %bb.131:                              #   in Loop: Header=BB26_125 Depth=1
	movsbq	%r15b, %rcx
	testb	$8, 1(%rax,%rcx,2)
	movq	-88(%rbp), %r12                 # 8-byte Reload
	je	.LBB26_178
# %bb.132:                              #   in Loop: Header=BB26_125 Depth=1
	movsbq	%bl, %rcx
	testb	$8, 1(%rax,%rcx,2)
	movq	stdout@GOTPCREL(%rip), %r15
	je	.LBB26_173
# %bb.133:                              #   in Loop: Header=BB26_125 Depth=1
	leaq	-336(%rbp), %rdi
	xorl	%esi, %esi
	movl	$10, %edx
	callq	strtoul@PLT
	cmpq	-48(%rbp), %rax                 # 8-byte Folded Reload
	jne	.LBB26_143
# %bb.134:                              #   in Loop: Header=BB26_125 Depth=1
	leaq	-896(%rbp), %rdi
	xorl	%esi, %esi
	movl	$10, %edx
	callq	strtoul@PLT
	cmpq	-80(%rbp), %rax                 # 8-byte Folded Reload
	jne	.LBB26_143
# %bb.135:                              #   in Loop: Header=BB26_125 Depth=1
	movq	%r13, %rdi
	callq	getc@PLT
	cmpl	$32, %eax
	jne	.LBB26_173
# %bb.136:                              #   in Loop: Header=BB26_125 Depth=1
	movq	%r13, %rdi
	callq	getc@PLT
	movl	%eax, %ebx
	movb	%bl, -64(%rbp)
	movq	%r13, %rdi
	callq	getc@PLT
	movb	%al, -63(%rbp)
	movb	$0, -62(%rbp)
	movq	-112(%rbp), %rcx                # 8-byte Reload
	movq	(%rcx), %rcx
	movsbq	%bl, %rdx
	testb	$16, 1(%rcx,%rdx,2)
	je	.LBB26_173
# %bb.137:                              # %.lr.ph637.preheader
                                        #   in Loop: Header=BB26_125 Depth=1
	movsbq	%al, %rax
	testb	$16, 1(%rcx,%rax,2)
	je	.LBB26_173
# %bb.138:                              # %.critedge.i371.preheader
                                        #   in Loop: Header=BB26_125 Depth=1
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB26_139:                             # %.critedge.i371
                                        #   Parent Loop BB26_125 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%rax, %rbx
	leaq	-64(%rbp), %rdi
	xorl	%esi, %esi
	movl	$16, %edx
	callq	strtoul@PLT
	movb	%al, -640(%rbp,%rbx)
	cmpq	$19, %rbx
	je	.LBB26_146
# %bb.140:                              #   in Loop: Header=BB26_139 Depth=2
	movq	%r13, %rdi
	callq	getc@PLT
	movl	%eax, %r14d
	movb	%r14b, -64(%rbp)
	movq	%r13, %rdi
	callq	getc@PLT
	movb	%al, -63(%rbp)
	movb	$0, -62(%rbp)
	movq	-112(%rbp), %rcx                # 8-byte Reload
	movq	(%rcx), %rcx
	movsbq	%r14b, %rdx
	testb	$16, 1(%rcx,%rdx,2)
	je	.LBB26_142
# %bb.141:                              # %.lr.ph637
                                        #   in Loop: Header=BB26_139 Depth=2
	movsbq	%al, %rdx
	leaq	1(%rbx), %rax
	testb	$16, 1(%rcx,%rdx,2)
	jne	.LBB26_139
.LBB26_142:                             # %._crit_edge638
                                        #   in Loop: Header=BB26_125 Depth=1
	cmpq	$19, %rbx
	jae	.LBB26_147
	jmp	.LBB26_173
	.p2align	4, 0x90
.LBB26_143:                             #   in Loop: Header=BB26_125 Depth=1
	movq	(%r15), %rsi
	movl	$95, %edi
	callq	putc@PLT
	.p2align	4, 0x90
.LBB26_144:                             #   Parent Loop BB26_125 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%r13, %rdi
	callq	getc@PLT
	shll	$24, %eax
	cmpl	$-16777216, %eax                # imm = 0xFF000000
	je	.LBB26_125
# %bb.145:                              #   in Loop: Header=BB26_144 Depth=2
	cmpl	$167772160, %eax                # imm = 0xA000000
	jne	.LBB26_144
	jmp	.LBB26_125
.LBB26_146:                             # %read_hash160.exit373
                                        #   in Loop: Header=BB26_125 Depth=1
	jb	.LBB26_173
.LBB26_147:                             #   in Loop: Header=BB26_125 Depth=1
	movq	%r13, %rdi
	callq	getc@PLT
	cmpl	$32, %eax
	jne	.LBB26_173
# %bb.148:                              #   in Loop: Header=BB26_125 Depth=1
	movq	%r13, %rdi
	callq	getc@PLT
	movl	%eax, %ebx
	movb	%bl, -64(%rbp)
	movq	%r13, %rdi
	callq	getc@PLT
	movb	%al, -63(%rbp)
	movb	$0, -62(%rbp)
	movq	-112(%rbp), %rcx                # 8-byte Reload
	movq	(%rcx), %rcx
	movsbq	%bl, %rdx
	testb	$16, 1(%rcx,%rdx,2)
	je	.LBB26_173
# %bb.149:                              # %.lr.ph645.preheader
                                        #   in Loop: Header=BB26_125 Depth=1
	movsbq	%al, %rax
	testb	$16, 1(%rcx,%rax,2)
	je	.LBB26_173
# %bb.150:                              # %.critedge.i365.preheader
                                        #   in Loop: Header=BB26_125 Depth=1
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB26_151:                             # %.critedge.i365
                                        #   Parent Loop BB26_125 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%rax, %rbx
	leaq	-64(%rbp), %rdi
	xorl	%esi, %esi
	movl	$16, %edx
	callq	strtoul@PLT
	movb	%al, -720(%rbp,%rbx)
	cmpq	$19, %rbx
	je	.LBB26_155
# %bb.152:                              #   in Loop: Header=BB26_151 Depth=2
	movq	%r13, %rdi
	callq	getc@PLT
	movl	%eax, %r14d
	movb	%r14b, -64(%rbp)
	movq	%r13, %rdi
	callq	getc@PLT
	movb	%al, -63(%rbp)
	movb	$0, -62(%rbp)
	movq	-112(%rbp), %rcx                # 8-byte Reload
	movq	(%rcx), %rcx
	movsbq	%r14b, %rdx
	testb	$16, 1(%rcx,%rdx,2)
	je	.LBB26_154
# %bb.153:                              # %.lr.ph645
                                        #   in Loop: Header=BB26_151 Depth=2
	movsbq	%al, %rdx
	leaq	1(%rbx), %rax
	testb	$16, 1(%rcx,%rdx,2)
	jne	.LBB26_151
.LBB26_154:                             # %._crit_edge646
                                        #   in Loop: Header=BB26_125 Depth=1
	cmpq	$19, %rbx
.LBB26_155:                             # %read_hash160.exit
                                        #   in Loop: Header=BB26_125 Depth=1
	jb	.LBB26_173
# %bb.156:                              #   in Loop: Header=BB26_125 Depth=1
	movq	%r13, %rdi
	callq	getc@PLT
	cmpl	$10, %eax
	jne	.LBB26_173
# %bb.157:                              #   in Loop: Header=BB26_125 Depth=1
	movq	-80(%rbp), %rsi                 # 8-byte Reload
	testq	%rsi, %rsi
	movl	$65535, %r14d                   # imm = 0xFFFF
	je	.LBB26_168
# %bb.158:                              # %.preheader.i.i293.preheader
                                        #   in Loop: Header=BB26_125 Depth=1
	xorl	%eax, %eax
	movq	%rsi, %rdx
	xorl	%r8d, %r8d
	xorl	%edi, %edi
	movq	-352(%rbp), %r9                 # 8-byte Reload
	movq	-120(%rbp), %r10                # 8-byte Reload
	movq	-192(%rbp), %r11                # 8-byte Reload
	jmp	.LBB26_160
	.p2align	4, 0x90
.LBB26_159:                             # %get_bits.exit.thread.i.i
                                        #   in Loop: Header=BB26_160 Depth=2
	cmpq	%r9, %rdi
	leaq	1(%rdi), %rcx
	setae	%r8b
	addq	$16, %rax
	addq	$-16, %rdx
	movq	%rcx, %rdi
	cmpq	%rcx, %r10
	je	.LBB26_162
.LBB26_160:                             # %.preheader.i.i293
                                        #   Parent Loop BB26_125 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	%rax, %rsi
	jb	.LBB26_159
# %bb.161:                              # %get_bits.exit.i.i
                                        #   in Loop: Header=BB26_160 Depth=2
	movzwl	-720(%rbp,%rdi,2), %ebx
	rolw	$8, %bx
	leal	(%r11,%rax), %ecx
	movl	$65535, %esi                    # imm = 0xFFFF
                                        # kill: def $cl killed $cl killed $ecx
	shll	%cl, %esi
	cmpq	$16, %rdx
	cmovael	%r14d, %esi
	testw	%si, %bx
	movq	-80(%rbp), %rsi                 # 8-byte Reload
	je	.LBB26_159
.LBB26_162:                             # %is_distinguished.exit.i
                                        #   in Loop: Header=BB26_125 Depth=1
	testb	$1, %r8b
	je	.LBB26_173
# %bb.163:                              # %.preheader.i37.i.preheader
                                        #   in Loop: Header=BB26_125 Depth=1
	xorl	%eax, %eax
	movq	%rsi, %rdx
	xorl	%r8d, %r8d
	xorl	%edi, %edi
	jmp	.LBB26_165
	.p2align	4, 0x90
.LBB26_164:                             # %get_bits.exit.thread.i45.i
                                        #   in Loop: Header=BB26_165 Depth=2
	cmpq	%r9, %rdi
	leaq	1(%rdi), %rcx
	setae	%r8b
	addq	$16, %rax
	addq	$-16, %rdx
	movq	%rcx, %rdi
	cmpq	%rcx, %r10
	je	.LBB26_167
.LBB26_165:                             # %.preheader.i37.i
                                        #   Parent Loop BB26_125 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	%rax, %rsi
	jb	.LBB26_164
# %bb.166:                              # %get_bits.exit.i42.i
                                        #   in Loop: Header=BB26_165 Depth=2
	movzwl	-640(%rbp,%rdi,2), %ebx
	rolw	$8, %bx
	leal	(%r11,%rax), %ecx
	movl	$65535, %esi                    # imm = 0xFFFF
                                        # kill: def $cl killed $cl killed $ecx
	shll	%cl, %esi
	cmpq	$16, %rdx
	cmovael	%r14d, %esi
	testw	%si, %bx
	movq	-80(%rbp), %rsi                 # 8-byte Reload
	je	.LBB26_164
.LBB26_167:                             # %is_distinguished.exit47.i
                                        #   in Loop: Header=BB26_125 Depth=1
	testb	$1, %r8b
	je	.LBB26_173
.LBB26_168:                             # %is_distinguished.exit47.thread.i
                                        #   in Loop: Header=BB26_125 Depth=1
	subq	$48, %rsp
	movl	-624(%rbp), %eax
	movl	%eax, 40(%rsp)
	movups	-640(%rbp), %xmm0
	movups	%xmm0, 24(%rsp)
	movl	-704(%rbp), %eax
	movl	%eax, 16(%rsp)
	movupd	-720(%rbp), %xmm0
	movupd	%xmm0, (%rsp)
	movq	%rsi, %rdx
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	movq	-48(%rbp), %rsi                 # 8-byte Reload
	callq	add_work
	addq	$48, %rsp
                                        # kill: def $eax killed $eax def $rax
	addl	$1, %eax
	cmpl	$3, %eax
	ja	.LBB26_125
# %bb.169:                              # %is_distinguished.exit47.thread.i
                                        #   in Loop: Header=BB26_125 Depth=1
	movl	$82, %edi
	leaq	.LJTI26_0(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	jmpq	*%rax
.LBB26_170:                             #   in Loop: Header=BB26_125 Depth=1
	movl	$88, %edi
.LBB26_171:                             # %.loopexit.sink.split.i
                                        #   in Loop: Header=BB26_125 Depth=1
	movq	(%r15), %rsi
	callq	putc@PLT
	movq	(%r15), %rdi
	callq	fflush@PLT
	jmp	.LBB26_125
.LBB26_172:                             # %.thread541
	movb	$0, stop(%rip)
	movb	$1, %r14b
	xorl	%r12d, %r12d
	cmpb	$0, stop(%rip)
	jne	.LBB26_199
	jmp	.LBB26_181
.LBB26_173:                             # %read_work.exit.loopexit.critedge922
	xorl	%ebx, %ebx
	jmp	.LBB26_179
.LBB26_174:                             # %.loopexit586
	movq	stderr@GOTPCREL(%rip), %r15
	movq	(%r15), %rdi
	movq	(%r14), %rdx
	leaq	.L.str.26(%rip), %rsi
	xorl	%ebx, %ebx
	xorl	%eax, %eax
	callq	fprintf@PLT
	movq	(%r15), %rcx
	leaq	.L.str.27(%rip), %rdi
	movl	$7, %esi
	movl	$1, %edx
	callq	fwrite@PLT
	movq	(%r15), %rcx
	leaq	.L.str.28(%rip), %rdi
	movl	$11, %esi
	movl	$1, %edx
	callq	fwrite@PLT
	movq	(%r15), %rcx
	leaq	.L.str.29(%rip), %rdi
	movl	$31, %esi
	movl	$1, %edx
	callq	fwrite@PLT
	movq	(%r15), %rcx
	leaq	.L.str.30(%rip), %rdi
	movl	$19, %esi
	movl	$1, %edx
	callq	fwrite@PLT
	movq	(%r15), %rcx
	leaq	.L.str.31(%rip), %rdi
	movl	$37, %esi
	movl	$1, %edx
	callq	fwrite@PLT
	jmp	.LBB26_322
.LBB26_175:
	xorl	%ebx, %ebx
	movq	stdout@GOTPCREL(%rip), %r15
	movq	-88(%rbp), %r12                 # 8-byte Reload
	movq	-296(%rbp), %r13                # 8-byte Reload
	jmp	.LBB26_179
.LBB26_176:
	xorl	%ebx, %ebx
	movq	stdout@GOTPCREL(%rip), %r15
	movq	-88(%rbp), %r12                 # 8-byte Reload
	jmp	.LBB26_179
.LBB26_177:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str.32(%rip), %rsi
	movq	-48(%rbp), %rdx                 # 8-byte Reload
	xorl	%eax, %eax
	callq	fprintf@PLT
	movl	$1, %ebx
	jmp	.LBB26_322
.LBB26_178:
	xorl	%ebx, %ebx
	movq	stdout@GOTPCREL(%rip), %r15
.LBB26_179:                             # %read_work.exit
	movb	%bl, stop(%rip)
	movq	%r13, %rdi
	callq	fclose@PLT
.LBB26_180:
	xorl	%r14d, %r14d
	movq	-80(%rbp), %rbx                 # 8-byte Reload
	cmpb	$0, stop(%rip)
	jne	.LBB26_199
.LBB26_181:
	movq	%rsp, %r13
	movq	%rsp, %rcx
	movq	-368(%rbp), %rax                # 8-byte Reload
	leaq	15(,%rax,8), %rax
	andq	$-16, %rax
	subq	%rax, %rcx
	movq	%rcx, -296(%rbp)                # 8-byte Spill
	movq	%rcx, %rsp
	callq	make_seed
	movq	%rax, -72(%rbp)                 # 8-byte Spill
	testb	%r14b, %r14b
	je	.LBB26_183
# %bb.182:
	xorl	%ecx, %ecx
	testq	%r12, %r12
	movq	-368(%rbp), %rax                # 8-byte Reload
	jne	.LBB26_184
	jmp	.LBB26_185
.LBB26_183:
	leaq	.L.str.45(%rip), %rsi
	movq	%r12, %rdi
	callq	fopen@PLT
	movq	%rax, %rcx
	testq	%r12, %r12
	movq	-368(%rbp), %rax                # 8-byte Reload
	je	.LBB26_185
.LBB26_184:
	testq	%rcx, %rcx
	je	.LBB26_326
.LBB26_185:                             # %.preheader568
	movq	%rcx, -88(%rbp)                 # 8-byte Spill
	testq	%rax, %rax
	je	.LBB26_323
# %bb.186:                              # %.lr.ph651
	movq	%r13, -360(%rbp)                # 8-byte Spill
	movq	-80(%rbp), %rax                 # 8-byte Reload
	movq	%rax, %rcx
	shrq	$3, %rcx
	cmpq	$2, %rcx
	movl	$1, %esi
	movq	%rcx, %rdx
	movq	%rcx, -352(%rbp)                # 8-byte Spill
	cmovaeq	%rcx, %rsi
	movq	%rsi, -192(%rbp)                # 8-byte Spill
	movl	%eax, %ecx
	andb	$7, %cl
	movb	$-1, %al
	shrb	%cl, %al
	movb	%al, -120(%rbp)                 # 1-byte Spill
	movq	-72(%rbp), %rax                 # 8-byte Reload
	movq	32(%rax), %r15
	movq	40(%rax), %r14
	xorl	%r13d, %r13d
	.p2align	4, 0x90
.LBB26_187:                             # =>This Inner Loop Header: Depth=1
	addq	$1, %r15
	adcq	$0, %r14
	movq	-72(%rbp), %rsi                 # 8-byte Reload
	movq	%r15, 32(%rsi)
	movq	%r14, 40(%rsi)
	movaps	.LCPI26_6(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	%xmm0, -640(%rbp)
	movaps	.LCPI26_7(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movaps	%xmm0, -624(%rbp)
	movq	$0, -448(%rbp)
	movl	$48, %edx
	leaq	-640(%rbp), %rbx
	movq	%rbx, %rdi
	callq	secp256k1_sha256_write
	movq	-448(%rbp), %rax
	movq	%rax, %rcx
	shrq	$29, %rcx
	shll	$24, %ecx
	movq	%rax, %rdx
	shrq	$21, %rdx
	andl	$16711680, %edx                 # imm = 0xFF0000
	orl	%ecx, %edx
	movq	%rax, %rcx
	shrq	$37, %rcx
	andl	$65280, %ecx                    # imm = 0xFF00
	orl	%edx, %ecx
	movq	%rax, %rdx
	shrq	$53, %rdx
	movzbl	%dl, %edx
	orl	%ecx, %edx
	movl	%edx, -720(%rbp)
	movl	%eax, %ecx
	shll	$27, %ecx
	movl	%eax, %edx
	shll	$11, %edx
	andl	$16711680, %edx                 # imm = 0xFF0000
	orl	%ecx, %edx
	movl	%eax, %ecx
	shrl	$5, %ecx
	andl	$65280, %ecx                    # imm = 0xFF00
	orl	%edx, %ecx
	movl	$55, %edx
	subl	%eax, %edx
	shrl	$21, %eax
	movzbl	%al, %eax
	orl	%ecx, %eax
	movl	%eax, -716(%rbp)
	andl	$63, %edx
	addq	$1, %rdx
	movq	%rbx, %rdi
	leaq	secp256k1_sha256_finalize.pad(%rip), %rsi
	callq	secp256k1_sha256_write
	movl	$8, %edx
	movq	%rbx, %rdi
	leaq	-720(%rbp), %rsi
	callq	secp256k1_sha256_write
	movdqa	-640(%rbp), %xmm0
	movdqa	%xmm0, %xmm1
	pxor	%xmm2, %xmm2
	punpckhbw	%xmm2, %xmm1            # xmm1 = xmm1[8],xmm2[8],xmm1[9],xmm2[9],xmm1[10],xmm2[10],xmm1[11],xmm2[11],xmm1[12],xmm2[12],xmm1[13],xmm2[13],xmm1[14],xmm2[14],xmm1[15],xmm2[15]
	pshuflw	$27, %xmm1, %xmm1               # xmm1 = xmm1[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm1, %xmm1               # xmm1 = xmm1[0,1,2,3,7,6,5,4]
	punpcklbw	%xmm2, %xmm0            # xmm0 = xmm0[0],xmm2[0],xmm0[1],xmm2[1],xmm0[2],xmm2[2],xmm0[3],xmm2[3],xmm0[4],xmm2[4],xmm0[5],xmm2[5],xmm0[6],xmm2[6],xmm0[7],xmm2[7]
	pshuflw	$27, %xmm0, %xmm0               # xmm0 = xmm0[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm0, %xmm0               # xmm0 = xmm0[0,1,2,3,7,6,5,4]
	packuswb	%xmm1, %xmm0
	movl	-624(%rbp), %eax
	bswapl	%eax
	movdqa	%xmm0, -640(%rbp)
	movl	%eax, -624(%rbp)
	cmpq	$8, -80(%rbp)                   # 8-byte Folded Reload
	jb	.LBB26_189
# %bb.188:                              # %.lr.ph.i297
                                        #   in Loop: Header=BB26_187 Depth=1
	leaq	-640(%rbp), %rdi
	xorl	%esi, %esi
	movq	-192(%rbp), %rdx                # 8-byte Reload
	callq	memset@PLT
.LBB26_189:                             # %make_distinguished.exit
                                        #   in Loop: Header=BB26_187 Depth=1
	movq	-352(%rbp), %rax                # 8-byte Reload
	movzbl	-120(%rbp), %ecx                # 1-byte Folded Reload
	andb	%cl, -640(%rbp,%rax)
	movaps	-640(%rbp), %xmm0
	movaps	%xmm0, -112(%rbp)               # 16-byte Spill
	movl	-624(%rbp), %r12d
	movl	$80, %edi
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB26_331
# %bb.190:                              #   in Loop: Header=BB26_187 Depth=1
	movapd	-112(%rbp), %xmm0               # 16-byte Reload
	movupd	%xmm0, (%rax)
	movl	%r12d, 16(%rax)
	movq	-48(%rbp), %rcx                 # 8-byte Reload
	movq	%rcx, 24(%rax)
	movq	-80(%rbp), %rcx                 # 8-byte Reload
	movq	%rcx, 32(%rax)
	movq	-88(%rbp), %rcx                 # 8-byte Reload
	movq	%rcx, 40(%rax)
	movq	$160, 48(%rax)
	xorpd	%xmm0, %xmm0
	movupd	%xmm0, 56(%rax)
	movl	$0, 72(%rax)
	leaq	-640(%rbp), %rdi
	xorl	%esi, %esi
	leaq	worker(%rip), %rdx
	movq	%rax, %rcx
	callq	pthread_create@PLT
	testl	%eax, %eax
	jne	.LBB26_332
# %bb.191:                              #   in Loop: Header=BB26_187 Depth=1
	movq	-640(%rbp), %rax
	testq	%rax, %rax
	je	.LBB26_332
# %bb.192:                              # %spawn_worker.exit
                                        #   in Loop: Header=BB26_187 Depth=1
	movq	-296(%rbp), %r12                # 8-byte Reload
	movq	%rax, (%r12,%r13,8)
	addq	$1, %r13
	cmpq	%r13, -368(%rbp)                # 8-byte Folded Reload
	jne	.LBB26_187
# %bb.193:                              # %._crit_edge652
	movq	-72(%rbp), %rdi                 # 8-byte Reload
	callq	free@PLT
	movq	-368(%rbp), %r14                # 8-byte Reload
	testq	%r14, %r14
	movq	stdout@GOTPCREL(%rip), %r15
	movq	-360(%rbp), %r13                # 8-byte Reload
	je	.LBB26_196
# %bb.194:                              # %.lr.ph655.preheader
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB26_195:                             # %.lr.ph655
                                        # =>This Inner Loop Header: Depth=1
	movq	(%r12,%rbx,8), %rdi
	xorl	%esi, %esi
	callq	pthread_join@PLT
	addq	$1, %rbx
	cmpq	%rbx, %r14
	jne	.LBB26_195
.LBB26_196:                             # %._crit_edge656
	movq	-88(%rbp), %rbx                 # 8-byte Reload
	testq	%rbx, %rbx
	je	.LBB26_198
.LBB26_197:                             # %.thread547
	movq	%rbx, %rdi
	callq	fflush@PLT
	movq	%rbx, %rdi
	callq	fclose@PLT
.LBB26_198:                             # %.sink.split
	movq	%r13, %rsp
	movq	-80(%rbp), %rbx                 # 8-byte Reload
.LBB26_199:
	leaq	.Lstr.6(%rip), %rdi
	callq	puts@PLT
	movq	(%r15), %rdi
	callq	fflush@PLT
	cmpq	$25, %rbx
	jb	.LBB26_201
# %bb.200:
	leaq	-10(%rbx), %r12
	jmp	.LBB26_204
.LBB26_201:
	cmpq	$13, %rbx
	jb	.LBB26_203
# %bb.202:
	leaq	-8(%rbx), %r12
	jmp	.LBB26_204
.LBB26_203:
	leaq	-4(%rbx), %rcx
	xorl	%r12d, %r12d
	cmpq	$9, %rbx
	cmovaeq	%rcx, %r12
	testq	%rbx, %rbx
	je	.LBB26_224
.LBB26_204:                             # %.lr.ph661
	leaq	table(%rip), %r15
	jmp	.LBB26_207
	.p2align	4, 0x90
.LBB26_205:                             #   in Loop: Header=BB26_207 Depth=1
	leaq	-10(%rbx), %r12
.LBB26_206:                             #   in Loop: Header=BB26_207 Depth=1
	testq	%rbx, %rbx
	je	.LBB26_224
.LBB26_207:                             # =>This Loop Header: Depth=1
                                        #     Child Loop BB26_211 Depth 2
                                        #       Child Loop BB26_212 Depth 3
	movq	%rbx, %r13
	leaq	.L.str.47(%rip), %rdi
	movq	%r12, %rsi
	xorl	%eax, %eax
	callq	printf@PLT
	movq	stdout@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	callq	fflush@PLT
	movb	$0, stop(%rip)
	cmpb	$0, collision(%rip)
	je	.LBB26_333
# %bb.208:                              # %get_collision.exit
                                        #   in Loop: Header=BB26_207 Depth=1
	movl	c1+16(%rip), %eax
	movl	%eax, -416(%rbp)
	movups	c1(%rip), %xmm0
	movaps	%xmm0, -432(%rbp)
	movups	c2(%rip), %xmm0
	movaps	%xmm0, -400(%rbp)
	movl	c2+16(%rip), %eax
	movl	%eax, -384(%rbp)
	movupd	c3(%rip), %xmm0
	movapd	%xmm0, -784(%rbp)
	movl	c3+16(%rip), %eax
	movl	%eax, -768(%rbp)
	leaq	table_lock(%rip), %rdi
	callq	pthread_mutex_lock@PLT
	testl	%eax, %eax
	jne	.LBB26_334
# %bb.209:                              # %mutex_lock.exit.i
                                        #   in Loop: Header=BB26_207 Depth=1
	movb	$0, collision(%rip)
	xorl	%r14d, %r14d
	jmp	.LBB26_211
	.p2align	4, 0x90
.LBB26_210:                             # %._crit_edge.i304
                                        #   in Loop: Header=BB26_211 Depth=2
	movq	$0, (%r15,%r14,8)
	addq	$1, %r14
	cmpq	$8192, %r14                     # imm = 0x2000
	je	.LBB26_213
.LBB26_211:                             #   Parent Loop BB26_207 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB26_212 Depth 3
	movq	(%r15,%r14,8), %rdi
	testq	%rdi, %rdi
	je	.LBB26_210
	.p2align	4, 0x90
.LBB26_212:                             # %.lr.ph.i302
                                        #   Parent Loop BB26_207 Depth=1
                                        #     Parent Loop BB26_211 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movq	40(%rdi), %rbx
	callq	free@PLT
	movq	%rbx, %rdi
	testq	%rbx, %rbx
	jne	.LBB26_212
	jmp	.LBB26_210
	.p2align	4, 0x90
.LBB26_213:                             #   in Loop: Header=BB26_207 Depth=1
	leaq	table_lock(%rip), %rdi
	callq	pthread_mutex_unlock@PLT
	testl	%eax, %eax
	jne	.LBB26_335
# %bb.214:                              # %reset_table.exit
                                        #   in Loop: Header=BB26_207 Depth=1
	movl	$80, %edi
	callq	malloc@PLT
	testq	%rax, %rax
	movq	-48(%rbp), %rbx                 # 8-byte Reload
	je	.LBB26_331
# %bb.215:                              #   in Loop: Header=BB26_207 Depth=1
	movl	-416(%rbp), %ecx
	movl	%ecx, 16(%rax)
	movaps	-432(%rbp), %xmm0
	movups	%xmm0, (%rax)
	movq	%rbx, 24(%rax)
	movq	%r12, 32(%rax)
	movq	$0, 40(%rax)
	movq	%r13, 48(%rax)
	movaps	-784(%rbp), %xmm0
	movups	%xmm0, 56(%rax)
	movl	-768(%rbp), %ecx
	movl	%ecx, 72(%rax)
	leaq	-640(%rbp), %rdi
	xorl	%esi, %esi
	leaq	worker(%rip), %rdx
	movq	%rax, %rcx
	callq	pthread_create@PLT
	testl	%eax, %eax
	jne	.LBB26_332
# %bb.216:                              #   in Loop: Header=BB26_207 Depth=1
	movq	-640(%rbp), %r14
	testq	%r14, %r14
	je	.LBB26_332
# %bb.217:                              # %spawn_worker.exit307
                                        #   in Loop: Header=BB26_207 Depth=1
	movl	$80, %edi
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB26_331
# %bb.218:                              #   in Loop: Header=BB26_207 Depth=1
	movl	-384(%rbp), %ecx
	movl	%ecx, 16(%rax)
	movaps	-400(%rbp), %xmm0
	movups	%xmm0, (%rax)
	movq	%rbx, 24(%rax)
	movq	%r12, %rbx
	movq	%r12, 32(%rax)
	movq	$0, 40(%rax)
	movq	%r13, 48(%rax)
	movapd	-784(%rbp), %xmm0
	movupd	%xmm0, 56(%rax)
	movl	-768(%rbp), %ecx
	movl	%ecx, 72(%rax)
	leaq	-640(%rbp), %rdi
	xorl	%esi, %esi
	leaq	worker(%rip), %rdx
	movq	%rax, %rcx
	callq	pthread_create@PLT
	testl	%eax, %eax
	jne	.LBB26_332
# %bb.219:                              #   in Loop: Header=BB26_207 Depth=1
	movq	-640(%rbp), %r13
	testq	%r13, %r13
	je	.LBB26_332
# %bb.220:                              # %spawn_worker.exit310
                                        #   in Loop: Header=BB26_207 Depth=1
	movq	%r14, %rdi
	xorl	%esi, %esi
	callq	pthread_join@PLT
	movq	%r13, %rdi
	xorl	%esi, %esi
	callq	pthread_join@PLT
	leaq	.Lstr.6(%rip), %rdi
	callq	puts@PLT
	cmpq	$25, %rbx
	jae	.LBB26_205
# %bb.221:                              #   in Loop: Header=BB26_207 Depth=1
	cmpq	$13, %rbx
	jb	.LBB26_223
# %bb.222:                              #   in Loop: Header=BB26_207 Depth=1
	leaq	-8(%rbx), %r12
	jmp	.LBB26_206
	.p2align	4, 0x90
.LBB26_223:                             #   in Loop: Header=BB26_207 Depth=1
	leaq	-4(%rbx), %r12
	cmpq	$9, %rbx
	movl	$0, %ecx
	cmovbq	%rcx, %r12
	jmp	.LBB26_206
.LBB26_224:                             # %._crit_edge662
	leaq	-640(%rbp), %rsi
	movl	$1, %edi
	callq	clock_gettime@PLT
	cmpb	$0, collision(%rip)
	je	.LBB26_333
# %bb.225:                              # %get_collision.exit312
	movq	-856(%rbp), %rsi                # 8-byte Reload
	movq	%rsi, %rax
	shrq	$63, %rax
	sarq	$18, %rsi
	addq	%rax, %rsi
	movq	-640(%rbp), %r14
	subq	-848(%rbp), %r14                # 8-byte Folded Reload
	movq	-632(%rbp), %rax
	movabsq	$4835703278458516699, %rcx      # imm = 0x431BDE82D7B634DB
	imulq	%rcx
	movq	%rdx, %rbx
	shrq	$63, %rbx
	sarq	$18, %rdx
	addq	%rdx, %rbx
	addq	%rsi, %rbx
	movl	c1+16(%rip), %eax
	movl	%eax, -416(%rbp)
	movups	c1(%rip), %xmm0
	movaps	%xmm0, -432(%rbp)
	movups	c2(%rip), %xmm0
	movaps	%xmm0, -400(%rbp)
	movl	c2+16(%rip), %eax
	movl	%eax, -384(%rbp)
	leaq	.L.str.48(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movq	stdout@GOTPCREL(%rip), %r15
	movq	(%r15), %rdi
	subq	$32, %rsp
	movl	-416(%rbp), %eax
	movl	%eax, 16(%rsp)
	movaps	-432(%rbp), %xmm0
	movups	%xmm0, (%rsp)
	callq	write_hash160
	addq	$32, %rsp
	leaq	.L.str.49(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movq	(%r15), %rdi
	subq	$32, %rsp
	movl	-384(%rbp), %eax
	movl	%eax, 16(%rsp)
	movdqa	-400(%rbp), %xmm0
	movdqu	%xmm0, (%rsp)
	callq	write_hash160
	addq	$32, %rsp
	imulq	$1000, %r14, %rsi               # imm = 0x3E8
	addq	%rbx, %rsi
	leaq	.L.str.50(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	leaq	.Lstr.2(%rip), %rdi
	callq	puts@PLT
	movq	-344(%rbp), %r14                # 8-byte Reload
	testq	%r14, %r14
	je	.LBB26_261
# %bb.226:
	leaq	.L.str.44(%rip), %rsi
	movq	%r14, %rdi
	callq	fopen@PLT
	movq	%rax, %r15
	testq	%rax, %rax
	je	.LBB26_263
# %bb.227:
	movl	$20971232, %edi                 # imm = 0x13FFEE0
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB26_343
# %bb.228:
	movq	%rax, %r13
	movl	$20971232, %esi                 # imm = 0x13FFEE0
	movl	$1, %edx
	movq	%rax, %rdi
	movq	%r15, %rcx
	callq	fread@PLT
	cmpq	$1, %rax
	jne	.LBB26_263
# %bb.229:
	movaps	.LCPI26_6(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	%xmm0, -640(%rbp)
	movdqa	.LCPI26_7(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movdqa	%xmm0, -624(%rbp)
	movq	$0, -448(%rbp)
	leaq	-640(%rbp), %r12
	movl	$20971200, %edx                 # imm = 0x13FFEC0
	movq	%r12, %rdi
	movq	%r13, %rsi
	callq	secp256k1_sha256_write
	movq	-448(%rbp), %rax
	movq	%rax, %rcx
	movq	%rax, %rsi
	movq	%rax, %r8
	movl	%eax, %r9d
	movl	%eax, %edi
	movl	%eax, %ebx
	movl	%eax, %r10d
	movl	$55, %edx
	subl	%eax, %edx
	shrq	$29, %rax
	shll	$24, %eax
	shrq	$21, %rcx
	andl	$16711680, %ecx                 # imm = 0xFF0000
	orl	%eax, %ecx
	shrq	$37, %rsi
	andl	$65280, %esi                    # imm = 0xFF00
	orl	%ecx, %esi
	shrq	$53, %r8
	movzbl	%r8b, %eax
	orl	%esi, %eax
	movl	%eax, -336(%rbp)
	shll	$27, %r9d
	shll	$11, %edi
	andl	$16711680, %edi                 # imm = 0xFF0000
	orl	%r9d, %edi
	shrl	$5, %ebx
	andl	$65280, %ebx                    # imm = 0xFF00
	orl	%edi, %ebx
	shrl	$21, %r10d
	movzbl	%r10b, %eax
	orl	%ebx, %eax
	movl	%eax, -332(%rbp)
	andl	$63, %edx
	addq	$1, %rdx
	leaq	secp256k1_sha256_finalize.pad(%rip), %rsi
	movq	%r12, %rdi
	callq	secp256k1_sha256_write
	leaq	-336(%rbp), %rsi
	movl	$8, %edx
	movq	%r12, %rdi
	callq	secp256k1_sha256_write
	pxor	%xmm0, %xmm0
	movdqa	-640(%rbp), %xmm1
	movdqa	-624(%rbp), %xmm2
	movdqa	%xmm1, %xmm3
	punpckhbw	%xmm0, %xmm3            # xmm3 = xmm3[8],xmm0[8],xmm3[9],xmm0[9],xmm3[10],xmm0[10],xmm3[11],xmm0[11],xmm3[12],xmm0[12],xmm3[13],xmm0[13],xmm3[14],xmm0[14],xmm3[15],xmm0[15]
	pshuflw	$27, %xmm3, %xmm3               # xmm3 = xmm3[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm3, %xmm3               # xmm3 = xmm3[0,1,2,3,7,6,5,4]
	punpcklbw	%xmm0, %xmm1            # xmm1 = xmm1[0],xmm0[0],xmm1[1],xmm0[1],xmm1[2],xmm0[2],xmm1[3],xmm0[3],xmm1[4],xmm0[4],xmm1[5],xmm0[5],xmm1[6],xmm0[6],xmm1[7],xmm0[7]
	pshuflw	$27, %xmm1, %xmm1               # xmm1 = xmm1[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm1, %xmm1               # xmm1 = xmm1[0,1,2,3,7,6,5,4]
	packuswb	%xmm3, %xmm1
	movdqa	%xmm2, %xmm3
	punpckhbw	%xmm0, %xmm3            # xmm3 = xmm3[8],xmm0[8],xmm3[9],xmm0[9],xmm3[10],xmm0[10],xmm3[11],xmm0[11],xmm3[12],xmm0[12],xmm3[13],xmm0[13],xmm3[14],xmm0[14],xmm3[15],xmm0[15]
	pshuflw	$27, %xmm3, %xmm3               # xmm3 = xmm3[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm3, %xmm3               # xmm3 = xmm3[0,1,2,3,7,6,5,4]
	punpcklbw	%xmm0, %xmm2            # xmm2 = xmm2[0],xmm0[0],xmm2[1],xmm0[1],xmm2[2],xmm0[2],xmm2[3],xmm0[3],xmm2[4],xmm0[4],xmm2[5],xmm0[5],xmm2[6],xmm0[6],xmm2[7],xmm0[7]
	pshuflw	$27, %xmm2, %xmm0               # xmm0 = xmm2[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm0, %xmm0               # xmm0 = xmm0[0,1,2,3,7,6,5,4]
	packuswb	%xmm3, %xmm0
	movdqa	%xmm1, -720(%rbp)
	movdqa	%xmm0, -704(%rbp)
	movdqu	20971200(%r13), %xmm2
	movdqu	20971216(%r13), %xmm3
	pcmpeqb	%xmm0, %xmm3
	pcmpeqb	%xmm1, %xmm2
	pand	%xmm3, %xmm2
	pmovmskb	%xmm2, %eax
	cmpl	$65535, %eax                    # imm = 0xFFFF
	jne	.LBB26_263
# %bb.230:
	movq	%r15, -112(%rbp)                # 8-byte Spill
	movq	%r13, %rbx
	movl	$65535, %r13d                   # imm = 0xFFFF
	leaq	-640(%rbp), %r12
	movq	%rbx, -72(%rbp)                 # 8-byte Spill
	leaq	priv_offsets(%rip), %r14
	leaq	priv_bases(%rip), %r15
	.p2align	4, 0x90
.LBB26_231:                             # =>This Inner Loop Header: Depth=1
	movq	%r15, %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	secp256k1_scalar_set_b32
	cmpl	$0, -640(%rbp)
	jne	.LBB26_262
# %bb.232:                              #   in Loop: Header=BB26_231 Depth=1
	addq	$32, %rbx
	addq	$32, %r15
	addq	$-1, %r13
	jne	.LBB26_231
# %bb.233:                              # %.preheader.preheader.i.preheader
	xorl	%r13d, %r13d
	leaq	-640(%rbp), %r12
	.p2align	4, 0x90
.LBB26_234:                             # %.preheader.preheader.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%rbx,%r13), %rsi
	movq	%r14, %rdi
	movq	%r12, %rdx
	callq	secp256k1_scalar_set_b32
	cmpl	$0, -640(%rbp)
	jne	.LBB26_262
# %bb.235:                              #   in Loop: Header=BB26_234 Depth=1
	addq	$32, %r13
	addq	$288, %r14                      # imm = 0x120
	cmpq	$2097120, %r13                  # imm = 0x1FFFE0
	jne	.LBB26_234
# %bb.236:                              # %.critedge.i.preheader
	addq	%r13, %rbx
	movl	$32, %r14d
	leaq	priv_offsets(%rip), %r13
	leaq	-640(%rbp), %r12
	.p2align	4, 0x90
.LBB26_237:                             # %.critedge.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r14,%r13), %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	secp256k1_scalar_set_b32
	cmpl	$0, -640(%rbp)
	jne	.LBB26_262
# %bb.238:                              #   in Loop: Header=BB26_237 Depth=1
	addq	$32, %rbx
	addq	$288, %r14                      # imm = 0x120
	cmpq	$18874112, %r14                 # imm = 0x11FFF00
	jne	.LBB26_237
# %bb.239:                              # %.critedge.1.i.preheader
	movl	$64, %r14d
	leaq	-640(%rbp), %r12
	.p2align	4, 0x90
.LBB26_240:                             # %.critedge.1.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r14,%r13), %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	secp256k1_scalar_set_b32
	cmpl	$0, -640(%rbp)
	jne	.LBB26_262
# %bb.241:                              #   in Loop: Header=BB26_240 Depth=1
	addq	$32, %rbx
	addq	$288, %r14                      # imm = 0x120
	cmpq	$18874144, %r14                 # imm = 0x11FFF20
	jne	.LBB26_240
# %bb.242:                              # %.critedge.2.i.preheader
	movl	$96, %r14d
	leaq	-640(%rbp), %r12
	.p2align	4, 0x90
.LBB26_243:                             # %.critedge.2.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r14,%r13), %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	secp256k1_scalar_set_b32
	cmpl	$0, -640(%rbp)
	jne	.LBB26_262
# %bb.244:                              #   in Loop: Header=BB26_243 Depth=1
	addq	$32, %rbx
	addq	$288, %r14                      # imm = 0x120
	cmpq	$18874176, %r14                 # imm = 0x11FFF40
	jne	.LBB26_243
# %bb.245:                              # %.critedge.3.i.preheader
	movl	$128, %r14d
	leaq	-640(%rbp), %r12
	.p2align	4, 0x90
.LBB26_246:                             # %.critedge.3.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r14,%r13), %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	secp256k1_scalar_set_b32
	cmpl	$0, -640(%rbp)
	jne	.LBB26_262
# %bb.247:                              #   in Loop: Header=BB26_246 Depth=1
	addq	$32, %rbx
	addq	$288, %r14                      # imm = 0x120
	cmpq	$18874208, %r14                 # imm = 0x11FFF60
	jne	.LBB26_246
# %bb.248:                              # %.critedge.4.i.preheader
	movl	$160, %r14d
	leaq	-640(%rbp), %r12
	.p2align	4, 0x90
.LBB26_249:                             # %.critedge.4.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r14,%r13), %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	secp256k1_scalar_set_b32
	cmpl	$0, -640(%rbp)
	jne	.LBB26_262
# %bb.250:                              #   in Loop: Header=BB26_249 Depth=1
	addq	$32, %rbx
	addq	$288, %r14                      # imm = 0x120
	cmpq	$18874240, %r14                 # imm = 0x11FFF80
	jne	.LBB26_249
# %bb.251:                              # %.critedge.5.i.preheader
	movl	$192, %r14d
	leaq	-640(%rbp), %r12
	.p2align	4, 0x90
.LBB26_252:                             # %.critedge.5.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r14,%r13), %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	secp256k1_scalar_set_b32
	cmpl	$0, -640(%rbp)
	jne	.LBB26_262
# %bb.253:                              #   in Loop: Header=BB26_252 Depth=1
	addq	$32, %rbx
	addq	$288, %r14                      # imm = 0x120
	cmpq	$18874272, %r14                 # imm = 0x11FFFA0
	jne	.LBB26_252
# %bb.254:                              # %.critedge.6.i.preheader
	movl	$224, %r14d
	leaq	-640(%rbp), %r12
	.p2align	4, 0x90
.LBB26_255:                             # %.critedge.6.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r14,%r13), %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	secp256k1_scalar_set_b32
	cmpl	$0, -640(%rbp)
	jne	.LBB26_262
# %bb.256:                              #   in Loop: Header=BB26_255 Depth=1
	addq	$32, %rbx
	addq	$288, %r14                      # imm = 0x120
	cmpq	$18874304, %r14                 # imm = 0x11FFFC0
	jne	.LBB26_255
# %bb.257:                              # %.critedge.7.i.preheader
	movl	$256, %r14d                     # imm = 0x100
	leaq	-640(%rbp), %r12
	.p2align	4, 0x90
.LBB26_258:                             # %.critedge.7.i
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r14,%r13), %rdi
	movq	%rbx, %rsi
	movq	%r12, %rdx
	callq	secp256k1_scalar_set_b32
	cmpl	$0, -640(%rbp)
	jne	.LBB26_262
# %bb.259:                              #   in Loop: Header=BB26_258 Depth=1
	addq	$32, %rbx
	addq	$288, %r14                      # imm = 0x120
	cmpq	$18874336, %r14                 # imm = 0x11FFFE0
	jne	.LBB26_258
# %bb.260:                              # %read_priv_table.exit
	movq	-72(%rbp), %rdi                 # 8-byte Reload
	callq	free@PLT
	movb	$1, %r12b
	movq	-112(%rbp), %r15                # 8-byte Reload
	testq	%r15, %r15
	movq	-48(%rbp), %rbx                 # 8-byte Reload
	jne	.LBB26_264
	jmp	.LBB26_265
.LBB26_261:
	movb	$1, %r12b
	movq	-48(%rbp), %rbx                 # 8-byte Reload
	jmp	.LBB26_266
.LBB26_262:                             # %read_priv_table.exit.thread552
	movq	-344(%rbp), %r14                # 8-byte Reload
	movq	-112(%rbp), %r15                # 8-byte Reload
.LBB26_263:                             # %read_priv_table.exit.thread
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str.52(%rip), %rsi
	xorl	%r12d, %r12d
	movq	%r14, %rdx
	xorl	%eax, %eax
	callq	fprintf@PLT
	testq	%r15, %r15
	movq	-48(%rbp), %rbx                 # 8-byte Reload
	je	.LBB26_265
.LBB26_264:
	movq	%r15, %rdi
	callq	fclose@PLT
.LBB26_265:
	movq	stdout@GOTPCREL(%rip), %r15
.LBB26_266:
	subq	$32, %rsp
	movl	-416(%rbp), %eax
	movl	%eax, 16(%rsp)
	movaps	-432(%rbp), %xmm0
	movups	%xmm0, (%rsp)
	leaq	-184(%rbp), %rdi
	movq	%rbx, %rsi
	callq	gen_priv_key
	movl	-384(%rbp), %eax
	movl	%eax, 16(%rsp)
	movaps	-400(%rbp), %xmm0
	movups	%xmm0, (%rsp)
	leaq	-152(%rbp), %rdi
	movq	%rbx, %rsi
	callq	gen_priv_key
	addq	$32, %rsp
	leaq	.L.str.53(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	testb	%r12b, %r12b
	je	.LBB26_268
# %bb.267:                              # %.preheader
	movzbl	-184(%rbp), %esi
	leaq	.L.str.54(%rip), %rbx
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-183(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-182(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-181(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-180(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-179(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-178(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-177(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-176(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-175(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-174(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-173(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-172(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-171(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-170(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-169(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-168(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-167(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-166(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-165(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-164(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-163(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-162(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-161(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-160(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-159(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-158(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-157(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-156(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-155(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-154(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-153(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	leaq	.L.str.56(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-152(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-151(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-150(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-149(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-148(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-147(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-146(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-145(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-144(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-143(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-142(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-141(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-140(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-139(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-138(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-137(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-136(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-135(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-134(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-133(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-132(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-131(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-130(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-129(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-128(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-127(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-126(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-125(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-124(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-123(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-122(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-121(%rbp), %esi
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	jmp	.LBB26_269
.LBB26_268:
	leaq	.L.str.55(%rip), %rbx
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	leaq	.L.str.56(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movq	%rbx, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
.LBB26_269:                             # %.loopexit
	movq	(%r15), %rsi
	movl	$10, %edi
	callq	putc@PLT
	subq	$32, %rsp
	movl	-416(%rbp), %eax
	movl	%eax, 16(%rsp)
	movaps	-432(%rbp), %xmm0
	movups	%xmm0, (%rsp)
	leaq	-288(%rbp), %r14
	movq	%r14, %rdi
	movq	-48(%rbp), %rbx                 # 8-byte Reload
	movq	%rbx, %rsi
	callq	gen_pub_key
	movl	-384(%rbp), %eax
	movl	%eax, 16(%rsp)
	movaps	-400(%rbp), %xmm0
	movups	%xmm0, (%rsp)
	leaq	-240(%rbp), %r15
	movq	%r15, %rdi
	movq	%rbx, %rsi
	callq	gen_pub_key
	addq	$32, %rsp
	leaq	-832(%rbp), %rdi
	movq	%r14, %rsi
	callq	hash160
	leaq	-808(%rbp), %rdi
	movq	%r15, %rsi
	callq	hash160
	movq	%rsp, %r14
	movb	$-128, -720(%rbp)
	movups	-184(%rbp), %xmm0
	movups	-168(%rbp), %xmm1
	movups	%xmm0, -719(%rbp)
	movups	%xmm1, -703(%rbp)
	movb	$1, -687(%rbp)
	leaq	-336(%rbp), %rdi
	leaq	-720(%rbp), %rsi
	movl	$34, %edx
	callq	sha256d
	movq	%rsp, %rbx
	movzwl	-688(%rbp), %eax
	movw	%ax, -608(%rbp)
	movaps	-720(%rbp), %xmm0
	movaps	-704(%rbp), %xmm1
	movaps	%xmm1, -624(%rbp)
	movaps	%xmm0, -640(%rbp)
	movl	-336(%rbp), %eax
	movl	%eax, -606(%rbp)
	leaq	-1232(%rbp), %r15
	leaq	-640(%rbp), %rsi
	movl	$38, %edx
	movq	%r15, %rdi
	callq	base58_encode
	movq	%rbx, %rsp
	movq	%r14, %rsp
	testb	%al, %al
	je	.LBB26_338
# %bb.270:                              # %make_wif.exit
	movq	%rsp, %r14
	movb	$-128, -720(%rbp)
	movups	-152(%rbp), %xmm0
	movups	-136(%rbp), %xmm1
	movups	%xmm0, -719(%rbp)
	movups	%xmm1, -703(%rbp)
	movb	$1, -687(%rbp)
	leaq	-336(%rbp), %rdi
	leaq	-720(%rbp), %rsi
	movl	$34, %edx
	callq	sha256d
	movq	%rsp, %rbx
	movzwl	-688(%rbp), %eax
	movw	%ax, -608(%rbp)
	movaps	-720(%rbp), %xmm0
	movaps	-704(%rbp), %xmm1
	movaps	%xmm1, -624(%rbp)
	movaps	%xmm0, -640(%rbp)
	movl	-336(%rbp), %eax
	movl	%eax, -606(%rbp)
	leaq	-1568(%rbp), %rdi
	leaq	-640(%rbp), %rsi
	movl	$38, %edx
	callq	base58_encode
	movq	%rbx, %rsp
	movq	%r14, %rsp
	testb	%al, %al
	je	.LBB26_338
# %bb.271:                              # %make_wif.exit333
	movq	%rsp, %r14
	movb	$0, -336(%rbp)
	movups	-832(%rbp), %xmm0
	movups	%xmm0, -335(%rbp)
	movl	-816(%rbp), %eax
	movl	%eax, -319(%rbp)
	leaq	-640(%rbp), %rdi
	leaq	-336(%rbp), %rsi
	movl	$21, %edx
	callq	sha256d
	movq	%rsp, %rbx
	movq	-323(%rbp), %rax
	movq	%rax, -707(%rbp)
	movaps	-336(%rbp), %xmm0
	movaps	%xmm0, -720(%rbp)
	movl	-640(%rbp), %eax
	movl	%eax, -699(%rbp)
	leaq	-1008(%rbp), %rdi
	leaq	-720(%rbp), %rsi
	movl	$25, %edx
	callq	base58_encode
	movq	%rbx, %rsp
	movq	%r14, %rsp
	testb	%al, %al
	je	.LBB26_339
# %bb.272:                              # %make_addr.exit
	movl	%r12d, -112(%rbp)               # 4-byte Spill
	movq	%rsp, %r14
	movb	$0, -336(%rbp)
	movups	-808(%rbp), %xmm0
	movups	%xmm0, -335(%rbp)
	movl	-792(%rbp), %eax
	movl	%eax, -319(%rbp)
	leaq	-640(%rbp), %rdi
	leaq	-336(%rbp), %rsi
	movl	$21, %edx
	callq	sha256d
	movq	%rsp, %rbx
	movq	-323(%rbp), %rax
	movq	%rax, -707(%rbp)
	movaps	-336(%rbp), %xmm0
	movaps	%xmm0, -720(%rbp)
	movl	-640(%rbp), %eax
	movl	%eax, -699(%rbp)
	leaq	-1120(%rbp), %rdi
	leaq	-720(%rbp), %rsi
	movl	$25, %edx
	callq	base58_encode
	movq	%rbx, %rsp
	movq	%r14, %rsp
	testb	%al, %al
	je	.LBB26_339
# %bb.273:                              # %make_addr.exit338
	subq	$32, %rsp
	movups	-184(%rbp), %xmm0
	movups	-168(%rbp), %xmm1
	movups	%xmm1, 16(%rsp)
	movups	%xmm0, (%rsp)
	leaq	-1456(%rbp), %rdi
	movq	-376(%rbp), %rbx                # 8-byte Reload
	movq	%rbx, %rsi
	callq	make_sig
	movupd	-152(%rbp), %xmm0
	movupd	-136(%rbp), %xmm1
	movupd	%xmm1, 16(%rsp)
	movupd	%xmm0, (%rsp)
	leaq	-1344(%rbp), %rdi
	movq	%rbx, %rsi
	callq	make_sig
	addq	$32, %rsp
	movq	-48(%rbp), %r10                 # 8-byte Reload
	leaq	1(%r10), %r13
	cmpq	$159, %r13
	ja	.LBB26_283
# %bb.274:                              # %.lr.ph669
	movl	$159, %r8d
	subq	%r10, %r8
	movl	$15, %r11d
	subl	-840(%rbp), %r11d               # 4-byte Folded Reload
	xorl	%r14d, %r14d
	movl	$65535, %r9d                    # imm = 0xFFFF
	.p2align	4, 0x90
.LBB26_275:                             # =>This Loop Header: Depth=1
                                        #     Child Loop BB26_277 Depth 2
	movq	%r10, %rbx
	shrq	$4, %rbx
	leaq	1(%rbx), %rdi
	xorl	%edx, %edx
	movq	%r13, %rax
	xorl	%esi, %esi
	xorl	%r12d, %r12d
	jmp	.LBB26_277
	.p2align	4, 0x90
.LBB26_276:                             # %get_bits.exit22.thread.i
                                        #   in Loop: Header=BB26_277 Depth=2
	cmpq	%rbx, %r12
	leaq	1(%r12), %rcx
	setae	%sil
	addq	$16, %rdx
	addq	$-16, %rax
	movq	%rcx, %r12
	cmpq	%rcx, %rdi
	je	.LBB26_279
.LBB26_277:                             #   Parent Loop BB26_275 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	%rdx, %r13
	jb	.LBB26_276
# %bb.278:                              # %get_bits.exit22.i
                                        #   in Loop: Header=BB26_277 Depth=2
	leal	(%r11,%rdx), %ecx
	movl	$65535, %r15d                   # imm = 0xFFFF
                                        # kill: def $cl killed $cl killed $ecx
	shll	%cl, %r15d
	cmpq	$16, %rax
	cmovael	%r9d, %r15d
	movzwl	-808(%rbp,%r12,2), %ecx
	xorw	-832(%rbp,%r12,2), %cx
	rolw	$8, %cx
	testw	%r15w, %cx
	je	.LBB26_276
.LBB26_279:                             # %is_equal.exit
                                        #   in Loop: Header=BB26_275 Depth=1
	testb	$1, %sil
	je	.LBB26_282
# %bb.280:                              #   in Loop: Header=BB26_275 Depth=1
	addq	$1, %r14
	addq	$1, %r13
	addq	$1, %r10
	addq	$-1, %r11
	cmpq	%r8, %r14
	jne	.LBB26_275
# %bb.281:
	movq	%r8, %r14
.LBB26_282:                             # %is_equal.exit._crit_edge
	movq	-48(%rbp), %r10                 # 8-byte Reload
	leaq	-1232(%rbp), %r15
	movb	-1008(%rbp), %al
	xorl	%r13d, %r13d
	testb	%al, %al
	jne	.LBB26_285
	jmp	.LBB26_287
.LBB26_283:
	xorl	%r14d, %r14d
	movb	-1008(%rbp), %al
	xorl	%r13d, %r13d
	testb	%al, %al
	je	.LBB26_287
	.p2align	4, 0x90
.LBB26_285:                             # %.lr.ph677
                                        # =>This Inner Loop Header: Depth=1
	cmpb	-1120(%rbp,%r13), %al
	jne	.LBB26_287
# %bb.286:                              #   in Loop: Header=BB26_285 Depth=1
	movzbl	-1007(%rbp,%r13), %eax
	addq	$1, %r13
	testb	%al, %al
	jne	.LBB26_285
.LBB26_287:                             # %.critedge
	addq	%r14, %r10
	movq	%r10, -48(%rbp)                 # 8-byte Spill
	cmpb	$0, -112(%rbp)                  # 1-byte Folded Reload
	leaq	.L.str.55(%rip), %rax
	cmoveq	%rax, %r15
	leaq	.L.str.57(%rip), %rdi
	leaq	-1568(%rbp), %rbx
	cmoveq	%rax, %rbx
	movq	%r15, %rsi
	leaq	-1456(%rbp), %r15
	cmoveq	%rax, %r15
	leaq	-1344(%rbp), %r12
	cmoveq	%rax, %r12
	xorl	%eax, %eax
	callq	printf@PLT
	leaq	.L.str.58(%rip), %rdi
	movq	%rbx, %rsi
	movq	-48(%rbp), %rbx                 # 8-byte Reload
	xorl	%eax, %eax
	callq	printf@PLT
	leaq	.Lstr.3(%rip), %rdi
	callq	puts@PLT
	leaq	.L.str.60(%rip), %rdi
	movq	-376(%rbp), %rsi                # 8-byte Reload
	xorl	%eax, %eax
	callq	printf@PLT
	leaq	.L.str.61(%rip), %rdi
	movq	%r15, %rsi
	xorl	%eax, %eax
	callq	printf@PLT
	leaq	.L.str.62(%rip), %rdi
	movq	%r12, %rsi
	xorl	%eax, %eax
	callq	printf@PLT
	leaq	.Lstr.4(%rip), %rdi
	callq	puts@PLT
	leaq	.L.str.64(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-288(%rbp), %esi
	leaq	.L.str.54(%rip), %r15
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-287(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-286(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-285(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-284(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-283(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-282(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-281(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-280(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-279(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-278(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-277(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-276(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-275(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-274(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-273(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-272(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-271(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-270(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-269(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-268(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-267(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-266(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-265(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-264(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-263(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-262(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-261(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-260(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-259(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-258(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-257(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	leaq	.L.str.65(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-240(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-239(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-238(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-237(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-236(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-235(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-234(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-233(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-232(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-231(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-230(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-229(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-228(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-227(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-226(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-225(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-224(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-223(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-222(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-221(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-220(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-219(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-218(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-217(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-216(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-215(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-214(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-213(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-212(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-211(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-210(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movzbl	-209(%rbp), %esi
	movq	%r15, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movq	%rbx, %r12
	shrq	$2, %r12
	leaq	.L.str.66(%rip), %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	printf@PLT
	leaq	.L.str.67(%rip), %rdi
	movq	%r12, %rsi
	xorl	%eax, %eax
	callq	printf@PLT
	leaq	.L.str.68(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movl	$1, %edi
	callq	isatty@PLT
	testl	%eax, %eax
	je	.LBB26_289
# %bb.288:                              # %.sink.split.i
	leaq	.L.str.111(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
.LBB26_289:                             # %set_bold.exit
	cmpq	$4, %rbx
	jb	.LBB26_292
# %bb.290:                              # %.lr.ph685
	cmpq	$2, %r12
	movl	$1, %r15d
	cmovaeq	%r12, %r15
	leaq	.L.str.69(%rip), %r14
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB26_291:                             # =>This Inner Loop Header: Depth=1
	movq	%rbx, %rax
	shrq	%rax
	movzbl	-832(%rbp,%rax), %eax
	movl	%eax, %ecx
	andb	$15, %cl
	shrb	$4, %al
	testb	$1, %bl
	movzbl	%al, %eax
	movzbl	%cl, %ecx
	cmovel	%eax, %ecx
	movzbl	%cl, %esi
	movq	%r14, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	addq	$1, %rbx
	cmpq	%rbx, %r15
	jne	.LBB26_291
.LBB26_292:                             # %._crit_edge686
	movl	$1, %edi
	callq	isatty@PLT
	testl	%eax, %eax
	je	.LBB26_294
# %bb.293:                              # %.sink.split.i343
	leaq	.L.str.112(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
.LBB26_294:                             # %set_bold.exit344
	cmpq	$159, -48(%rbp)                 # 8-byte Folded Reload
	movq	stdout@GOTPCREL(%rip), %r15
	ja	.LBB26_297
# %bb.295:
	leaq	.L.str.69(%rip), %r14
	movq	%r12, %rbx
	.p2align	4, 0x90
.LBB26_296:                             # =>This Inner Loop Header: Depth=1
	movq	%rbx, %rax
	shrq	%rax
	movzbl	-832(%rbp,%rax), %eax
	movl	%eax, %ecx
	andb	$15, %cl
	shrb	$4, %al
	testb	$1, %bl
	movzbl	%al, %eax
	movzbl	%cl, %ecx
	cmovel	%eax, %ecx
	movzbl	%cl, %esi
	movq	%r14, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	leaq	1(%rbx), %rax
	cmpq	$39, %rbx
	movq	%rax, %rbx
	jb	.LBB26_296
.LBB26_297:                             # %._crit_edge690
	movq	(%r15), %rsi
	movl	$10, %edi
	callq	putc@PLT
	leaq	.L.str.70(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movl	$1, %edi
	callq	isatty@PLT
	testl	%eax, %eax
	je	.LBB26_299
# %bb.298:                              # %.sink.split.i346
	leaq	.L.str.111(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
.LBB26_299:                             # %set_bold.exit347
	cmpq	$4, -48(%rbp)                   # 8-byte Folded Reload
	jb	.LBB26_302
# %bb.300:                              # %.lr.ph693
	cmpq	$2, %r12
	movl	$1, %r15d
	cmovaeq	%r12, %r15
	leaq	.L.str.69(%rip), %r14
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB26_301:                             # =>This Inner Loop Header: Depth=1
	movq	%rbx, %rax
	shrq	%rax
	movzbl	-808(%rbp,%rax), %eax
	movl	%eax, %ecx
	andb	$15, %cl
	shrb	$4, %al
	testb	$1, %bl
	movzbl	%al, %eax
	movzbl	%cl, %ecx
	cmovel	%eax, %ecx
	movzbl	%cl, %esi
	movq	%r14, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	addq	$1, %rbx
	cmpq	%rbx, %r15
	jne	.LBB26_301
.LBB26_302:                             # %._crit_edge694
	movl	$1, %edi
	callq	isatty@PLT
	testl	%eax, %eax
	je	.LBB26_304
# %bb.303:                              # %.sink.split.i349
	leaq	.L.str.112(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
.LBB26_304:                             # %set_bold.exit350
	cmpq	$159, -48(%rbp)                 # 8-byte Folded Reload
	movq	stdout@GOTPCREL(%rip), %r15
	ja	.LBB26_307
# %bb.305:
	leaq	.L.str.69(%rip), %r14
	.p2align	4, 0x90
.LBB26_306:                             # =>This Inner Loop Header: Depth=1
	movq	%r12, %rax
	shrq	%rax
	movzbl	-808(%rbp,%rax), %eax
	movl	%eax, %ecx
	andb	$15, %cl
	shrb	$4, %al
	testb	$1, %r12b
	movzbl	%al, %eax
	movzbl	%cl, %ecx
	cmovel	%eax, %ecx
	movzbl	%cl, %esi
	movq	%r14, %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	leaq	1(%r12), %rax
	cmpq	$39, %r12
	movq	%rax, %r12
	jb	.LBB26_306
.LBB26_307:                             # %._crit_edge698
	movq	(%r15), %rsi
	movl	$10, %edi
	callq	putc@PLT
	movq	(%r15), %rsi
	movl	$10, %edi
	callq	putc@PLT
	leaq	.L.str.67(%rip), %rdi
	movq	%r13, %rsi
	xorl	%eax, %eax
	callq	printf@PLT
	leaq	.L.str.71(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movl	$1, %edi
	callq	isatty@PLT
	testl	%eax, %eax
	je	.LBB26_309
# %bb.308:                              # %.sink.split.i352
	leaq	.L.str.111(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
.LBB26_309:                             # %set_bold.exit353
	testq	%r13, %r13
	je	.LBB26_312
# %bb.310:                              # %.lr.ph700.preheader
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB26_311:                             # %.lr.ph700
                                        # =>This Inner Loop Header: Depth=1
	movsbl	-1008(%rbp,%rbx), %edi
	movq	(%r15), %rsi
	callq	putc@PLT
	addq	$1, %rbx
	cmpq	%rbx, %r13
	jne	.LBB26_311
.LBB26_312:                             # %._crit_edge701
	movl	$1, %edi
	callq	isatty@PLT
	testl	%eax, %eax
	je	.LBB26_314
# %bb.313:                              # %.sink.split.i355
	leaq	.L.str.112(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
.LBB26_314:                             # %set_bold.exit356
	leaq	-1008(,%r13), %rdi
	addq	%rbp, %rdi
	callq	puts@PLT
	leaq	.L.str.73(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
	movl	$1, %edi
	callq	isatty@PLT
	testl	%eax, %eax
	je	.LBB26_316
# %bb.315:                              # %.sink.split.i358
	leaq	.L.str.111(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
.LBB26_316:                             # %set_bold.exit359
	testq	%r13, %r13
	je	.LBB26_319
# %bb.317:                              # %.lr.ph703.preheader
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB26_318:                             # %.lr.ph703
                                        # =>This Inner Loop Header: Depth=1
	movsbl	-1120(%rbp,%rbx), %edi
	movq	(%r15), %rsi
	callq	putc@PLT
	addq	$1, %rbx
	cmpq	%rbx, %r13
	jne	.LBB26_318
.LBB26_319:                             # %._crit_edge704
	movl	$1, %edi
	callq	isatty@PLT
	testl	%eax, %eax
	je	.LBB26_321
# %bb.320:                              # %.sink.split.i361
	leaq	.L.str.112(%rip), %rdi
	xorl	%eax, %eax
	callq	printf@PLT
.LBB26_321:                             # %set_bold.exit362
	leaq	-1120(,%r13), %rsi
	addq	%rbp, %rsi
	leaq	.L.str.74(%rip), %rdi
	xorl	%ebx, %ebx
	xorl	%eax, %eax
	callq	printf@PLT
	leaq	.Lstr.5(%rip), %rdi
	callq	puts@PLT
.LBB26_322:
	movl	%ebx, %eax
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa %rsp, 8
	retq
.LBB26_323:                             # %._crit_edge652.thread
	.cfi_def_cfa %rbp, 16
	movq	-72(%rbp), %rdi                 # 8-byte Reload
	callq	free@PLT
	movq	-88(%rbp), %rbx                 # 8-byte Reload
	testq	%rbx, %rbx
	jne	.LBB26_197
	jmp	.LBB26_198
.LBB26_324:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str.33(%rip), %rsi
	movq	-48(%rbp), %rdx                 # 8-byte Reload
	xorl	%eax, %eax
	callq	fprintf@PLT
	movl	$1, %ebx
	jmp	.LBB26_322
.LBB26_325:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rcx
	leaq	.L.str.24(%rip), %rdi
	movl	$44, %esi
	movl	$1, %edx
	callq	fwrite@PLT
	movl	$1, %ebx
	jmp	.LBB26_322
.LBB26_326:                             # %.thread545
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str.46(%rip), %rsi
	movq	%r12, %rdx
	xorl	%eax, %eax
	callq	fprintf@PLT
	movl	$1, %ebx
	movq	%r13, %rsp
	jmp	.LBB26_322
.LBB26_327:
	movb	$0, stop(%rip)
	movq	(%r15), %rsi
	movl	$89, %edi
	callq	putc@PLT
	movq	(%r15), %rdi
	callq	fflush@PLT
	movb	$1, %bl
	jmp	.LBB26_179
.LBB26_328:                             # %read_pub_table.exit.thread.i
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str.82(%rip), %rsi
	movq	-192(%rbp), %rdx                # 8-byte Reload
	xorl	%eax, %eax
	callq	fprintf@PLT
	movl	$1, %edi
	callq	exit@PLT
.LBB26_329:                             # %write_pub_table.exit.thread.i
	movq	-192(%rbp), %r12                # 8-byte Reload
.LBB26_330:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str.86(%rip), %rsi
	movq	%r12, %rdx
	xorl	%eax, %eax
	callq	fprintf@PLT
	movl	$1, %edi
	callq	exit@PLT
.LBB26_331:
	leaq	.L.str.97(%rip), %rdi
	leaq	.L.str.23(%rip), %rsi
	leaq	.L__PRETTY_FUNCTION__.spawn_worker(%rip), %rcx
	movl	$861, %edx                      # imm = 0x35D
	callq	__assert_fail@PLT
.LBB26_332:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rcx
	leaq	.L.str.98(%rip), %rdi
	movl	$29, %esi
	movl	$1, %edx
	callq	fwrite@PLT
	movl	$1, %edi
	callq	exit@PLT
.LBB26_333:
	leaq	.L.str.102(%rip), %rdi
	leaq	.L.str.23(%rip), %rsi
	leaq	.L__PRETTY_FUNCTION__.get_collision(%rip), %rcx
	movl	$742, %edx                      # imm = 0x2E6
	callq	__assert_fail@PLT
.LBB26_334:
	leaq	.L.str.80(%rip), %rdi
	leaq	.L.str.23(%rip), %rsi
	leaq	.L__PRETTY_FUNCTION__.mutex_lock(%rip), %rcx
	movl	$62, %edx
	callq	__assert_fail@PLT
.LBB26_335:
	leaq	.L.str.80(%rip), %rdi
	leaq	.L.str.23(%rip), %rsi
	leaq	.L__PRETTY_FUNCTION__.mutex_unlock(%rip), %rcx
	movl	$68, %edx
	callq	__assert_fail@PLT
.LBB26_336:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rcx
	leaq	.L.str.84(%rip), %rdi
	movl	$42, %esi
	movl	$1, %edx
	callq	fwrite@PLT
	movl	$1, %edi
	callq	exit@PLT
.LBB26_337:
	leaq	.L.str.22(%rip), %rdi
	leaq	.L.str.23(%rip), %rsi
	leaq	.L__PRETTY_FUNCTION__.main(%rip), %rcx
	movl	$1248, %edx                     # imm = 0x4E0
	callq	__assert_fail@PLT
.LBB26_338:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rcx
	leaq	.L.str.104(%rip), %rdi
	movl	$28, %esi
	movl	$1, %edx
	callq	fwrite@PLT
	movl	$1, %edi
	callq	exit@PLT
.LBB26_339:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rcx
	leaq	.L.str.107(%rip), %rdi
	movl	$34, %esi
	movl	$1, %edx
	callq	fwrite@PLT
	movl	$1, %edi
	callq	exit@PLT
.LBB26_340:
	leaq	.L.str.81(%rip), %rdi
	leaq	.L.str.23(%rip), %rsi
	leaq	.L__PRETTY_FUNCTION__.gen_filename(%rip), %rcx
	movl	$1205, %edx                     # imm = 0x4B5
	callq	__assert_fail@PLT
.LBB26_341:
	leaq	.L.str.80(%rip), %rdi
	leaq	.L.str.23(%rip), %rsi
	leaq	.L__PRETTY_FUNCTION__.mutex_init(%rip), %rcx
	movl	$56, %edx
	callq	__assert_fail@PLT
.LBB26_342:                             # %write_priv_table.exit.i
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str.86(%rip), %rsi
	movq	-344(%rbp), %rdx                # 8-byte Reload
	xorl	%eax, %eax
	callq	fprintf@PLT
	movl	$1, %edi
	callq	exit@PLT
.LBB26_343:
	leaq	.L.str.87(%rip), %rdi
	leaq	.L.str.23(%rip), %rsi
	leaq	.L__PRETTY_FUNCTION__.read_priv_table(%rip), %rcx
	movl	$1142, %edx                     # imm = 0x476
	callq	__assert_fail@PLT
.LBB26_344:
	leaq	.L.str.87(%rip), %rdi
	leaq	.L.str.23(%rip), %rsi
	leaq	.L__PRETTY_FUNCTION__.read_pub_table(%rip), %rcx
	movl	$1077, %edx                     # imm = 0x435
	callq	__assert_fail@PLT
.LBB26_345:
	leaq	.L.str.87(%rip), %rdi
	leaq	.L.str.23(%rip), %rsi
	leaq	.L__PRETTY_FUNCTION__.write_pub_table(%rip), %rcx
	movl	$1109, %edx                     # imm = 0x455
	callq	__assert_fail@PLT
.LBB26_346:
	leaq	.L.str.88(%rip), %rdi
	leaq	.L.str.23(%rip), %rsi
	leaq	.L__PRETTY_FUNCTION__.write_pub_table(%rip), %rcx
	movl	$1130, %edx                     # imm = 0x46A
	callq	__assert_fail@PLT
.LBB26_347:
	leaq	.L.str.87(%rip), %rdi
	leaq	.L.str.23(%rip), %rsi
	leaq	.L__PRETTY_FUNCTION__.write_priv_table(%rip), %rcx
	movl	$1174, %edx                     # imm = 0x496
	callq	__assert_fail@PLT
.LBB26_348:
	leaq	.L.str.89(%rip), %rdi
	leaq	.L.str.23(%rip), %rsi
	leaq	.L__PRETTY_FUNCTION__.write_priv_table(%rip), %rcx
	movl	$1189, %edx                     # imm = 0x4A5
	callq	__assert_fail@PLT
.Lfunc_end26:
	.size	main, .Lfunc_end26-main
	.cfi_endproc
	.section	.rodata,"a",@progbits
	.p2align	2
.LJTI26_0:
	.long	.LBB26_170-.LJTI26_0
	.long	.LBB26_327-.LJTI26_0
	.long	.LBB26_171-.LJTI26_0
	.long	.LBB26_179-.LJTI26_0
                                        # -- End function
	.text
	.p2align	4, 0x90                         # -- Begin function make_seed
	.type	make_seed,@function
make_seed:                              # @make_seed
	.cfi_startproc
# %bb.0:
	pushq	%r15
	.cfi_def_cfa_offset 16
	pushq	%r14
	.cfi_def_cfa_offset 24
	pushq	%rbx
	.cfi_def_cfa_offset 32
	.cfi_offset %rbx, -32
	.cfi_offset %r14, -24
	.cfi_offset %r15, -16
	movl	$48, %edi
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB27_8
# %bb.1:
	movq	%rax, %rbx
	xorps	%xmm0, %xmm0
	movaps	%xmm0, 32(%rax)
	leaq	.L.str.96(%rip), %rdi
	leaq	.L.str.44(%rip), %rsi
	callq	fopen@PLT
	testq	%rax, %rax
	je	.LBB27_3
# %bb.2:                                # %init_rand.exit
	movq	%rax, %r14
	movl	$1, %esi
	movl	$48, %edx
	movq	%rbx, %rdi
	movq	%rax, %rcx
	callq	fread@PLT
	movq	%rax, %r15
	movq	%r14, %rdi
	callq	fclose@PLT
	cmpq	$48, %r15
	jne	.LBB27_3
# %bb.5:
	movq	32(%rbx), %rax
	orq	40(%rbx), %rax
	je	.LBB27_6
# %bb.7:
	movq	%rbx, %rax
	popq	%rbx
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	retq
.LBB27_3:                               # %init_rand.exit.thread
	.cfi_def_cfa_offset 32
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rcx
	leaq	.L.str.94(%rip), %rdi
	movl	$34, %esi
	jmp	.LBB27_4
.LBB27_8:
	leaq	.L.str.93(%rip), %rdi
	leaq	.L.str.23(%rip), %rsi
	leaq	.L__PRETTY_FUNCTION__.make_seed(%rip), %rcx
	movl	$255, %edx
	callq	__assert_fail@PLT
.LBB27_6:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rcx
	leaq	.L.str.95(%rip), %rdi
	movl	$41, %esi
.LBB27_4:                               # %init_rand.exit.thread
	movl	$1, %edx
	callq	fwrite@PLT
	movl	$1, %edi
	callq	exit@PLT
.Lfunc_end27:
	.size	make_seed, .Lfunc_end27-make_seed
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function write_hash160
	.type	write_hash160,@function
write_hash160:                          # @write_hash160
	.cfi_startproc
# %bb.0:
	pushq	%r14
	.cfi_def_cfa_offset 16
	pushq	%rbx
	.cfi_def_cfa_offset 24
	pushq	%rax
	.cfi_def_cfa_offset 32
	.cfi_offset %rbx, -24
	.cfi_offset %r14, -16
	movq	%rdi, %rbx
	movzbl	32(%rsp), %edx
	leaq	.L.str.103(%rip), %r14
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	33(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	34(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	35(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	36(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	37(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	38(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	39(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	40(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	41(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	42(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	43(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	44(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	45(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	46(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	47(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	48(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	49(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	50(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	callq	fprintf@PLT
	movzbl	51(%rsp), %edx
	movq	%rbx, %rdi
	movq	%r14, %rsi
	xorl	%eax, %eax
	addq	$8, %rsp
	.cfi_def_cfa_offset 24
	popq	%rbx
	.cfi_def_cfa_offset 16
	popq	%r14
	.cfi_def_cfa_offset 8
	jmp	fprintf@PLT                     # TAILCALL
.Lfunc_end28:
	.size	write_hash160, .Lfunc_end28-write_hash160
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function gen_priv_key
	.type	gen_priv_key,@function
gen_priv_key:                           # @gen_priv_key
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movzwl	56(%rsp), %eax
	rolw	$8, %ax
	movb	$16, %cl
	subb	%sil, %cl
	movl	$65535, %ebp                    # imm = 0xFFFF
	movl	$65535, %edx                    # imm = 0xFFFF
	shll	%cl, %edx
	cmpq	$16, %rsi
	cmovael	%ebp, %edx
	andl	%eax, %edx
	movzwl	%dx, %eax
	shlq	$5, %rax
	leaq	priv_bases(%rip), %rcx
	movq	(%rax,%rcx), %rbx
	movq	8(%rax,%rcx), %r15
	movq	16(%rax,%rcx), %r10
	leaq	-1(%rsi), %r9
	movq	24(%rax,%rcx), %rax
	cmpq	$16, %r9
	jae	.LBB29_1
.LBB29_6:                               # %._crit_edge
	movq	%rax, %rcx
	shrq	$56, %rcx
	movb	%cl, (%rdi)
	movq	%rax, %rcx
	shrq	$48, %rcx
	movb	%cl, 1(%rdi)
	movq	%rax, %rcx
	shrq	$40, %rcx
	movb	%cl, 2(%rdi)
	movq	%rax, %rcx
	shrq	$32, %rcx
	movb	%cl, 3(%rdi)
	movq	%rax, %rcx
	shrq	$24, %rcx
	movb	%cl, 4(%rdi)
	movq	%rax, %rcx
	shrq	$16, %rcx
	movb	%cl, 5(%rdi)
	movb	%ah, 6(%rdi)
	movb	%al, 7(%rdi)
	movq	%r10, %rcx
	movq	%r10, %rax
	shrq	$56, %rax
	movb	%al, 8(%rdi)
	movq	%rcx, %rax
	shrq	$48, %rax
	movb	%al, 9(%rdi)
	movq	%rcx, %rax
	shrq	$40, %rax
	movb	%al, 10(%rdi)
	movq	%rcx, %rax
	shrq	$32, %rax
	movb	%al, 11(%rdi)
	movq	%rcx, %rax
	shrq	$24, %rax
	movb	%al, 12(%rdi)
	movq	%rcx, %rax
	shrq	$16, %rax
	movb	%al, 13(%rdi)
	movb	%ch, 14(%rdi)
	movb	%cl, 15(%rdi)
	movq	%r15, %rcx
	movq	%r15, %rax
	shrq	$56, %rax
	movb	%al, 16(%rdi)
	movq	%r15, %rax
	shrq	$48, %rax
	movb	%al, 17(%rdi)
	movq	%r15, %rax
	shrq	$40, %rax
	movb	%al, 18(%rdi)
	movq	%r15, %rax
	shrq	$32, %rax
	movb	%al, 19(%rdi)
	movq	%r15, %rax
	shrq	$24, %rax
	movb	%al, 20(%rdi)
	movq	%r15, %rax
	shrq	$16, %rax
	movb	%al, 21(%rdi)
	movb	%ch, 22(%rdi)
	movb	%cl, 23(%rdi)
	movq	%rbx, %rax
	shrq	$56, %rax
	movb	%al, 24(%rdi)
	movq	%rbx, %rax
	shrq	$48, %rax
	movb	%al, 25(%rdi)
	movq	%rbx, %rax
	shrq	$40, %rax
	movb	%al, 26(%rdi)
	movq	%rbx, %rax
	shrq	$32, %rax
	movb	%al, 27(%rdi)
	movq	%rbx, %rax
	shrq	$24, %rax
	movb	%al, 28(%rdi)
	movq	%rbx, %rax
	shrq	$16, %rax
	movb	%al, 29(%rdi)
	movb	%bh, 30(%rdi)
	movb	%bl, 31(%rdi)
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB29_1:                               # %.lr.ph.preheader
	.cfi_def_cfa_offset 56
	shrq	$4, %r9
	leaq	-16(%rsi), %r14
	movl	$32, %ecx
	subl	%esi, %ecx
	movq	%rcx, -8(%rsp)                  # 8-byte Spill
	xorl	%r11d, %r11d
	xorl	%r8d, %r8d
	jmp	.LBB29_2
	.p2align	4, 0x90
.LBB29_4:                               #   in Loop: Header=BB29_2 Depth=1
	leaq	56(%rsp), %rcx
	movzwl	2(%rcx,%r8,2), %ebp
	rolw	$8, %bp
	movq	-8(%rsp), %rcx                  # 8-byte Reload
	addl	%r11d, %ecx
	movl	$65535, %r13d                   # imm = 0xFFFF
                                        # kill: def $cl killed $cl killed $ecx
	shll	%cl, %r13d
	cmpq	$16, %r14
	movl	$65535, %ecx                    # imm = 0xFFFF
	cmovael	%ecx, %r13d
	andl	%ebp, %r13d
.LBB29_5:                               # %get_bits.exit
                                        #   in Loop: Header=BB29_2 Depth=1
	movzwl	%r13w, %ecx
	leaq	(%rcx,%rcx,8), %rbp
	shlq	$5, %rbp
	leaq	priv_offsets(%rip), %rcx
	addq	%rcx, %rbp
	movq	%rbx, %r13
	addq	(%rbp,%r11,2), %r13
	adcq	8(%rbp,%r11,2), %r15
	adcq	16(%rbp,%r11,2), %r10
	adcq	24(%rbp,%r11,2), %rax
	setb	%r11b
	cmpq	$-1, %rax
	setne	%dl
	cmpq	$-2, %r10
	setb	%bl
	orb	%dl, %bl
	movzbl	%bl, %edx
	xorl	%ebx, %ebx
	cmpq	$-1, %r10
	sete	%bl
	movl	%edx, %ebp
	notl	%ebp
	andl	%ebx, %ebp
	xorl	%ebx, %ebx
	movabsq	$-4994812053365940165, %rcx     # imm = 0xBAAEDCE6AF48A03B
	cmpq	%rcx, %r15
	seta	%bl
	setb	%cl
	orb	%cl, %dl
	movzbl	%dl, %ecx
	notl	%ecx
	andl	%ecx, %ebx
	orl	%ebp, %ebx
	xorl	%ebp, %ebp
	movabsq	$-4624529908474429120, %rdx     # imm = 0xBFD25E8CD0364140
	cmpq	%rdx, %r13
	seta	%bpl
	andl	%ecx, %ebp
	orl	%ebx, %ebp
	addb	$255, %r11b
	adcl	$0, %ebp
	movq	%rbp, %rbx
	movabsq	$4624529908474429119, %rcx      # imm = 0x402DA1732FC9BEBF
	imulq	%rcx, %rbx
	movq	%rbp, %rcx
	movq	%rax, %rdx
	movabsq	$4994812053365940164, %rax      # imm = 0x4551231950B75FC4
	imulq	%rax, %rcx
	movq	%rdx, %rax
	addq	%r13, %rbx
	adcq	%rcx, %r15
	adcq	%rbp, %r10
	adcq	$0, %rax
	addq	$1, %r8
	addq	$-16, %r14
	movq	%r12, %r11
	cmpq	%r8, %r9
	je	.LBB29_6
.LBB29_2:                               # %.lr.ph
                                        # =>This Inner Loop Header: Depth=1
	leaq	16(%r11), %r12
	cmpq	%rsi, %r12
	jbe	.LBB29_4
# %bb.3:                                #   in Loop: Header=BB29_2 Depth=1
	xorl	%r13d, %r13d
	jmp	.LBB29_5
.Lfunc_end29:
	.size	gen_priv_key, .Lfunc_end29-gen_priv_key
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function gen_pub_key
	.type	gen_pub_key,@function
gen_pub_key:                            # @gen_pub_key
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$264, %rsp                      # imm = 0x108
	.cfi_def_cfa_offset 320
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rdi, 40(%rsp)                  # 8-byte Spill
	movzwl	320(%rsp), %eax
	rolw	$8, %ax
	movb	$16, %cl
	subb	%sil, %cl
	movl	$65535, %edi                    # imm = 0xFFFF
	movl	$65535, %edx                    # imm = 0xFFFF
	shll	%cl, %edx
	cmpq	$16, %rsi
	cmovael	%edi, %edx
	andl	%eax, %edx
	movzwl	%dx, %eax
	shlq	$7, %rax
	leaq	bases(%rip), %rcx
	movups	112(%rax,%rcx), %xmm0
	movaps	%xmm0, 160(%rsp)
	movups	96(%rax,%rcx), %xmm0
	movaps	%xmm0, 144(%rsp)
	movups	80(%rax,%rcx), %xmm0
	movaps	%xmm0, 128(%rsp)
	movups	64(%rax,%rcx), %xmm0
	movaps	%xmm0, 112(%rsp)
	movups	(%rax,%rcx), %xmm0
	movups	16(%rax,%rcx), %xmm1
	movups	32(%rax,%rcx), %xmm2
	movups	48(%rax,%rcx), %xmm3
	movaps	%xmm3, 96(%rsp)
	movaps	%xmm2, 80(%rsp)
	movaps	%xmm1, 64(%rsp)
	movaps	%xmm0, 48(%rsp)
	movq	%rsi, 24(%rsp)                  # 8-byte Spill
	leaq	-1(%rsi), %rax
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	cmpq	$16, %rax
	jae	.LBB30_1
.LBB30_6:                               # %._crit_edge
	leaq	128(%rsp), %rbp
	movq	%rbp, %rdi
	movq	%rbp, %rsi
	callq	secp256k1_fe_inv_var
	leaq	224(%rsp), %rdi
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 184(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, (%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 8(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rsp), %rsi
	movq	184(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	184(%rsp), %rdi
	leaq	224(%rsp), %rbx
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 8(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 32(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, (%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	32(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	(%rsp), %rsi
	movq	8(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	48(%rsp), %rdi
	leaq	224(%rsp), %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 8(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 32(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, (%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	32(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	(%rsp), %rsi
	movq	8(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	88(%rsp), %rdi
	leaq	184(%rsp), %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 8(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 32(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, (%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	32(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	(%rsp), %rsi
	movq	8(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	56(%rsp), %rax
	movq	64(%rsp), %r8
	movq	72(%rsp), %rbp
	movq	80(%rsp), %rdi
	movb	88(%rsp), %bl
	movq	%rax, %rcx
	shlq	$52, %rcx
	orq	48(%rsp), %rcx
	andb	$1, %bl
	orb	$2, %bl
	movq	40(%rsp), %rdx                  # 8-byte Reload
	movb	%bl, (%rdx)
	shlq	$16, %rdi
	movq	%rbp, %rsi
	shrq	$36, %rsi
	orq	%rdi, %rsi
	bswapq	%rsi
	movq	%rsi, 1(%rdx)
	shlq	$28, %rbp
	movq	%r8, %rsi
	shrq	$24, %rsi
	orq	%rbp, %rsi
	bswapq	%rsi
	movq	%rsi, 9(%rdx)
	shlq	$40, %r8
	shrq	$12, %rax
	orq	%r8, %rax
	bswapq	%rax
	movq	%rax, 17(%rdx)
	bswapq	%rcx
	movq	%rcx, 25(%rdx)
	addq	$264, %rsp                      # imm = 0x108
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB30_1:                               # %.lr.ph.preheader
	.cfi_def_cfa_offset 320
	shrq	$4, 16(%rsp)                    # 8-byte Folded Spill
	movl	$32, %ebp
	movq	24(%rsp), %rax                  # 8-byte Reload
	subl	%eax, %ebp
	leaq	-16(%rax), %r15
	leaq	offsets(%rip), %r13
	movl	$16, %r14d
	xorl	%r12d, %r12d
	leaq	48(%rsp), %rbx
	jmp	.LBB30_2
	.p2align	4, 0x90
.LBB30_4:                               #   in Loop: Header=BB30_2 Depth=1
	leaq	320(%rsp), %rax
	movzwl	2(%rax,%r12,2), %edx
	rolw	$8, %dx
	movl	$65535, %eax                    # imm = 0xFFFF
	movl	%ebp, %ecx
	shll	%cl, %eax
	cmpq	$16, %r15
	movl	$65535, %ecx                    # imm = 0xFFFF
	cmovael	%ecx, %eax
	andl	%edx, %eax
.LBB30_5:                               # %get_bits.exit
                                        #   in Loop: Header=BB30_2 Depth=1
	movzwl	%ax, %eax
	imulq	$792, %rax, %rdx                # imm = 0x318
	addq	%r13, %rdx
	movq	%rbx, %rdi
	movq	%rbx, %rsi
	callq	secp256k1_gej_add_ge
	addq	$1, %r12
	addq	$88, %r13
	addl	$16, %ebp
	addq	$-16, %r15
	addq	$16, %r14
	cmpq	%r12, 16(%rsp)                  # 8-byte Folded Reload
	je	.LBB30_6
.LBB30_2:                               # %.lr.ph
                                        # =>This Inner Loop Header: Depth=1
	cmpq	24(%rsp), %r14                  # 8-byte Folded Reload
	jbe	.LBB30_4
# %bb.3:                                #   in Loop: Header=BB30_2 Depth=1
	xorl	%eax, %eax
	jmp	.LBB30_5
.Lfunc_end30:
	.size	gen_pub_key, .Lfunc_end30-gen_pub_key
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function make_sig
	.type	make_sig,@function
make_sig:                               # @make_sig
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset %rbp, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r12
	pushq	%rbx
	subq	$112, %rsp
	.cfi_offset %rbx, -48
	.cfi_offset %r12, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	movq	%rsi, %r12
	movq	%rdi, %r14
	movq	%rsi, %rdi
	callq	strlen@PLT
	leaq	26(%rax), %r15
	movq	%rsp, %rcx
	leaq	41(%rax), %rdx
	andq	$-16, %rdx
	movq	%rcx, %rbx
	subq	%rdx, %rbx
	movq	%rbx, %rsp
	movb	$24, (%rbx)
	movabsq	$737012983719490405, %rsi       # imm = 0xA3A656761737365
	movq	%rsi, 17(%rbx)
	movups	.L.str.108(%rip), %xmm0
	movups	%xmm0, 1(%rbx)
	negq	%rdx
	movb	%al, 25(%rcx,%rdx)
	leaq	26(%rbx), %rdi
	movq	%r12, %rsi
	movq	%rax, %rdx
	callq	memcpy@PLT
	leaq	-144(%rbp), %r12
	movq	%r12, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	sha256d
	movl	$-1, -36(%rbp)
	movq	cxt(%rip), %rdi
	leaq	-111(%rbp), %rdx
	subq	$8, %rsp
	leaq	-36(%rbp), %rax
	leaq	nonce_function_rfc6979(%rip), %r8
	leaq	16(%rbp), %rcx
	movq	%r12, %rsi
	xorl	%r9d, %r9d
	pushq	%rax
	callq	secp256k1_ecdsa_sign_compact
	addq	$16, %rsp
	testl	%eax, %eax
	je	.LBB31_15
# %bb.1:
	movl	-36(%rbp), %eax
	cmpl	$-1, %eax
	je	.LBB31_15
# %bb.2:
	addb	$31, %al
	movb	%al, -112(%rbp)
	xorl	%eax, %eax
	leaq	.L.str.110(%rip), %r8
	xorl	%r10d, %r10d
	xorl	%r9d, %r9d
	xorl	%ebx, %ebx
.LBB31_3:                               # %.outer.i
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB31_4 Depth 2
	leaq	(%r14,%rax), %rcx
	leaq	(%r10,%rbp), %rdx
	addq	$-112, %rdx
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB31_4:                               #   Parent Loop BB31_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movzbl	(%rdx,%rsi), %edi
	cmpl	$2, %ebx
	jne	.LBB31_5
# %bb.16:                               # %.thread.i
                                        #   in Loop: Header=BB31_4 Depth=2
	movl	%edi, %ebx
	shrl	$6, %ebx
	orl	%r9d, %ebx
	movslq	%ebx, %rbx
	movzbl	(%rbx,%r8), %ebx
	movb	%bl, (%rcx,%rsi,2)
	andl	$63, %edi
	movzbl	(%rdi,%r8), %ebx
	addq	$2, %rax
	movb	%bl, 1(%rcx,%rsi,2)
	leaq	(%r10,%rsi), %rdi
	addq	$1, %rdi
	addq	$1, %rsi
	xorl	%ebx, %ebx
	cmpq	$65, %rdi
	jne	.LBB31_4
	jmp	.LBB31_14
	.p2align	4, 0x90
.LBB31_5:                               #   in Loop: Header=BB31_3 Depth=1
	cmpl	$1, %ebx
	je	.LBB31_8
# %bb.6:                                #   in Loop: Header=BB31_3 Depth=1
	testl	%ebx, %ebx
	jne	.LBB31_10
# %bb.7:                                #   in Loop: Header=BB31_3 Depth=1
	movq	%rdi, %rdx
	shrq	$2, %rdx
	movb	(%rdx,%r8), %dl
	addq	$1, %rax
	movb	%dl, (%rcx,%rsi,2)
	shll	$4, %edi
	andl	$48, %edi
	movl	$1, %ebx
	jmp	.LBB31_9
	.p2align	4, 0x90
.LBB31_8:                               #   in Loop: Header=BB31_3 Depth=1
	movl	%edi, %edx
	shrl	$4, %edx
	orl	%r9d, %edx
	movslq	%edx, %rdx
	movb	(%rdx,%r8), %dl
	addq	$1, %rax
	movb	%dl, (%rcx,%rsi,2)
	shll	$2, %edi
	andl	$60, %edi
	movl	$2, %ebx
.LBB31_9:                               # %.loopexit.i
                                        #   in Loop: Header=BB31_3 Depth=1
	movl	%edi, %r9d
.LBB31_10:                              # %.loopexit.i
                                        #   in Loop: Header=BB31_3 Depth=1
	leaq	(%r10,%rsi), %rcx
	addq	%rsi, %r10
	addq	$1, %r10
	cmpq	$64, %rcx
	jne	.LBB31_3
# %bb.11:
	movl	%r9d, %ecx
	movb	(%rcx,%r8), %cl
	movb	%cl, (%r14,%rax)
	movb	$61, 1(%r14,%rax)
	cmpl	$1, %ebx
	jne	.LBB31_12
# %bb.13:
	movb	$61, 2(%r14,%rax)
	addq	$3, %rax
	jmp	.LBB31_14
.LBB31_12:
	addq	$2, %rax
.LBB31_14:                              # %base64_encode.exit
	movb	$0, (%r14,%rax)
	leaq	-32(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa %rsp, 8
	retq
.LBB31_15:
	.cfi_def_cfa %rbp, 16
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rcx
	leaq	.L.str.109(%rip), %rdi
	movl	$36, %esi
	movl	$1, %edx
	callq	fwrite@PLT
	movl	$1, %edi
	callq	exit@PLT
.Lfunc_end31:
	.size	make_sig, .Lfunc_end31-make_sig
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function secp256k1_ge_set_xo_var
	.type	secp256k1_ge_set_xo_var,@function
secp256k1_ge_set_xo_var:                # @secp256k1_ge_set_xo_var
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$584, %rsp                      # imm = 0x248
	.cfi_def_cfa_offset 640
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	%edx, -92(%rsp)                 # 4-byte Spill
	movq	%rsi, %rbp
	movq	%rdi, -104(%rsp)                # 8-byte Spill
	movq	32(%rsi), %rax
	movq	%rax, 32(%rdi)
	movups	(%rsi), %xmm0
	movups	16(%rsi), %xmm1
	movups	%xmm1, 16(%rdi)
	movups	%xmm0, (%rdi)
	leaq	544(%rsp), %rdi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 464(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 16(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 424(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	16(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	424(%rsp), %rsi
	movq	464(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	504(%rsp), %rdi
	leaq	544(%rsp), %rbx
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 464(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 16(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 424(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	16(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	424(%rsp), %rsi
	movq	464(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	-104(%rsp), %rax                # 8-byte Reload
	movl	$0, 80(%rax)
	movq	504(%rsp), %rax
	addq	$7, %rax
	movq	%rax, 56(%rsp)
	movups	512(%rsp), %xmm0
	movups	%xmm0, 64(%rsp)
	movups	528(%rsp), %xmm0
	movups	%xmm0, 80(%rsp)
	leaq	464(%rsp), %rdi
	leaq	56(%rsp), %rbp
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	424(%rsp), %rdi
	leaq	464(%rsp), %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rbx
	movq	%rdi, %rsi
	movq	%rdi, %rbp
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movups	424(%rsp), %xmm0
	movups	440(%rsp), %xmm1
	movaps	%xmm0, 16(%rsp)
	movaps	%xmm1, 32(%rsp)
	movq	456(%rsp), %rax
	movq	%rax, 48(%rsp)
	leaq	16(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movaps	16(%rsp), %xmm0
	movaps	32(%rsp), %xmm1
	movaps	%xmm0, 384(%rsp)
	movaps	%xmm1, 400(%rsp)
	movq	48(%rsp), %rax
	movq	%rax, 416(%rsp)
	leaq	384(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movaps	384(%rsp), %xmm0
	movaps	400(%rsp), %xmm1
	movaps	%xmm0, 192(%rsp)
	movaps	%xmm1, 208(%rsp)
	movq	416(%rsp), %rax
	movq	%rax, 224(%rsp)
	leaq	192(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	464(%rsp), %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movaps	192(%rsp), %xmm0
	movaps	208(%rsp), %xmm1
	movaps	%xmm0, 144(%rsp)
	movaps	%xmm1, 160(%rsp)
	movq	224(%rsp), %rax
	movq	%rax, 176(%rsp)
	leaq	144(%rsp), %rbp
	movq	%rbp, %rdi
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	192(%rsp), %rbx
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movaps	144(%rsp), %xmm0
	movaps	160(%rsp), %xmm1
	movaps	%xmm0, -32(%rsp)
	movaps	%xmm1, -16(%rsp)
	movq	176(%rsp), %rax
	movq	%rax, (%rsp)
	leaq	-32(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movaps	-32(%rsp), %xmm0
	movaps	-16(%rsp), %xmm1
	movaps	%xmm0, 96(%rsp)
	movaps	%xmm1, 112(%rsp)
	movq	(%rsp), %rax
	movq	%rax, 128(%rsp)
	movl	$44, %ebp
	leaq	96(%rsp), %rdi
	.p2align	4, 0x90
.LBB32_1:                               # =>This Inner Loop Header: Depth=1
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	addl	$-1, %ebp
	jne	.LBB32_1
# %bb.2:
	leaq	-32(%rsp), %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movaps	96(%rsp), %xmm0
	movaps	112(%rsp), %xmm1
	movaps	%xmm0, 336(%rsp)
	movaps	%xmm1, 352(%rsp)
	movq	128(%rsp), %rax
	movq	%rax, 368(%rsp)
	movl	$88, %ebp
	leaq	336(%rsp), %rdi
	.p2align	4, 0x90
.LBB32_3:                               # =>This Inner Loop Header: Depth=1
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	addl	$-1, %ebp
	jne	.LBB32_3
# %bb.4:
	leaq	96(%rsp), %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movaps	336(%rsp), %xmm0
	movaps	352(%rsp), %xmm1
	movaps	%xmm0, 288(%rsp)
	movaps	%xmm1, 304(%rsp)
	movq	368(%rsp), %rax
	movq	%rax, 320(%rsp)
	movl	$44, %ebp
	leaq	288(%rsp), %rdi
	.p2align	4, 0x90
.LBB32_5:                               # =>This Inner Loop Header: Depth=1
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	addl	$-1, %ebp
	jne	.LBB32_5
# %bb.6:
	leaq	-32(%rsp), %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movaps	288(%rsp), %xmm0
	movaps	304(%rsp), %xmm1
	movaps	%xmm0, 240(%rsp)
	movaps	%xmm1, 256(%rsp)
	movq	320(%rsp), %rax
	movq	%rax, 272(%rsp)
	leaq	240(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	424(%rsp), %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movaps	240(%rsp), %xmm0
	movaps	256(%rsp), %xmm1
	movaps	%xmm0, -80(%rsp)
	movaps	%xmm1, -64(%rsp)
	movq	272(%rsp), %rax
	movq	%rax, -48(%rsp)
	leaq	-80(%rsp), %rbp
	movq	%rbp, %rdi
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	144(%rsp), %rbx
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	464(%rsp), %rbx
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	-104(%rsp), %rax                # 8-byte Reload
	leaq	40(%rax), %rdi
	movq	%rdi, -88(%rsp)                 # 8-byte Spill
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rdi
	movq	-88(%rsp), %rsi                 # 8-byte Reload
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -112(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	-112(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movabsq	$18014381329608892, %r11        # imm = 0x3FFFFBFFFFF0BC
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	movabsq	$1125899906842620, %r9          # imm = 0x3FFFFFFFFFFFC
	movq	%r11, %rdx
	subq	-80(%rsp), %rdx
	movq	%r9, %rcx
	subq	-48(%rsp), %rcx
	addq	56(%rsp), %rdx
	addq	88(%rsp), %rcx
	movabsq	$4294968273, %r10               # imm = 0x1000003D1
	movq	%rcx, %rax
	shrq	$48, %rax
	imulq	%r10, %rax
	addq	%rdx, %rax
	movq	%rax, %rsi
	andq	%r15, %rsi
	leaq	-1(%r10), %r12
	xorq	%rsi, %r12
	testq	%rsi, %rsi
	je	.LBB32_9
# %bb.7:
	cmpq	%r15, %r12
	je	.LBB32_9
# %bb.8:                                # %secp256k1_fe_sqrt_var.exit.thread
	xorl	%eax, %eax
	jmp	.LBB32_15
.LBB32_9:                               # %secp256k1_fe_sqrt_var.exit
	movabsq	$18014398509481980, %r14        # imm = 0x3FFFFFFFFFFFFC
	movabsq	$281474976710655, %r8           # imm = 0xFFFFFFFFFFFF
	movq	%r14, %rdi
	subq	-56(%rsp), %rdi
	addq	80(%rsp), %rdi
	movq	%r14, %rbx
	subq	-64(%rsp), %rbx
	addq	72(%rsp), %rbx
	shrq	$52, %rax
	addq	%r14, %rax
	subq	-72(%rsp), %rax
	andq	%r8, %rcx
	addq	64(%rsp), %rax
	movq	%rax, %rbp
	shrq	$52, %rbp
	addq	%rbx, %rbp
	movq	%rax, %rdx
	andq	%r15, %rdx
	orq	%rsi, %rdx
	movq	%rbp, %rbx
	shrq	$52, %rbx
	addq	%rdi, %rbx
	movq	%rbp, %rdi
	andq	%r15, %rdi
	orq	%rdx, %rdi
	movq	%rbx, %rsi
	shrq	$52, %rsi
	addq	%rcx, %rsi
	movq	%rbx, %rcx
	andq	%r15, %rcx
	orq	%rdi, %rcx
	orq	%rsi, %rcx
	je	.LBB32_11
# %bb.10:                               # %secp256k1_fe_sqrt_var.exit
	andq	%r12, %rax
	andq	%rbp, %rax
	andq	%rbx, %rax
	movabsq	$4222124650659840, %rcx         # imm = 0xF000000000000
	xorq	%rcx, %rsi
	andq	%rax, %rsi
	xorl	%eax, %eax
	cmpq	%r15, %rsi
	jne	.LBB32_15
.LBB32_11:
	movq	-104(%rsp), %rsi                # 8-byte Reload
	movq	72(%rsi), %rax
	movq	%rax, %rbp
	shrq	$48, %rbp
	imulq	%r10, %rbp
	addq	40(%rsi), %rbp
	movq	%rbp, %rdx
	shrq	$52, %rdx
	addq	48(%rsi), %rdx
	andq	%r8, %rax
	movq	%rdx, %rbx
	shrq	$52, %rbx
	addq	56(%rsi), %rbx
	andq	%r15, %rbp
	movq	%rbx, %rcx
	shrq	$52, %rcx
	addq	64(%rsi), %rcx
	andq	%r15, %rbx
	movq	%rbx, %rdi
	andq	%rdx, %rdi
	andq	%r15, %rdx
	movq	%rcx, %r12
	shrq	$52, %r12
	addq	%rax, %r12
	andq	%rcx, %rdi
	andq	%r15, %rcx
	movq	%r12, %r13
	shrq	$48, %r13
	movq	%r12, %rsi
	xorq	%r8, %rsi
	xorq	%r15, %rdi
	orq	%rsi, %rdi
	sete	%sil
	movabsq	$4503595332402222, %rdi         # imm = 0xFFFFEFFFFFC2E
	cmpq	%rdi, %rbp
	seta	%al
	andb	%sil, %al
	movzbl	%al, %eax
	orq	%r13, %rax
	je	.LBB32_13
# %bb.12:
	addq	%r10, %rbp
	movq	%rbp, %rax
	shrq	$52, %rax
	addq	%rax, %rdx
	andq	%r15, %rbp
	movq	%rdx, %rax
	shrq	$52, %rax
	addq	%rax, %rbx
	andq	%r15, %rdx
	movq	%rbx, %rax
	shrq	$52, %rax
	addq	%rax, %rcx
	andq	%r15, %rbx
	movq	%rcx, %rax
	shrq	$52, %rax
	addq	%r12, %rax
	andq	%r15, %rcx
	andq	%r8, %rax
	movq	%rax, %r12
.LBB32_13:                              # %secp256k1_fe_normalize_var.exit
	movq	-104(%rsp), %rdi                # 8-byte Reload
	movq	%rbp, 40(%rdi)
	movq	%rdx, 48(%rdi)
	movq	%rbx, 56(%rdi)
	movq	%rcx, 64(%rdi)
	movq	%r12, 72(%rdi)
	movl	%ebp, %esi
	andl	$1, %esi
	movl	$1, %eax
	cmpl	-92(%rsp), %esi                 # 4-byte Folded Reload
	je	.LBB32_15
# %bb.14:
	subq	%rbp, %r11
	movq	%r11, 40(%rdi)
	movq	%r14, %rsi
	subq	%rdx, %rsi
	movq	%rsi, 48(%rdi)
	movq	%r14, %rdx
	subq	%rbx, %rdx
	movq	%rdx, 56(%rdi)
	subq	%rcx, %r14
	movq	%r14, 64(%rdi)
	subq	%r12, %r9
	movq	%r9, 72(%rdi)
.LBB32_15:
	addq	$584, %rsp                      # imm = 0x248
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end32:
	.size	secp256k1_ge_set_xo_var, .Lfunc_end32-secp256k1_ge_set_xo_var
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function secp256k1_gej_add_ge_var
.LCPI33_0:
	.quad	36028762659217784               # 0x7ffff7ffffe178
	.quad	36028797018963960               # 0x7ffffffffffff8
.LCPI33_1:
	.quad	36028797018963960               # 0x7ffffffffffff8
	.quad	36028797018963960               # 0x7ffffffffffff8
.LCPI33_2:
	.quad	18014381329608892               # 0x3ffffbfffff0bc
	.quad	18014398509481980               # 0x3ffffffffffffc
.LCPI33_3:
	.quad	18014398509481980               # 0x3ffffffffffffc
	.quad	18014398509481980               # 0x3ffffffffffffc
	.text
	.p2align	4, 0x90
	.type	secp256k1_gej_add_ge_var,@function
secp256k1_gej_add_ge_var:               # @secp256k1_gej_add_ge_var
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$584, %rsp                      # imm = 0x248
	.cfi_def_cfa_offset 640
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rdx, %rbp
	cmpl	$0, 120(%rsi)
	movl	80(%rdx), %eax
	je	.LBB33_2
# %bb.1:
	movl	%eax, 120(%rdi)
	movups	(%rbp), %xmm0
	movups	16(%rbp), %xmm1
	movups	%xmm0, (%rdi)
	movups	%xmm1, 16(%rdi)
	movq	32(%rbp), %rax
	movq	%rax, 32(%rdi)
	movdqu	40(%rbp), %xmm0
	movdqu	56(%rbp), %xmm1
	movdqu	%xmm0, 40(%rdi)
	movdqu	%xmm1, 56(%rdi)
	movq	72(%rbp), %rax
	movq	%rax, 72(%rdi)
	movq	$1, 80(%rdi)
	pxor	%xmm0, %xmm0
	movdqu	%xmm0, 88(%rdi)
	movdqu	%xmm0, 104(%rdi)
	jmp	.LBB33_15
.LBB33_2:
	testl	%eax, %eax
	je	.LBB33_4
# %bb.3:
	movups	112(%rsi), %xmm0
	movups	%xmm0, 112(%rdi)
	movups	96(%rsi), %xmm0
	movups	%xmm0, 96(%rdi)
	movups	80(%rsi), %xmm0
	movups	%xmm0, 80(%rdi)
	movups	64(%rsi), %xmm0
	movups	%xmm0, 64(%rdi)
	movdqu	(%rsi), %xmm0
	movdqu	16(%rsi), %xmm1
	movdqu	32(%rsi), %xmm2
	movdqu	48(%rsi), %xmm3
	movdqu	%xmm3, 48(%rdi)
	movdqu	%xmm2, 32(%rdi)
	movdqu	%xmm1, 16(%rdi)
	movdqu	%xmm0, (%rdi)
	jmp	.LBB33_15
.LBB33_4:
	movq	%rdi, 32(%rsp)                  # 8-byte Spill
	movl	$0, 120(%rdi)
	leaq	80(%rsi), %rax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	leaq	544(%rsp), %rdi
	movq	%rsi, 72(%rsp)                  # 8-byte Spill
	movq	%rax, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 144(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 80(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 424(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	80(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	424(%rsp), %rsi
	movq	144(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	72(%rsp), %rax                  # 8-byte Reload
	movups	(%rax), %xmm0
	movups	16(%rax), %xmm1
	movaps	%xmm0, 144(%rsp)
	movaps	%xmm1, 160(%rsp)
	movq	32(%rax), %rax
	movq	%rax, 176(%rsp)
	movq	%rax, %rdx
	shrq	$48, %rdx
	movabsq	$281474976710655, %rcx          # imm = 0xFFFFFFFFFFFF
	andq	%rcx, %rax
	movabsq	$4294968273, %rcx               # imm = 0x1000003D1
	imulq	%rcx, %rdx
	addq	144(%rsp), %rdx
	movq	%rdx, %rsi
	shrq	$52, %rsi
	addq	152(%rsp), %rsi
	movabsq	$4503599627370495, %rcx         # imm = 0xFFFFFFFFFFFFF
	andq	%rcx, %rdx
	movq	%rdx, %r8
	movq	%rdx, 40(%rsp)                  # 8-byte Spill
	movq	%rsi, %rdx
	shrq	$52, %rdx
	addq	160(%rsp), %rdx
	andq	%rcx, %rsi
	movq	%rsi, %rbx
	movq	%rsi, 48(%rsp)                  # 8-byte Spill
	movq	%rdx, %rsi
	shrq	$52, %rsi
	addq	168(%rsp), %rsi
	andq	%rcx, %rdx
	movq	%rdx, %rdi
	movq	%rdx, 56(%rsp)                  # 8-byte Spill
	movq	%rsi, %rdx
	shrq	$52, %rdx
	addq	%rax, %rdx
	movq	%rdx, 8(%rsp)                   # 8-byte Spill
	andq	%rcx, %rsi
	movq	%rsi, 64(%rsp)                  # 8-byte Spill
	movq	%r8, 144(%rsp)
	movq	%rbx, 152(%rsp)
	movq	%rdi, 160(%rsp)
	movq	%rsi, 168(%rsp)
	movq	%rdx, 176(%rsp)
	leaq	424(%rsp), %rdi
	leaq	544(%rsp), %rbx
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 80(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 272(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 384(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	272(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	384(%rsp), %rsi
	movq	80(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	72(%rsp), %rax                  # 8-byte Reload
	movdqu	40(%rax), %xmm0
	movdqu	56(%rax), %xmm1
	movdqa	%xmm0, 80(%rsp)
	movdqa	%xmm1, 96(%rsp)
	movq	72(%rax), %rdi
	movq	%rdi, 112(%rsp)
	movq	%rdi, %rax
	shrq	$48, %rax
	movabsq	$4294968273, %rcx               # imm = 0x1000003D1
	imulq	%rcx, %rax
	addq	80(%rsp), %rax
	movq	%rax, %rbx
	shrq	$52, %rbx
	addq	88(%rsp), %rbx
	movabsq	$281474976710655, %rdx          # imm = 0xFFFFFFFFFFFF
	andq	%rdx, %rdi
	movq	%rbx, %rcx
	shrq	$52, %rcx
	addq	96(%rsp), %rcx
	movq	%rcx, %rdx
	shrq	$52, %rdx
	addq	104(%rsp), %rdx
	movq	%rdx, %rsi
	shrq	$52, %rsi
	addq	%rdi, %rsi
	movq	%rsi, 328(%rsp)                 # 8-byte Spill
	movabsq	$18014381329608892, %r8         # imm = 0x3FFFFBFFFFF0BC
	movq	%r8, %rdi
	movq	%r8, %r9
	subq	40(%rsp), %rdi                  # 8-byte Folded Reload
	movq	%rdi, 40(%rsp)                  # 8-byte Spill
	movabsq	$18014398509481980, %r10        # imm = 0x3FFFFFFFFFFFFC
	movq	%r10, %r8
	subq	48(%rsp), %r8                   # 8-byte Folded Reload
	movq	%r10, %rdi
	subq	56(%rsp), %rdi                  # 8-byte Folded Reload
	movq	%rdi, 56(%rsp)                  # 8-byte Spill
	movq	%r10, %rdi
	subq	64(%rsp), %rdi                  # 8-byte Folded Reload
	movq	%rdi, 64(%rsp)                  # 8-byte Spill
	movabsq	$1125899906842620, %rdi         # imm = 0x3FFFFFFFFFFFC
	subq	8(%rsp), %rdi                   # 8-byte Folded Reload
	movq	%rdi, 48(%rsp)                  # 8-byte Spill
	movabsq	$4503599627370495, %rdi         # imm = 0xFFFFFFFFFFFFF
	andq	%rdi, %rax
	movq	%rax, 80(%rsp)
	subq	%rax, %r9
	movq	%r9, 336(%rsp)                  # 8-byte Spill
	movq	%rdi, %rax
	andq	%rdi, %rbx
	movq	%rbx, 88(%rsp)
	movq	%r10, %rdi
	subq	%rbx, %rdi
	movq	%rdi, 8(%rsp)                   # 8-byte Spill
	andq	%rax, %rcx
	movq	%rcx, 96(%rsp)
	movq	%r10, %rdi
	subq	%rcx, %rdi
	movq	%rdi, 320(%rsp)                 # 8-byte Spill
	andq	%rax, %rdx
	movq	%rdx, 104(%rsp)
	subq	%rdx, %r10
	movq	%r10, 312(%rsp)                 # 8-byte Spill
	movq	%rsi, 112(%rsp)
	addq	$40, %rbp
	leaq	384(%rsp), %rdi
	leaq	544(%rsp), %rbx
	movq	%rbp, %rsi
	movq	%r8, %rbp
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 272(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 464(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 344(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	464(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	344(%rsp), %rsi
	movq	272(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	128(%rsp), %rbx                 # 8-byte Reload
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 272(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 464(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 344(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	464(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	344(%rsp), %rsi
	movq	272(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	56(%rsp), %rcx                  # 8-byte Reload
	movq	48(%rsp), %rbx                  # 8-byte Reload
	movq	320(%rsp), %r13                 # 8-byte Reload
	movq	336(%rsp), %r12                 # 8-byte Reload
	movq	312(%rsp), %r15                 # 8-byte Reload
	movabsq	$4294968273, %r14               # imm = 0x1000003D1
	movabsq	$4503599627370495, %r11         # imm = 0xFFFFFFFFFFFFF
	movq	40(%rsp), %rax                  # 8-byte Reload
	addq	424(%rsp), %rax
	movabsq	$1125899906842620, %r10         # imm = 0x3FFFFFFFFFFFC
	subq	328(%rsp), %r10                 # 8-byte Folded Reload
	movq	%rax, 272(%rsp)
	addq	432(%rsp), %rbp
	addq	456(%rsp), %rbx
	movq	%rbx, %rsi
	shrq	$48, %rsi
	imulq	%r14, %rsi
	addq	%rax, %rsi
	movq	%rbp, 280(%rsp)
	addq	440(%rsp), %rcx
	movq	%rcx, 288(%rsp)
	movq	64(%rsp), %rax                  # 8-byte Reload
	addq	448(%rsp), %rax
	movq	%rax, %r8
	movq	%rax, 296(%rsp)
	movq	%rbx, 304(%rsp)
	addq	384(%rsp), %r12
	movq	%r12, 344(%rsp)
	movq	8(%rsp), %rax                   # 8-byte Reload
	addq	392(%rsp), %rax
	addq	400(%rsp), %r13
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movq	%rax, 352(%rsp)
	movq	%r13, 360(%rsp)
	addq	408(%rsp), %r15
	movq	%r15, 368(%rsp)
	addq	416(%rsp), %r10
	movq	%r10, 376(%rsp)
	movq	%rsi, %rdi
	andq	%r11, %rdi
	leaq	-1(%r14), %r9
	movq	%rdi, %rdx
	xorq	%r9, %rdx
	testq	%rdi, %rdi
	je	.LBB33_6
# %bb.5:
	cmpq	%r11, %rdx
	jne	.LBB33_14
.LBB33_6:                               # %secp256k1_fe_normalizes_to_zero_var.exit
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rbx
	shrq	$52, %rsi
	addq	%rbp, %rsi
	movq	%rsi, %rax
	shrq	$52, %rax
	addq	%rcx, %rax
	movq	%rsi, %rbp
	andq	%r11, %rbp
	orq	%rdi, %rbp
	movq	%rax, %rcx
	shrq	$52, %rcx
	addq	%r8, %rcx
	movq	%rax, %rdi
	andq	%r11, %rdi
	orq	%rbp, %rdi
	movq	%rcx, %rbp
	shrq	$52, %rbp
	addq	%rbx, %rbp
	movq	%rcx, %rbx
	andq	%r11, %rbx
	orq	%rdi, %rbx
	orq	%rbp, %rbx
	movabsq	$4222124650659840, %rbx         # imm = 0xF000000000000
	je	.LBB33_8
# %bb.7:                                # %secp256k1_fe_normalizes_to_zero_var.exit
	andq	%rdx, %rsi
	andq	%rax, %rsi
	andq	%rcx, %rsi
	xorq	%rbx, %rbp
	andq	%rsi, %rbp
	cmpq	%r11, %rbp
	je	.LBB33_8
.LBB33_14:                              # %secp256k1_fe_normalizes_to_zero_var.exit.thread
	leaq	464(%rsp), %rdi
	leaq	344(%rsp), %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 504(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 232(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 192(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	232(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	192(%rsp), %rsi
	movq	504(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	504(%rsp), %rbp
	leaq	272(%rsp), %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 192(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 16(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 232(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	16(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	232(%rsp), %rsi
	movq	192(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	192(%rsp), %rdi
	movq	%rbp, %rbx
	leaq	272(%rsp), %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 232(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 24(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 16(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	24(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rsp), %rsi
	movq	232(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	32(%rsp), %rbp                  # 8-byte Reload
	leaq	80(%rbp), %rdi
	movq	128(%rsp), %rcx                 # 8-byte Reload
	movq	32(%rcx), %rax
	movq	%rax, 112(%rbp)
	movups	(%rcx), %xmm0
	movups	16(%rcx), %xmm1
	movups	%xmm1, 96(%rbp)
	movups	%xmm0, 80(%rbp)
	leaq	272(%rsp), %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 232(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 24(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 16(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	24(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rsp), %rsi
	movq	232(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	232(%rsp), %rdi
	leaq	144(%rsp), %rsi
	leaq	504(%rsp), %rbx
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 16(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 136(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 24(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	136(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	24(%rsp), %rsi
	movq	16(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movups	232(%rsp), %xmm0
	movups	248(%rsp), %xmm1
	movups	%xmm0, (%rbp)
	movups	%xmm1, 16(%rbp)
	movq	264(%rsp), %rax
	movq	%rax, 32(%rbp)
	movdqu	(%rbp), %xmm0
	paddq	%xmm0, %xmm0
	movdqu	16(%rbp), %xmm1
	paddq	%xmm1, %xmm1
	paddq	192(%rsp), %xmm0
	paddq	208(%rsp), %xmm1
	addq	%rax, %rax
	addq	224(%rsp), %rax
	movdqa	464(%rsp), %xmm2
	psubq	%xmm0, %xmm2
	paddq	.LCPI33_0(%rip), %xmm2
	movdqu	%xmm2, (%rbp)
	movdqa	480(%rsp), %xmm0
	psubq	%xmm1, %xmm0
	paddq	.LCPI33_1(%rip), %xmm0
	movdqu	%xmm0, 16(%rbp)
	movq	496(%rsp), %rcx
	subq	%rax, %rcx
	movabsq	$2251799813685240, %r10         # imm = 0x7FFFFFFFFFFF8
	addq	%rcx, %r10
	movq	%r10, 32(%rbp)
	movq	%xmm2, %r8
	leaq	40(%rbp), %rdi
	pshufd	$238, %xmm2, %xmm1              # xmm1 = xmm2[2,3,2,3]
	movq	%xmm1, %rdx
	movabsq	$54043195528445940, %rsi        # imm = 0xBFFFFFFFFFFFF4
	movq	%rsi, %rax
	subq	%rdx, %rax
	movq	%xmm0, %rdx
	movq	%rsi, %rbx
	subq	%rdx, %rbx
	pshufd	$238, %xmm0, %xmm0              # xmm0 = xmm0[2,3,2,3]
	movq	%xmm0, %r9
	movq	232(%rsp), %rcx
	subq	%r8, %rcx
	movabsq	$54043143988826676, %rdx        # imm = 0xBFFFF3FFFFD234
	addq	%rcx, %rdx
	movq	%rdx, 40(%rbp)
	addq	240(%rsp), %rax
	subq	%r9, %rsi
	movq	%rax, 48(%rbp)
	addq	248(%rsp), %rbx
	movq	%rbx, 56(%rbp)
	addq	256(%rsp), %rsi
	movq	%rsi, 64(%rbp)
	movq	264(%rsp), %rcx
	subq	%r10, %rcx
	movabsq	$3377699720527860, %rax         # imm = 0xBFFFFFFFFFFF4
	addq	%rcx, %rax
	movq	%rax, 72(%rbp)
	leaq	344(%rsp), %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 16(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 136(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 24(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	136(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	24(%rsp), %rsi
	movq	16(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	80(%rsp), %rbx
	leaq	192(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 16(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 136(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 24(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	136(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	24(%rsp), %rsi
	movq	16(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movdqa	.LCPI33_2(%rip), %xmm0          # xmm0 = [18014381329608892,18014398509481980]
	psubq	192(%rsp), %xmm0
	movdqa	%xmm0, 192(%rsp)
	movdqa	.LCPI33_3(%rip), %xmm1          # xmm1 = [18014398509481980,18014398509481980]
	psubq	208(%rsp), %xmm1
	movdqa	%xmm1, 208(%rsp)
	movabsq	$1125899906842620, %rax         # imm = 0x3FFFFFFFFFFFC
	subq	224(%rsp), %rax
	movq	%rax, 224(%rsp)
	movdqu	40(%rbp), %xmm2
	movdqu	56(%rbp), %xmm3
	paddq	%xmm0, %xmm2
	movdqu	%xmm2, 40(%rbp)
	paddq	%xmm1, %xmm3
	movdqu	%xmm3, 56(%rbp)
	addq	%rax, 72(%rbp)
.LBB33_15:
	addq	$584, %rsp                      # imm = 0x248
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB33_8:
	.cfi_def_cfa_offset 640
	movq	%r10, %rsi
	shrq	$48, %rsi
	imulq	%r14, %rsi
	addq	%r12, %rsi
	movq	%rsi, %rdi
	andq	%r11, %rdi
	xorq	%rdi, %r9
	testq	%rdi, %rdi
	je	.LBB33_10
# %bb.9:
	cmpq	%r11, %r9
	jne	.LBB33_13
.LBB33_10:                              # %secp256k1_fe_normalizes_to_zero_var.exit41
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %r10
	shrq	$52, %rsi
	addq	8(%rsp), %rsi                   # 8-byte Folded Reload
	movq	%rsi, %rbp
	shrq	$52, %rbp
	addq	%r13, %rbp
	movq	%rsi, %rax
	andq	%r11, %rax
	orq	%rdi, %rax
	movq	%rbp, %rdi
	shrq	$52, %rdi
	addq	%r15, %rdi
	movq	%rbp, %rdx
	andq	%r11, %rdx
	orq	%rax, %rdx
	movq	%rdi, %rcx
	shrq	$52, %rcx
	addq	%r10, %rcx
	movq	%rdi, %rax
	andq	%r11, %rax
	orq	%rdx, %rax
	orq	%rcx, %rax
	je	.LBB33_12
# %bb.11:                               # %secp256k1_fe_normalizes_to_zero_var.exit41
	andq	%r9, %rsi
	andq	%rbp, %rsi
	andq	%rdi, %rsi
	xorq	%rbx, %rcx
	andq	%rsi, %rcx
	cmpq	%r11, %rcx
	je	.LBB33_12
.LBB33_13:                              # %secp256k1_fe_normalizes_to_zero_var.exit41.thread
	movq	32(%rsp), %rax                  # 8-byte Reload
	movl	$1, 120(%rax)
	jmp	.LBB33_15
.LBB33_12:
	movq	32(%rsp), %rdi                  # 8-byte Reload
	movq	72(%rsp), %rsi                  # 8-byte Reload
	callq	secp256k1_gej_double_var
	jmp	.LBB33_15
.Lfunc_end33:
	.size	secp256k1_gej_add_ge_var, .Lfunc_end33-secp256k1_gej_add_ge_var
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function secp256k1_gej_add_var
.LCPI34_0:
	.quad	18014381329608892               # 0x3ffffbfffff0bc
	.quad	18014398509481980               # 0x3ffffffffffffc
.LCPI34_1:
	.quad	18014398509481980               # 0x3ffffffffffffc
	.quad	18014398509481980               # 0x3ffffffffffffc
.LCPI34_2:
	.quad	36028762659217784               # 0x7ffff7ffffe178
	.quad	36028797018963960               # 0x7ffffffffffff8
.LCPI34_3:
	.quad	36028797018963960               # 0x7ffffffffffff8
	.quad	36028797018963960               # 0x7ffffffffffff8
	.text
	.p2align	4, 0x90
	.type	secp256k1_gej_add_var,@function
secp256k1_gej_add_var:                  # @secp256k1_gej_add_var
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$600, %rsp                      # imm = 0x258
	.cfi_def_cfa_offset 656
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rdx, %rbp
	cmpl	$0, 120(%rsi)
	je	.LBB34_3
# %bb.1:
	movups	112(%rbp), %xmm0
	movups	%xmm0, 112(%rdi)
	movups	96(%rbp), %xmm0
	movups	%xmm0, 96(%rdi)
	movups	80(%rbp), %xmm0
	movups	%xmm0, 80(%rdi)
	movups	64(%rbp), %xmm0
	movups	%xmm0, 64(%rdi)
	movdqu	(%rbp), %xmm0
	movdqu	16(%rbp), %xmm1
	movdqu	32(%rbp), %xmm2
	movdqu	48(%rbp), %xmm3
	jmp	.LBB34_2
.LBB34_3:
	cmpl	$0, 120(%rbp)
	je	.LBB34_5
# %bb.4:
	movups	112(%rsi), %xmm0
	movups	%xmm0, 112(%rdi)
	movups	96(%rsi), %xmm0
	movups	%xmm0, 96(%rdi)
	movups	80(%rsi), %xmm0
	movups	%xmm0, 80(%rdi)
	movups	64(%rsi), %xmm0
	movups	%xmm0, 64(%rdi)
	movdqu	(%rsi), %xmm0
	movdqu	16(%rsi), %xmm1
	movdqu	32(%rsi), %xmm2
	movdqu	48(%rsi), %xmm3
.LBB34_2:
	movdqu	%xmm3, 48(%rdi)
	movdqu	%xmm2, 32(%rdi)
	movdqu	%xmm1, 16(%rdi)
	movdqu	%xmm0, (%rdi)
.LBB34_16:
	addq	$600, %rsp                      # imm = 0x258
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB34_5:
	.cfi_def_cfa_offset 656
	movq	%rdi, 24(%rsp)                  # 8-byte Spill
	movl	$0, 120(%rdi)
	leaq	80(%rbp), %rax
	movq	%rax, 40(%rsp)                  # 8-byte Spill
	leaq	560(%rsp), %rdi
	movq	%rsi, 16(%rsp)                  # 8-byte Spill
	movq	%rax, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 480(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 336(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 384(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	336(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	384(%rsp), %rsi
	movq	480(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	16(%rsp), %rax                  # 8-byte Reload
	leaq	80(%rax), %rsi
	movq	%rsi, 32(%rsp)                  # 8-byte Spill
	leaq	480(%rsp), %rdi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 384(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 288(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 336(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	288(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	336(%rsp), %rsi
	movq	384(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	384(%rsp), %rdi
	leaq	560(%rsp), %rbx
	movq	16(%rsp), %rsi                  # 8-byte Reload
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 336(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 240(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 288(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	240(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	288(%rsp), %rsi
	movq	336(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	336(%rsp), %rdi
	leaq	480(%rsp), %rbx
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 288(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 144(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 240(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	144(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	240(%rsp), %rsi
	movq	288(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	16(%rsp), %rax                  # 8-byte Reload
	leaq	40(%rax), %rsi
	leaq	288(%rsp), %rdi
	leaq	560(%rsp), %rbx
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 240(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 192(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 144(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	192(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	144(%rsp), %rsi
	movq	240(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	40(%rsp), %rbx                  # 8-byte Reload
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 240(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 192(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 144(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	192(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	144(%rsp), %rsi
	movq	240(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	addq	$40, %rbp
	leaq	240(%rsp), %rdi
	leaq	480(%rsp), %rbx
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 144(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 432(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 192(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	432(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	192(%rsp), %rsi
	movq	144(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	32(%rsp), %rbx                  # 8-byte Reload
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 144(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 432(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 192(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	432(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	192(%rsp), %rsi
	movq	144(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movabsq	$4503599627370495, %r10         # imm = 0xFFFFFFFFFFFFF
	movabsq	$1125899906842620, %r14         # imm = 0x3FFFFFFFFFFFC
	movabsq	$4294968272, %r11               # imm = 0x1000003D0
	movdqa	.LCPI34_0(%rip), %xmm1          # xmm1 = [18014381329608892,18014398509481980]
	movdqa	.LCPI34_1(%rip), %xmm0          # xmm0 = [18014398509481980,18014398509481980]
	movq	%r14, %rdi
	subq	416(%rsp), %rdi
	movdqa	336(%rsp), %xmm5
	movdqa	352(%rsp), %xmm4
	psubq	384(%rsp), %xmm5
	paddq	%xmm1, %xmm5
	movdqa	%xmm5, 144(%rsp)
	psubq	400(%rsp), %xmm4
	paddq	%xmm0, %xmm4
	movdqa	%xmm4, 160(%rsp)
	addq	368(%rsp), %rdi
	movq	%rdi, 176(%rsp)
	subq	320(%rsp), %r14
	movdqa	240(%rsp), %xmm3
	movdqa	256(%rsp), %xmm2
	psubq	288(%rsp), %xmm3
	paddq	%xmm1, %xmm3
	movdqa	%xmm3, 192(%rsp)
	psubq	304(%rsp), %xmm2
	paddq	%xmm0, %xmm2
	movdqa	%xmm2, 208(%rsp)
	addq	272(%rsp), %r14
	movq	%r14, 224(%rsp)
	movq	%rdi, %rax
	shrq	$48, %rax
	leaq	1(%r11), %rcx
	imulq	%rax, %rcx
	movq	%xmm5, %rbx
	addq	%rcx, %rbx
	movq	%rbx, %rax
	andq	%r10, %rax
	movq	%rax, %rcx
	xorq	%r11, %rcx
	testq	%rax, %rax
	je	.LBB34_7
# %bb.6:
	cmpq	%r10, %rcx
	jne	.LBB34_15
.LBB34_7:                               # %secp256k1_fe_normalizes_to_zero_var.exit
	movabsq	$4222124650659840, %r8          # imm = 0xF000000000000
	movabsq	$281474976710655, %r9           # imm = 0xFFFFFFFFFFFF
	andq	%r9, %rdi
	shrq	$52, %rbx
	pshufd	$238, %xmm5, %xmm5              # xmm5 = xmm5[2,3,2,3]
	movq	%xmm5, %rdx
	addq	%rbx, %rdx
	movq	%rdx, %rbx
	shrq	$52, %rbx
	movq	%xmm4, %r15
	addq	%rbx, %r15
	movq	%rdx, %rsi
	andq	%r10, %rsi
	orq	%rax, %rsi
	movq	%r15, %rax
	shrq	$52, %rax
	pshufd	$238, %xmm4, %xmm4              # xmm4 = xmm4[2,3,2,3]
	movq	%xmm4, %rbx
	addq	%rax, %rbx
	movq	%r15, %rbp
	andq	%r10, %rbp
	orq	%rsi, %rbp
	movq	%rbx, %rax
	shrq	$52, %rax
	addq	%rdi, %rax
	movq	%rbx, %rsi
	andq	%r10, %rsi
	orq	%rbp, %rsi
	orq	%rax, %rsi
	je	.LBB34_9
# %bb.8:                                # %secp256k1_fe_normalizes_to_zero_var.exit
	andq	%rcx, %rdx
	andq	%r15, %rdx
	andq	%rbx, %rdx
	xorq	%r8, %rax
	andq	%rdx, %rax
	cmpq	%r10, %rax
	je	.LBB34_9
.LBB34_15:                              # %secp256k1_fe_normalizes_to_zero_var.exit.thread
	leaq	432(%rsp), %rdi
	leaq	192(%rsp), %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 520(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 56(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 96(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	56(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	96(%rsp), %rsi
	movq	520(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	520(%rsp), %rbp
	leaq	144(%rsp), %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 96(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, (%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 56(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	56(%rsp), %rsi
	movq	96(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	96(%rsp), %rdi
	movq	%rbp, %rbx
	leaq	144(%rsp), %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 56(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 8(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, (%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	8(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	(%rsp), %rsi
	movq	56(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	24(%rsp), %rbp                  # 8-byte Reload
	leaq	80(%rbp), %rdi
	movq	40(%rsp), %rbx                  # 8-byte Reload
	movq	32(%rsp), %rsi                  # 8-byte Reload
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 56(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 8(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, (%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	8(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	(%rsp), %rsi
	movq	56(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	144(%rsp), %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 56(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 8(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, (%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	8(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	(%rsp), %rsi
	movq	56(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	56(%rsp), %rdi
	leaq	520(%rsp), %rbx
	leaq	384(%rsp), %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, (%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 48(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 8(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	48(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rsp), %rsi
	movq	(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movups	56(%rsp), %xmm2
	movups	72(%rsp), %xmm3
	movups	%xmm2, (%rbp)
	movups	%xmm3, 16(%rbp)
	movq	88(%rsp), %rax
	movq	%rax, 32(%rbp)
	movdqu	(%rbp), %xmm2
	paddq	%xmm2, %xmm2
	movdqu	16(%rbp), %xmm3
	paddq	%xmm3, %xmm3
	addq	%rax, %rax
	paddq	96(%rsp), %xmm2
	paddq	112(%rsp), %xmm3
	addq	128(%rsp), %rax
	movdqa	432(%rsp), %xmm4
	psubq	%xmm2, %xmm4
	paddq	.LCPI34_2(%rip), %xmm4
	movdqu	%xmm4, (%rbp)
	movdqa	448(%rsp), %xmm2
	psubq	%xmm3, %xmm2
	paddq	.LCPI34_3(%rip), %xmm2
	movdqu	%xmm2, 16(%rbp)
	movq	464(%rsp), %rcx
	subq	%rax, %rcx
	movabsq	$2251799813685240, %rax         # imm = 0x7FFFFFFFFFFF8
	addq	%rcx, %rax
	movq	%rax, 32(%rbp)
	movq	%xmm4, %rcx
	pshufd	$238, %xmm4, %xmm3              # xmm3 = xmm4[2,3,2,3]
	movq	%xmm3, %rdx
	movabsq	$54043195528445940, %rsi        # imm = 0xBFFFFFFFFFFFF4
	movq	%rsi, %rdi
	subq	%rdx, %rdi
	movq	%xmm2, %rdx
	movq	%rsi, %rbx
	subq	%rdx, %rbx
	pshufd	$238, %xmm2, %xmm2              # xmm2 = xmm2[2,3,2,3]
	movq	56(%rsp), %rdx
	subq	%rcx, %rdx
	movabsq	$54043143988826676, %rcx        # imm = 0xBFFFF3FFFFD234
	addq	%rdx, %rcx
	movq	%rcx, 40(%rbp)
	addq	64(%rsp), %rdi
	movq	%xmm2, %rcx
	movq	%rdi, 48(%rbp)
	addq	72(%rsp), %rbx
	subq	%rcx, %rsi
	movq	%rbx, 56(%rbp)
	addq	80(%rsp), %rsi
	leaq	40(%rbp), %rdi
	movq	%rsi, 64(%rbp)
	movq	88(%rsp), %rcx
	subq	%rax, %rcx
	movabsq	$3377699720527860, %rax         # imm = 0xBFFFFFFFFFFF4
	addq	%rcx, %rax
	movq	%rax, 72(%rbp)
	leaq	192(%rsp), %rbx
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, (%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 48(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 8(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	48(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rsp), %rsi
	movq	(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	288(%rsp), %rbx
	leaq	96(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, (%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 48(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 8(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	48(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rsp), %rsi
	movq	(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	psubq	96(%rsp), %xmm1
	movdqa	%xmm1, 96(%rsp)
	psubq	112(%rsp), %xmm0
	movdqa	%xmm0, 112(%rsp)
	movabsq	$1125899906842620, %rax         # imm = 0x3FFFFFFFFFFFC
	subq	128(%rsp), %rax
	movq	%rax, 128(%rsp)
	movdqu	40(%rbp), %xmm2
	movdqu	56(%rbp), %xmm3
	paddq	%xmm1, %xmm2
	movdqu	%xmm2, 40(%rbp)
	paddq	%xmm0, %xmm3
	movdqu	%xmm3, 56(%rbp)
	addq	%rax, 72(%rbp)
	jmp	.LBB34_16
.LBB34_9:
	movq	%r14, %rax
	shrq	$48, %rax
	leaq	1(%r11), %rdx
	imulq	%rax, %rdx
	movq	%xmm3, %rcx
	addq	%rdx, %rcx
	movq	%rcx, %rax
	andq	%r10, %rax
	xorq	%rax, %r11
	testq	%rax, %rax
	je	.LBB34_11
# %bb.10:
	cmpq	%r10, %r11
	jne	.LBB34_14
.LBB34_11:                              # %secp256k1_fe_normalizes_to_zero_var.exit39
	andq	%r9, %r14
	shrq	$52, %rcx
	pshufd	$238, %xmm3, %xmm0              # xmm0 = xmm3[2,3,2,3]
	movq	%xmm0, %rsi
	addq	%rcx, %rsi
	movq	%rsi, %rcx
	shrq	$52, %rcx
	movq	%xmm2, %rdi
	addq	%rcx, %rdi
	movq	%rsi, %rcx
	andq	%r10, %rcx
	orq	%rax, %rcx
	movq	%rdi, %rdx
	shrq	$52, %rdx
	pshufd	$238, %xmm2, %xmm0              # xmm0 = xmm2[2,3,2,3]
	movq	%xmm0, %rbp
	addq	%rdx, %rbp
	movq	%rdi, %rdx
	andq	%r10, %rdx
	orq	%rcx, %rdx
	movq	%rbp, %rcx
	shrq	$52, %rcx
	addq	%r14, %rcx
	movq	%rbp, %rax
	andq	%r10, %rax
	orq	%rdx, %rax
	orq	%rcx, %rax
	je	.LBB34_13
# %bb.12:                               # %secp256k1_fe_normalizes_to_zero_var.exit39
	andq	%r11, %rsi
	andq	%rdi, %rsi
	andq	%rbp, %rsi
	xorq	%r8, %rcx
	andq	%rsi, %rcx
	cmpq	%r10, %rcx
	je	.LBB34_13
.LBB34_14:                              # %secp256k1_fe_normalizes_to_zero_var.exit39.thread
	movq	24(%rsp), %rax                  # 8-byte Reload
	movl	$1, 120(%rax)
	jmp	.LBB34_16
.LBB34_13:
	movq	24(%rsp), %rdi                  # 8-byte Reload
	movq	16(%rsp), %rsi                  # 8-byte Reload
	callq	secp256k1_gej_double_var
	jmp	.LBB34_16
.Lfunc_end34:
	.size	secp256k1_gej_add_var, .Lfunc_end34-secp256k1_gej_add_var
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function secp256k1_gej_double_var
.LCPI35_0:
	.quad	27021571994413338               # 0x5ffff9ffffe91a
	.quad	27021597764222970               # 0x5ffffffffffffa
.LCPI35_1:
	.quad	27021597764222970               # 0x5ffffffffffffa
	.quad	27021597764222970               # 0x5ffffffffffffa
	.text
	.p2align	4, 0x90
	.type	secp256k1_gej_double_var,@function
secp256k1_gej_double_var:               # @secp256k1_gej_double_var
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$88, %rsp
	.cfi_def_cfa_offset 144
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	120(%rsi), %eax
	movl	%eax, 120(%rdi)
	testl	%eax, %eax
	jne	.LBB35_2
# %bb.1:
	movq	%rdi, %rbp
	addq	$80, %rdi
	movq	%rsi, %rax
	movq	%rsi, -64(%rsp)                 # 8-byte Spill
	addq	$80, %rsi
	leaq	40(%rax), %rbx
	movq	%rbx, 80(%rsp)                  # 8-byte Spill
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -48(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -128(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -8(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-128(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-8(%rsp), %rsi
	movq	-48(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movdqu	80(%rbp), %xmm0
	movdqu	96(%rbp), %xmm1
	paddq	%xmm0, %xmm0
	movdqu	%xmm0, 80(%rbp)
	paddq	%xmm1, %xmm1
	movdqu	%xmm1, 96(%rbp)
	shlq	112(%rbp)
	leaq	-48(%rsp), %rdi
	movq	-64(%rsp), %rsi                 # 8-byte Reload
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -8(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 32(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -128(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	32(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-128(%rsp), %rsi
	movq	-8(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	-48(%rsp), %rax
	movq	-40(%rsp), %rcx
	leaq	(%rax,%rax,2), %rax
	movq	%rax, -48(%rsp)
	leaq	(%rcx,%rcx,2), %rax
	movq	%rax, -40(%rsp)
	movq	-32(%rsp), %rax
	leaq	(%rax,%rax,2), %rax
	movq	%rax, -32(%rsp)
	movq	-24(%rsp), %rax
	leaq	(%rax,%rax,2), %rax
	movq	%rax, -24(%rsp)
	movq	-16(%rsp), %rax
	leaq	(%rax,%rax,2), %rax
	movq	%rax, -16(%rsp)
	leaq	-8(%rsp), %rdi
	leaq	-48(%rsp), %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -128(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -80(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 32(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-80(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	32(%rsp), %rsi
	movq	-128(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	-128(%rsp), %rdi
	movq	80(%rsp), %rsi                  # 8-byte Reload
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 32(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -72(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -80(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-72(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-80(%rsp), %rsi
	movq	32(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movdqa	-128(%rsp), %xmm0
	movdqa	-112(%rsp), %xmm1
	paddq	%xmm0, %xmm0
	movdqa	%xmm0, -128(%rsp)
	paddq	%xmm1, %xmm1
	movdqa	%xmm1, -112(%rsp)
	shlq	-96(%rsp)
	leaq	32(%rsp), %rdi
	leaq	-128(%rsp), %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -80(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -56(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -72(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-56(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-72(%rsp), %rsi
	movq	-80(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movdqa	32(%rsp), %xmm0
	movdqa	48(%rsp), %xmm1
	paddq	%xmm0, %xmm0
	movdqa	%xmm0, 32(%rsp)
	paddq	%xmm1, %xmm1
	movdqa	%xmm1, 48(%rsp)
	shlq	64(%rsp)
	movq	-64(%rsp), %rbx                 # 8-byte Reload
	leaq	-128(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -80(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -56(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -72(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-56(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-72(%rsp), %rsi
	movq	-80(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movaps	-128(%rsp), %xmm0
	movaps	-112(%rsp), %xmm1
	movups	%xmm0, (%rbp)
	movups	%xmm1, 16(%rbp)
	movq	-96(%rsp), %r14
	movq	%r14, 32(%rbp)
	movq	(%rbp), %rdi
	movq	8(%rbp), %r10
	shlq	$2, %rdi
	shlq	$2, %r10
	movq	16(%rbp), %r11
	movq	24(%rbp), %r8
	shlq	$2, %r11
	shlq	$2, %r8
	shlq	$2, %r14
	movq	-8(%rsp), %r9
	movabsq	$18014381329608892, %r15        # imm = 0x3FFFFBFFFFF0BC
	subq	%r9, %r15
	subq	%rdi, %r9
	movabsq	$45035953324022230, %rdi        # imm = 0x9FFFF5FFFFD9D6
	addq	%r9, %rdi
	movq	%rdi, (%rbp)
	movq	(%rsp), %rdi
	movabsq	$18014398509481980, %rdx        # imm = 0x3FFFFFFFFFFFFC
	movq	%rdx, %rsi
	subq	%rdi, %rsi
	subq	%r10, %rdi
	movabsq	$45035996273704950, %rax        # imm = 0x9FFFFFFFFFFFF6
	addq	%rax, %rdi
	movq	%rdi, 8(%rbp)
	movq	8(%rsp), %rdi
	movq	%rdx, %rbx
	subq	%rdi, %rbx
	subq	%r11, %rdi
	addq	%rax, %rdi
	movq	%rdi, 16(%rbp)
	movq	16(%rsp), %rdi
	subq	%rdi, %rdx
	subq	%r8, %rdi
	addq	%rax, %rdi
	movq	%rdi, 24(%rbp)
	movq	24(%rsp), %rax
	movabsq	$1125899906842620, %rdi         # imm = 0x3FFFFFFFFFFFC
	subq	%rax, %rdi
	subq	%r14, %rax
	movabsq	$2814749767106550, %rcx         # imm = 0x9FFFFFFFFFFF6
	addq	%rax, %rcx
	movq	%rcx, 32(%rbp)
	movq	%r15, -8(%rsp)
	movq	%rsi, (%rsp)
	movq	%rbx, 8(%rsp)
	movq	%rdx, 16(%rsp)
	movq	%rdi, 24(%rsp)
	movq	-128(%rsp), %rax
	movq	-120(%rsp), %rcx
	leaq	(%rax,%rax,2), %rax
	leaq	(%rcx,%rcx,2), %r8
	movq	-112(%rsp), %rcx
	leaq	(%rcx,%rcx,2), %r9
	movq	-104(%rsp), %rcx
	leaq	(%rcx,%rcx,2), %r10
	movq	-96(%rsp), %rcx
	leaq	(%rcx,%rcx,2), %rcx
	leaq	(%r15,%rax,2), %rax
	movq	%rax, -128(%rsp)
	leaq	(%rsi,%r8,2), %rax
	movq	%rax, -120(%rsp)
	leaq	(%rbx,%r9,2), %rax
	movq	%rax, -112(%rsp)
	leaq	(%rdx,%r10,2), %rax
	movq	%rax, -104(%rsp)
	leaq	(%rdi,%rcx,2), %rax
	movq	%rax, -96(%rsp)
	leaq	40(%rbp), %rdi
	leaq	-128(%rsp), %rbx
	leaq	-48(%rsp), %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -80(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -56(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -72(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-56(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-72(%rsp), %rsi
	movq	-80(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movdqu	40(%rbp), %xmm0
	movdqu	56(%rbp), %xmm1
	psubq	32(%rsp), %xmm0
	paddq	.LCPI35_0(%rip), %xmm0
	psubq	48(%rsp), %xmm1
	movq	72(%rbp), %rax
	subq	64(%rsp), %rax
	movdqu	%xmm0, 40(%rbp)
	paddq	.LCPI35_1(%rip), %xmm1
	movdqu	%xmm1, 56(%rbp)
	movabsq	$1688849860263930, %rcx         # imm = 0x5FFFFFFFFFFFA
	addq	%rax, %rcx
	movq	%rcx, 72(%rbp)
.LBB35_2:
	addq	$88, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end35:
	.size	secp256k1_gej_double_var, .Lfunc_end35-secp256k1_gej_double_var
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function secp256k1_ge_set_all_gej_var
	.type	secp256k1_ge_set_all_gej_var,@function
secp256k1_ge_set_all_gej_var:           # @secp256k1_ge_set_all_gej_var
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$168, %rsp
	.cfi_def_cfa_offset 224
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rdx, %rbp
	movq	%rsi, %rbx
	movq	%rdi, %r14
	leaq	(,%rdi,8), %rax
	leaq	(%rax,%rax,4), %rdi
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB36_29
# %bb.1:                                # %checked_malloc.exit
	movq	%rax, %r15
	testq	%r14, %r14
	movq	%rbp, 32(%rsp)                  # 8-byte Spill
	je	.LBB36_2
# %bb.3:                                # %.lr.ph.preheader
	cmpq	$1, %r14
	jne	.LBB36_5
# %bb.4:
	xorl	%eax, %eax
	xorl	%r13d, %r13d
.LBB36_11:                              # %._crit_edge.loopexit.unr-lcssa
	testb	$1, %r14b
	je	.LBB36_14
# %bb.12:                               # %.lr.ph.epil
	shlq	$7, %rax
	movq	32(%rsp), %rsi                  # 8-byte Reload
	cmpl	$0, 120(%rsi,%rax)
	jne	.LBB36_14
# %bb.13:
	leaq	(,%r13,4), %rcx
	addq	%r13, %rcx
	addq	$1, %r13
	movq	112(%rsi,%rax), %rdx
	movq	%rdx, 32(%r15,%rcx,8)
	movups	80(%rsi,%rax), %xmm0
	movups	96(%rsi,%rax), %xmm1
	movups	%xmm1, 16(%r15,%rcx,8)
	movups	%xmm0, (%r15,%rcx,8)
	jmp	.LBB36_14
.LBB36_2:
	xorl	%r13d, %r13d
.LBB36_14:                              # %._crit_edge
	leaq	(,%r13,8), %rax
	leaq	(%rax,%rax,4), %rdi
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB36_29
# %bb.15:                               # %checked_malloc.exit50
	movq	%rax, %r12
	testq	%r13, %r13
	movq	%rax, 48(%rsp)                  # 8-byte Spill
	je	.LBB36_23
# %bb.16:
	movq	32(%r15), %rax
	movq	%rax, 32(%r12)
	movups	(%r15), %xmm0
	movups	16(%r15), %xmm1
	movups	%xmm1, 16(%r12)
	movups	%xmm0, (%r12)
	cmpq	$1, %r13
	movq	%rbx, 8(%rsp)                   # 8-byte Spill
	movq	%r14, 40(%rsp)                  # 8-byte Spill
	movq	%r15, 64(%rsp)                  # 8-byte Spill
	jne	.LBB36_18
# %bb.17:                               # %._crit_edge.thread.i
	leaq	80(%rsp), %rdi
	movq	%r12, %rsi
	callq	secp256k1_fe_inv_var
	jmp	.LBB36_22
.LBB36_5:                               # %.lr.ph.preheader.new
	movq	%r14, %rcx
	andq	$-2, %rcx
	leaq	248(%rbp), %rdx
	xorl	%eax, %eax
	xorl	%r13d, %r13d
	jmp	.LBB36_6
	.p2align	4, 0x90
.LBB36_10:                              #   in Loop: Header=BB36_6 Depth=1
	addq	$2, %rax
	addq	$256, %rdx                      # imm = 0x100
	cmpq	%rax, %rcx
	je	.LBB36_11
.LBB36_6:                               # %.lr.ph
                                        # =>This Inner Loop Header: Depth=1
	cmpl	$0, -128(%rdx)
	je	.LBB36_7
# %bb.8:                                # %.lr.ph.1
                                        #   in Loop: Header=BB36_6 Depth=1
	cmpl	$0, (%rdx)
	jne	.LBB36_10
	jmp	.LBB36_9
	.p2align	4, 0x90
.LBB36_7:                               #   in Loop: Header=BB36_6 Depth=1
	leaq	(,%r13,4), %rsi
	addq	%r13, %rsi
	addq	$1, %r13
	movq	-136(%rdx), %rdi
	movq	%rdi, 32(%r15,%rsi,8)
	movups	-168(%rdx), %xmm0
	movups	-152(%rdx), %xmm1
	movups	%xmm1, 16(%r15,%rsi,8)
	movups	%xmm0, (%r15,%rsi,8)
	cmpl	$0, (%rdx)
	jne	.LBB36_10
.LBB36_9:                               #   in Loop: Header=BB36_6 Depth=1
	leaq	(,%r13,4), %rsi
	addq	%r13, %rsi
	addq	$1, %r13
	movq	-8(%rdx), %rdi
	movq	%rdi, 32(%r15,%rsi,8)
	movups	-40(%rdx), %xmm0
	movups	-24(%rdx), %xmm1
	movups	%xmm1, 16(%r15,%rsi,8)
	movups	%xmm0, (%r15,%rsi,8)
	jmp	.LBB36_10
.LBB36_18:                              # %.lr.ph.i
	addq	$-1, %r13
	leaq	40(%r15), %rbx
	xorl	%r14d, %r14d
	xorl	%ebp, %ebp
	movq	%r12, %r15
	movq	%r13, 56(%rsp)                  # 8-byte Spill
	.p2align	4, 0x90
.LBB36_19:                              # =>This Inner Loop Header: Depth=1
	movq	%r14, 16(%rsp)                  # 8-byte Spill
	leaq	40(%r15), %rdi
	movq	%r15, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 80(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, (%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 128(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	128(%rsp), %rsi
	movq	80(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rdi, %r15
	movq	16(%rsp), %r14                  # 8-byte Reload
	movq	56(%rsp), %r13                  # 8-byte Reload
	addq	$1, %rbp
	movabsq	$4294967296, %rax               # imm = 0x100000000
	addq	%rax, %r14
	addq	$40, %rbx
	cmpq	%rbp, %r13
	jne	.LBB36_19
# %bb.20:                               # %.lr.ph38.i
	leaq	80(%rsp), %rdi
	movq	%r15, %rsi
	callq	secp256k1_fe_inv_var
	addq	$-40, %r15
	negq	%rbp
	movq	48(%rsp), %r12                  # 8-byte Reload
	.p2align	4, 0x90
.LBB36_21:                              # =>This Inner Loop Header: Depth=1
	movq	%r15, 120(%rsp)                 # 8-byte Spill
	movq	%rbp, 56(%rsp)                  # 8-byte Spill
	movq	%r14, 16(%rsp)                  # 8-byte Spill
	movq	%r14, %rax
	sarq	$29, %rax
	leaq	(%rax,%rax,4), %rbp
	leaq	(%r12,%rbp), %rdi
	leaq	80(%rsp), %rbx
	movq	%r15, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 128(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 24(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, (%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	24(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	(%rsp), %rsi
	movq	128(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	addq	64(%rsp), %rbp                  # 8-byte Folded Reload
	movq	%rbp, %rbx
	leaq	80(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 128(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 24(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, (%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	24(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	(%rsp), %rsi
	movq	128(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	120(%rsp), %r15                 # 8-byte Reload
	movq	56(%rsp), %rbp                  # 8-byte Reload
	movq	16(%rsp), %r14                  # 8-byte Reload
	movq	48(%rsp), %r12                  # 8-byte Reload
	movabsq	$-4294967296, %rax              # imm = 0xFFFFFFFF00000000
	addq	%rax, %r14
	addq	$-40, %r15
	incq	%rbp
	jne	.LBB36_21
.LBB36_22:                              # %._crit_edge39.i
	movq	112(%rsp), %rax
	movq	%rax, 32(%r12)
	movups	80(%rsp), %xmm0
	movups	96(%rsp), %xmm1
	movups	%xmm1, 16(%r12)
	movups	%xmm0, (%r12)
	movq	8(%rsp), %rbx                   # 8-byte Reload
	movq	40(%rsp), %r14                  # 8-byte Reload
	movq	64(%rsp), %r15                  # 8-byte Reload
.LBB36_23:                              # %secp256k1_fe_inv_all_var.exit
	movq	%r15, %rdi
	callq	free@PLT
	testq	%r14, %r14
	movq	32(%rsp), %rbp                  # 8-byte Reload
	je	.LBB36_28
# %bb.24:                               # %.lr.ph58
	addq	$40, %rbx
	xorl	%ecx, %ecx
	jmp	.LBB36_25
	.p2align	4, 0x90
.LBB36_27:                              #   in Loop: Header=BB36_25 Depth=1
	subq	$-128, %rbp
	addq	$88, %rbx
	addq	$-1, %r14
	je	.LBB36_28
.LBB36_25:                              # =>This Inner Loop Header: Depth=1
	movl	120(%rbp), %eax
	movl	%eax, 40(%rbx)
	testl	%eax, %eax
	jne	.LBB36_27
# %bb.26:                               #   in Loop: Header=BB36_25 Depth=1
	leaq	(%rcx,%rcx,4), %rax
	movq	%rbp, 32(%rsp)                  # 8-byte Spill
	leaq	(%r12,%rax,8), %rbp
	leaq	80(%rsp), %rdi
	movq	%rbp, %rsi
	movq	%rbx, 8(%rsp)                   # 8-byte Spill
	movq	%r14, 40(%rsp)                  # 8-byte Spill
	movq	%rcx, 16(%rsp)                  # 8-byte Spill
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 128(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 24(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, (%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	24(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	(%rsp), %rsi
	movq	128(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	%rbp, %rbx
	movq	32(%rsp), %rbp                  # 8-byte Reload
	leaq	128(%rsp), %rdi
	leaq	80(%rsp), %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, (%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 72(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 24(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	72(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	24(%rsp), %rsi
	movq	(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	8(%rsp), %rax                   # 8-byte Reload
	leaq	-40(%rax), %rdi
	leaq	80(%rsp), %rbx
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, (%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 72(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 24(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	72(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	24(%rsp), %rsi
	movq	(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	40(%rbp), %rsi
	leaq	128(%rsp), %rbx
	movq	8(%rsp), %rdi                   # 8-byte Reload
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, (%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 72(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 24(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	72(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	24(%rsp), %rsi
	movq	(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	48(%rsp), %r12                  # 8-byte Reload
	movq	40(%rsp), %r14                  # 8-byte Reload
	movq	8(%rsp), %rbx                   # 8-byte Reload
	movq	16(%rsp), %rax                  # 8-byte Reload
	leaq	1(%rax), %rcx
	jmp	.LBB36_27
.LBB36_28:                              # %._crit_edge59
	movq	%r12, %rdi
	callq	free@PLT
	addq	$168, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB36_29:
	.cfi_def_cfa_offset 224
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str(%rip), %rsi
	leaq	.L.str.76(%rip), %rdx
	leaq	.L.str.77(%rip), %r8
	movl	$66, %ecx
	xorl	%eax, %eax
	callq	fprintf@PLT
	callq	abort@PLT
.Lfunc_end36:
	.size	secp256k1_ge_set_all_gej_var, .Lfunc_end36-secp256k1_ge_set_all_gej_var
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function secp256k1_fe_inv_var
	.type	secp256k1_fe_inv_var,@function
secp256k1_fe_inv_var:                   # @secp256k1_fe_inv_var
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$376, %rsp                      # imm = 0x178
	.cfi_def_cfa_offset 432
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movabsq	$4503599627370495, %r13         # imm = 0xFFFFFFFFFFFFF
	movabsq	$4294968273, %r9                # imm = 0x1000003D1
	movq	32(%rsi), %r10
	movq	%r10, %r11
	shrq	$48, %r11
	imulq	%r9, %r11
	addq	(%rsi), %r11
	movabsq	$281474976710655, %r8           # imm = 0xFFFFFFFFFFFF
	movq	%r11, %rcx
	shrq	$52, %rcx
	addq	8(%rsi), %rcx
	andq	%r8, %r10
	movq	%rcx, %rbp
	shrq	$52, %rbp
	addq	16(%rsi), %rbp
	andq	%r13, %r11
	movq	%rbp, %rdx
	shrq	$52, %rdx
	addq	24(%rsi), %rdx
	andq	%r13, %rbp
	movq	%rbp, %rbx
	andq	%rcx, %rbx
	andq	%r13, %rcx
	movq	%rdx, %rax
	shrq	$52, %rax
	addq	%r10, %rax
	andq	%rdx, %rbx
	andq	%r13, %rdx
	movq	%rax, %r10
	shrq	$48, %r10
	movq	%rax, %rsi
	xorq	%r8, %rsi
	xorq	%r13, %rbx
	orq	%rsi, %rbx
	sete	%r15b
	movabsq	$4503595332402222, %rsi         # imm = 0xFFFFEFFFFFC2E
	cmpq	%rsi, %r11
	seta	%bl
	movq	%rdi, %r14
	andb	%r15b, %bl
	movzbl	%bl, %esi
	orq	%r10, %rsi
	je	.LBB37_2
# %bb.1:
	addq	%r9, %r11
	movq	%r11, %rsi
	shrq	$52, %rsi
	addq	%rsi, %rcx
	andq	%r13, %r11
	movq	%rcx, %rsi
	shrq	$52, %rsi
	addq	%rsi, %rbp
	andq	%r13, %rcx
	movq	%rbp, %rsi
	shrq	$52, %rsi
	addq	%rsi, %rdx
	andq	%r13, %rbp
	movq	%rdx, %rsi
	shrq	$52, %rsi
	addq	%rax, %rsi
	andq	%r13, %rdx
	andq	%r8, %rsi
	movq	%rsi, %rax
.LBB37_2:                               # %secp256k1_fe_normalize_var.exit
	shlq	$16, %rax
	movq	%rdx, %rsi
	shrq	$36, %rsi
	orq	%rax, %rsi
	bswapq	%rsi
	movq	%rsi, 16(%rsp)
	shlq	$28, %rdx
	movq	%rbp, %rax
	shrq	$24, %rax
	orq	%rdx, %rax
	bswapq	%rax
	movq	%rax, 24(%rsp)
	shlq	$40, %rbp
	movq	%rcx, %rax
	shrq	$12, %rax
	orq	%rbp, %rax
	bswapq	%rax
	movq	%rax, 32(%rsp)
	shlq	$52, %rcx
	orq	%r11, %rcx
	bswapq	%rcx
	movq	%rcx, 40(%rsp)
	leaq	64(%rsp), %rdi
	leaq	16(%rsp), %rsi
	movl	$32, %edx
	movl	$256, %ecx                      # imm = 0x100
	callq	__gmpn_set_str@PLT
	testl	%eax, %eax
	je	.LBB37_7
# %bb.3:
	movl	%eax, 132(%rsp)
	movl	$0, 128(%rsp)
	cmpl	$2, %eax
	jl	.LBB37_8
# %bb.4:                                # %.lr.ph.preheader.i
	movl	%eax, %ecx
	addq	$1, %rcx
	addl	$-1, %eax
	.p2align	4, 0x90
.LBB37_5:                               # %.lr.ph.i
                                        # =>This Inner Loop Header: Depth=1
	leal	-2(%rcx), %edx
	cmpq	$0, 64(%rsp,%rdx,8)
	jne	.LBB37_8
# %bb.6:                                #   in Loop: Header=BB37_5 Depth=1
	movl	%eax, 132(%rsp)
	addq	$-1, %rcx
	addl	$-1, %eax
	cmpq	$2, %rcx
	ja	.LBB37_5
	jmp	.LBB37_8
.LBB37_7:                               # %.thread.i
	movq	$0, 64(%rsp)
	movabsq	$4294967296, %rax               # imm = 0x100000000
	movq	%rax, 128(%rsp)
.LBB37_8:                               # %secp256k1_num_set_bin.exit
	leaq	secp256k1_fe_inv_var.prime(%rip), %rsi
	leaq	136(%rsp), %rdi
	movl	$32, %edx
	movl	$256, %ecx                      # imm = 0x100
	callq	__gmpn_set_str@PLT
	movq	%rax, %r15
	testl	%r15d, %r15d
	je	.LBB37_14
# %bb.9:
	movl	%r15d, 204(%rsp)
	movl	$0, 200(%rsp)
	cmpl	$2, %r15d
	jl	.LBB37_17
# %bb.10:                               # %.lr.ph.preheader.i3
	leal	-1(%r15), %eax
	cmpq	$0, 136(%rsp,%rax,8)
	jne	.LBB37_17
# %bb.11:
	movl	%r15d, %r15d
	.p2align	4, 0x90
.LBB37_12:                              # %.lr.ph
                                        # =>This Inner Loop Header: Depth=1
	cmpq	$2, %r15
	jbe	.LBB37_15
# %bb.13:                               # %.lr.ph.i6
                                        #   in Loop: Header=BB37_12 Depth=1
	leal	-2(%r15), %eax
	addq	$-1, %r15
	cmpq	$0, 136(%rsp,%rax,8)
	je	.LBB37_12
	jmp	.LBB37_16
.LBB37_14:                              # %secp256k1_num_set_bin.exit8.thread
	movq	$0, 136(%rsp)
	movabsq	$4294967296, %rax               # imm = 0x100000000
	movq	%rax, 200(%rsp)
	movl	$1, %r15d
	jmp	.LBB37_18
.LBB37_15:                              # %secp256k1_num_set_bin.exit8.loopexit.split.loop.exit
	addl	$-1, %r15d
.LBB37_16:                              # %secp256k1_num_set_bin.exit8.loopexit
	movl	%r15d, 204(%rsp)
.LBB37_17:                              # %secp256k1_num_set_bin.exit8
	testl	%r15d, %r15d
	jle	.LBB37_31
.LBB37_18:                              # %.lr.ph.i9
	movslq	132(%rsp), %rbx
	movl	%r15d, %r12d
	leaq	(,%r12,8), %rdx
	leaq	336(%rsp), %rdi
	leaq	136(%rsp), %rsi
	callq	memcpy@PLT
	cmpl	$1, %r15d
	jne	.LBB37_20
# %bb.19:
	xorl	%eax, %eax
	testb	$1, %r12b
	jne	.LBB37_27
	jmp	.LBB37_31
.LBB37_20:                              # %.lr.ph.i9.new
	movl	%r12d, %ecx
	andl	$-2, %ecx
	xorl	%edx, %edx
	jmp	.LBB37_22
	.p2align	4, 0x90
.LBB37_21:                              #   in Loop: Header=BB37_22 Depth=1
	movq	72(%rsp,%rax,8), %rsi
	movq	%rsi, 216(%rsp,%rax,8)
	addq	$1, %rdx
	cmpq	%rcx, %rdx
	je	.LBB37_26
.LBB37_22:                              # =>This Inner Loop Header: Depth=1
	movq	%rdx, %rax
	cmpq	%rbx, %rdx
	jge	.LBB37_24
# %bb.23:                               #   in Loop: Header=BB37_22 Depth=1
	movq	64(%rsp,%rax,8), %rdx
	movq	%rdx, 208(%rsp,%rax,8)
	leaq	1(%rax), %rdx
	cmpq	%rbx, %rdx
	jl	.LBB37_21
	jmp	.LBB37_25
	.p2align	4, 0x90
.LBB37_24:                              #   in Loop: Header=BB37_22 Depth=1
	xorl	%edx, %edx
	movq	%rdx, 208(%rsp,%rax,8)
	leaq	1(%rax), %rdx
	cmpq	%rbx, %rdx
	jl	.LBB37_21
.LBB37_25:                              #   in Loop: Header=BB37_22 Depth=1
	xorl	%esi, %esi
	movq	%rsi, 216(%rsp,%rax,8)
	addq	$1, %rdx
	cmpq	%rcx, %rdx
	jne	.LBB37_22
.LBB37_26:
	addq	$2, %rax
	testb	$1, %r12b
	je	.LBB37_31
.LBB37_27:                              # %.epil.preheader
	cmpq	%rbx, %rax
	jge	.LBB37_29
# %bb.28:
	movq	64(%rsp,%rax,8), %rcx
	jmp	.LBB37_30
.LBB37_29:
	xorl	%ecx, %ecx
.LBB37_30:                              # %._crit_edge.i.loopexit.epilog-lcssa
	movq	%rcx, 208(%rsp,%rax,8)
.LBB37_31:                              # %._crit_edge.i
	movq	$5, 56(%rsp)
	movslq	%r15d, %r8
	movq	%r8, (%rsp)
	leaq	256(%rsp), %rdi
	leaq	64(%rsp), %rsi
	leaq	56(%rsp), %rdx
	leaq	208(%rsp), %rcx
	leaq	336(%rsp), %r9
	callq	__gmpn_gcdext@PLT
	movl	200(%rsp), %eax
	xorl	%eax, 128(%rsp)
	movq	56(%rsp), %rbx
	testq	%rbx, %rbx
	jns	.LBB37_42
# %bb.32:
	movslq	204(%rsp), %r15
	negq	%rbx
	leaq	136(%rsp), %rsi
	leaq	64(%rsp), %rdi
	movq	%rdi, %rdx
	movq	%rbx, %rcx
	callq	__gmpn_sub_n@PLT
	testq	%rax, %rax
	je	.LBB37_54
# %bb.33:                               # %.preheader40.i.preheader.i
	cmpq	%r15, %rbx
	movq	%r15, %rcx
	cmovgq	%rbx, %rcx
	.p2align	4, 0x90
.LBB37_34:                              # %.preheader40.i.i
                                        # =>This Inner Loop Header: Depth=1
	cmpq	%rbx, %rcx
	je	.LBB37_38
# %bb.35:                               #   in Loop: Header=BB37_34 Depth=1
	movq	136(%rsp,%rbx,8), %rdx
	leaq	1(%rbx), %rax
	subq	$1, %rdx
	movq	%rdx, 64(%rsp,%rbx,8)
	movq	%rax, %rbx
	jb	.LBB37_34
# %bb.36:                               # %.loopexit42.i.i
	cmpq	%r15, %rax
	jge	.LBB37_38
.LBB37_37:                              # %.lr.ph.i.i.preheader
	leaq	(%rsp,%rax,8), %rdi
	addq	$64, %rdi
	leaq	(%rsp,%rax,8), %rsi
	addq	$136, %rsi
	subq	%rax, %r15
	shlq	$3, %r15
	movq	%r15, %rdx
	callq	memcpy@PLT
.LBB37_38:                              # %__gmpn_sub.exit.i
	movl	204(%rsp), %eax
	testl	%eax, %eax
	movl	$1, %ebx
	cmovlel	%eax, %ebx
	.p2align	4, 0x90
.LBB37_39:                              # =>This Inner Loop Header: Depth=1
	cmpl	$2, %eax
	jl	.LBB37_42
# %bb.40:                               #   in Loop: Header=BB37_39 Depth=1
	leaq	-1(%rax), %rcx
	cmpq	$0, 56(%rsp,%rax,8)
	movq	%rcx, %rax
	je	.LBB37_39
# %bb.41:                               # %.critedge.loopexit.i.split.loop.exit46
	addl	$1, %ecx
	movl	%ecx, %ebx
.LBB37_42:                              # %.critedge.loopexit.i
	movl	%ebx, 132(%rsp)
	cmpl	$1, %ebx
	jg	.LBB37_45
# %bb.43:                               # %secp256k1_num_mod_inverse.exit
	cmpq	$0, 64(%rsp)
	jne	.LBB37_45
# %bb.44:
	xorl	%r15d, %r15d
	jmp	.LBB37_50
.LBB37_45:
	movslq	%ebx, %rcx
	leaq	256(%rsp), %rdi
	leaq	64(%rsp), %rdx
	movl	$256, %esi                      # imm = 0x100
	callq	__gmpn_get_str@PLT
	movq	%rax, %r15
	testl	%r15d, %r15d
	jle	.LBB37_50
# %bb.46:                               # %.lr.ph.preheader.i13
	movl	%r15d, %eax
	movq	%r15, %rbx
	shlq	$32, %rbx
	xorl	%ebp, %ebp
	movabsq	$-4294967296, %rcx              # imm = 0xFFFFFFFF00000000
	.p2align	4, 0x90
.LBB37_47:                              # %.lr.ph.i15
                                        # =>This Inner Loop Header: Depth=1
	cmpb	$0, 256(%rsp,%rbp)
	jne	.LBB37_53
# %bb.48:                               #   in Loop: Header=BB37_47 Depth=1
	addq	$1, %rbp
	addq	%rcx, %rbx
	cmpq	%rbp, %rax
	jne	.LBB37_47
# %bb.49:
	movq	%r15, %rdx
	jmp	.LBB37_51
.LBB37_50:
	xorl	%edx, %edx
.LBB37_51:                              # %.critedge1.i
	subl	%r15d, %edx
	addl	$32, %edx
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	callq	memset@PLT
.LBB37_52:                              # %secp256k1_num_get_bin.exit
	movq	16(%rsp), %rax
	movq	24(%rsp), %rcx
	bswapq	%rax
	bswapq	%rcx
	movq	32(%rsp), %rdx
	bswapq	%rdx
	movq	40(%rsp), %rsi
	bswapq	%rsi
	movq	%rsi, %rdi
	andq	%r13, %rdi
	movq	%rdi, (%r14)
	shrq	$52, %rsi
	movq	%rdx, %rdi
	shlq	$12, %rdi
	leaq	-4095(%r13), %rbp
	andq	%rdi, %rbp
	orq	%rsi, %rbp
	movq	%rbp, 8(%r14)
	shrq	$40, %rdx
	movq	%rax, %rsi
	shldq	$36, %rcx, %rsi
	shlq	$24, %rcx
	leaq	-16777215(%r13), %rdi
	andq	%rcx, %rdi
	orq	%rdx, %rdi
	movq	%rdi, 16(%r14)
	andq	%r13, %rsi
	movq	%rsi, 24(%r14)
	shrq	$16, %rax
	movq	%rax, 32(%r14)
	addq	$376, %rsp                      # imm = 0x178
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB37_53:
	.cfi_def_cfa_offset 432
	leaq	(%rsp,%rbp), %r12
	addq	$256, %r12                      # imm = 0x100
	movl	%ebp, %edx
	subl	%r15d, %edx
	addl	$32, %edx
	leaq	16(%rsp), %rdi
	xorl	%esi, %esi
	callq	memset@PLT
	movslq	%r15d, %rax
	negq	%rax
	addq	%rsp, %rax
	addq	$16, %rax
	leaq	(%rax,%rbp), %rdi
	addq	$32, %rdi
	sarq	$32, %rbx
	movq	%r12, %rsi
	movq	%rbx, %rdx
	callq	memcpy@PLT
	jmp	.LBB37_52
.LBB37_54:
	movq	%rbx, %rax
	cmpq	%r15, %rax
	jl	.LBB37_37
	jmp	.LBB37_38
.Lfunc_end37:
	.size	secp256k1_fe_inv_var, .Lfunc_end37-secp256k1_fe_inv_var
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function secp256k1_ecmult
	.type	secp256k1_ecmult,@function
secp256k1_ecmult:                       # @secp256k1_ecmult
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$3336, %rsp                     # imm = 0xD08
	.cfi_def_cfa_offset 3392
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r8, %r15
	movq	%rdx, %rbp
	movq	%rsi, %r14
	movq	%rdi, 104(%rsp)                 # 8-byte Spill
	leaq	2304(%rsp), %rdi
	movq	%rcx, %rsi
	movl	$5, %edx
	callq	secp256k1_ecmult_wnaf
	movl	%eax, %r12d
	movups	(%rbp), %xmm0
	movups	16(%rbp), %xmm1
	movups	32(%rbp), %xmm2
	movups	48(%rbp), %xmm3
	movaps	%xmm0, 240(%rsp)
	movaps	%xmm1, 256(%rsp)
	movaps	%xmm2, 272(%rsp)
	movaps	%xmm3, 288(%rsp)
	movups	64(%rbp), %xmm0
	movaps	%xmm0, 304(%rsp)
	movups	80(%rbp), %xmm0
	movaps	%xmm0, 320(%rsp)
	movups	96(%rbp), %xmm0
	movaps	%xmm0, 336(%rsp)
	movups	112(%rbp), %xmm0
	movaps	%xmm0, 352(%rsp)
	leaq	1264(%rsp), %rbp
	leaq	240(%rsp), %r13
	movq	%rbp, %rdi
	movq	%r13, %rsi
	callq	secp256k1_gej_double_var
	leaq	368(%rsp), %rbx
	movq	%rbx, %rdi
	movq	%rbp, %rsi
	movq	%r13, %rdx
	callq	secp256k1_gej_add_var
	leaq	496(%rsp), %r13
	movq	%r13, %rdi
	movq	%rbp, %rsi
	movq	%rbx, %rdx
	callq	secp256k1_gej_add_var
	leaq	624(%rsp), %rbx
	movq	%rbx, %rdi
	movq	%rbp, %rsi
	movq	%r13, %rdx
	callq	secp256k1_gej_add_var
	leaq	752(%rsp), %r13
	movq	%r13, %rdi
	movq	%rbp, %rsi
	movq	%rbx, %rdx
	callq	secp256k1_gej_add_var
	leaq	880(%rsp), %rbx
	movq	%rbx, %rdi
	movq	%rbp, %rsi
	movq	%r13, %rdx
	callq	secp256k1_gej_add_var
	leaq	1008(%rsp), %r13
	movq	%r13, %rdi
	movq	%rbp, %rsi
	movq	%rbx, %rdx
	callq	secp256k1_gej_add_var
	leaq	1136(%rsp), %rdi
	movq	%rbp, %rsi
	movq	%r13, %rdx
	callq	secp256k1_gej_add_var
	leaq	1264(%rsp), %rdi
	movq	%r15, %rsi
	movl	$16, %edx
	callq	secp256k1_ecmult_wnaf
	cmpl	%r12d, %eax
	movl	%r12d, %ecx
	cmovgl	%eax, %ecx
	movl	$1, 120(%r14)
	xorps	%xmm0, %xmm0
	movups	%xmm0, (%r14)
	movups	%xmm0, 16(%r14)
	movups	%xmm0, 32(%r14)
	movups	%xmm0, 48(%r14)
	movups	%xmm0, 64(%r14)
	movups	%xmm0, 80(%r14)
	movups	%xmm0, 96(%r14)
	movq	$0, 112(%r14)
	testl	%ecx, %ecx
	jle	.LBB38_15
# %bb.1:                                # %.lr.ph
	movl	%ecx, %r13d
	movslq	%eax, %rbx
	movslq	%r12d, %r12
	movq	%rbx, 8(%rsp)                   # 8-byte Spill
	jmp	.LBB38_2
.LBB38_12:                              #   in Loop: Header=BB38_2 Depth=1
	notl	%ecx
	movl	%ecx, %edx
	shrl	$31, %edx
	addl	%ecx, %edx
	sarl	%edx
	shlq	$6, %rdx
	movq	(%rax,%rdx), %rcx
	movabsq	$4503599627370495, %rbx         # imm = 0xFFFFFFFFFFFFF
	andq	%rbx, %rcx
	movq	%rcx, 16(%rsp)
	movq	(%rax,%rdx), %rsi
	movq	8(%rax,%rdx), %rdi
	shrq	$52, %rsi
	shlq	$12, %rdi
	leaq	-4095(%rbx), %r8
	andq	%r8, %rdi
	orq	%rsi, %rdi
	movq	%rdi, 24(%rsp)
	movq	8(%rax,%rdx), %rdi
	movq	16(%rax,%rdx), %rbp
	shrq	$40, %rdi
	shlq	$24, %rbp
	leaq	-16777215(%rbx), %r9
	andq	%r9, %rbp
	orq	%rdi, %rbp
	movq	%rbp, 32(%rsp)
	movq	16(%rax,%rdx), %rdi
	movq	24(%rax,%rdx), %rbp
	shrq	$28, %rdi
	shlq	$36, %rbp
	movabsq	$4503530907893760, %r10         # imm = 0xFFFF000000000
	andq	%r10, %rbp
	orq	%rdi, %rbp
	movq	%rbp, 40(%rsp)
	movq	24(%rax,%rdx), %rdi
	shrq	$16, %rdi
	movq	%rdi, 48(%rsp)
	movq	32(%rax,%rdx), %rdi
	andq	%rbx, %rdi
	movq	%rdi, 56(%rsp)
	movq	32(%rax,%rdx), %rcx
	movq	40(%rax,%rdx), %rbp
	shrq	$52, %rcx
	shlq	$12, %rbp
	andq	%r8, %rbp
	orq	%rcx, %rbp
	movq	%rbp, 64(%rsp)
	movq	40(%rax,%rdx), %rsi
	movq	48(%rax,%rdx), %rcx
	shrq	$40, %rsi
	shlq	$24, %rcx
	andq	%r9, %rcx
	orq	%rsi, %rcx
	movq	%rcx, 72(%rsp)
	movq	48(%rax,%rdx), %rsi
	movq	56(%rax,%rdx), %rbx
	shrq	$28, %rsi
	shlq	$36, %rbx
	andq	%r10, %rbx
	orq	%rsi, %rbx
	movq	%rbx, 80(%rsp)
	movq	56(%rax,%rdx), %rax
	shrq	$16, %rax
	movl	$0, 96(%rsp)
	movabsq	$18014381329608892, %rdx        # imm = 0x3FFFFBFFFFF0BC
	subq	%rdi, %rdx
	movq	%rdx, 56(%rsp)
	movabsq	$18014398509481980, %rsi        # imm = 0x3FFFFFFFFFFFFC
	movq	%rsi, %rdx
	subq	%rbp, %rdx
	movq	%rdx, 64(%rsp)
	movq	%rsi, %rdx
	subq	%rcx, %rdx
	movq	%rdx, 72(%rsp)
	movq	%rsi, %rcx
	subq	%rbx, %rcx
	movq	8(%rsp), %rbx                   # 8-byte Reload
	movq	%rcx, 80(%rsp)
	movabsq	$1125899906842620, %rcx         # imm = 0x3FFFFFFFFFFFC
	subq	%rax, %rcx
	movq	%rcx, 88(%rsp)
.LBB38_13:                              #   in Loop: Header=BB38_2 Depth=1
	movq	%r14, %rdi
	movq	%r14, %rsi
	leaq	16(%rsp), %rdx
	callq	secp256k1_gej_add_ge_var
.LBB38_14:                              #   in Loop: Header=BB38_2 Depth=1
	movq	%r15, %rax
	addq	$1, %rax
	movq	%r15, %r13
	cmpq	$1, %rax
	jbe	.LBB38_15
.LBB38_2:                               # =>This Inner Loop Header: Depth=1
	leaq	-1(%r13), %r15
	movq	%r14, %rdi
	movq	%r14, %rsi
	callq	secp256k1_gej_double_var
	cmpq	%r12, %r13
	jg	.LBB38_8
# %bb.3:                                #   in Loop: Header=BB38_2 Depth=1
	movl	%r15d, %eax
	movl	2304(%rsp,%rax,4), %eax
	testl	%eax, %eax
	je	.LBB38_8
# %bb.4:                                #   in Loop: Header=BB38_2 Depth=1
	jle	.LBB38_6
# %bb.5:                                #   in Loop: Header=BB38_2 Depth=1
	addl	$-1, %eax
	shrl	%eax
	shlq	$7, %rax
	movups	352(%rsp,%rax), %xmm0
	movaps	%xmm0, 224(%rsp)
	movups	336(%rsp,%rax), %xmm0
	movaps	%xmm0, 208(%rsp)
	movups	320(%rsp,%rax), %xmm0
	movaps	%xmm0, 192(%rsp)
	movups	304(%rsp,%rax), %xmm0
	movaps	%xmm0, 176(%rsp)
	movups	240(%rsp,%rax), %xmm0
	movups	256(%rsp,%rax), %xmm1
	movups	272(%rsp,%rax), %xmm2
	movups	288(%rsp,%rax), %xmm3
	movaps	%xmm3, 160(%rsp)
	movaps	%xmm2, 144(%rsp)
	movaps	%xmm1, 128(%rsp)
	movaps	%xmm0, 112(%rsp)
	jmp	.LBB38_7
.LBB38_6:                               #   in Loop: Header=BB38_2 Depth=1
	notl	%eax
	movl	%eax, %ecx
	shrl	$31, %ecx
	addl	%eax, %ecx
	sarl	%ecx
	shlq	$7, %rcx
	movl	360(%rsp,%rcx), %eax
	movl	%eax, 232(%rsp)
	movups	240(%rsp,%rcx), %xmm0
	movups	256(%rsp,%rcx), %xmm1
	movaps	%xmm0, 112(%rsp)
	movaps	%xmm1, 128(%rsp)
	movq	272(%rsp,%rcx), %rax
	movq	%rax, 144(%rsp)
	movq	312(%rsp,%rcx), %rax
	leaq	152(%rsp), %rdx
	movq	%rax, 32(%rdx)
	movups	296(%rsp,%rcx), %xmm0
	movups	%xmm0, 16(%rdx)
	movups	280(%rsp,%rcx), %xmm0
	movups	%xmm0, (%rdx)
	movq	352(%rsp,%rcx), %rax
	movq	%rax, 72(%rdx)
	movups	320(%rsp,%rcx), %xmm0
	movups	336(%rsp,%rcx), %xmm1
	movups	%xmm1, 56(%rdx)
	movups	%xmm0, 40(%rdx)
	movq	184(%rsp), %rcx
	movq	%rcx, %rax
	shrq	$48, %rax
	movabsq	$4294968273, %rdx               # imm = 0x1000003D1
	imulq	%rdx, %rax
	movabsq	$281474976710655, %rdx          # imm = 0xFFFFFFFFFFFF
	andq	%rdx, %rcx
	addq	152(%rsp), %rax
	movq	%rax, %rdx
	shrq	$52, %rdx
	addq	160(%rsp), %rdx
	movabsq	$4503599627370495, %rbx         # imm = 0xFFFFFFFFFFFFF
	andq	%rbx, %rax
	movq	%rdx, %rsi
	shrq	$52, %rsi
	addq	168(%rsp), %rsi
	andq	%rbx, %rdx
	movq	%rsi, %rdi
	shrq	$52, %rdi
	addq	176(%rsp), %rdi
	andq	%rbx, %rsi
	movq	%rdi, %rbp
	shrq	$52, %rbp
	addq	%rcx, %rbp
	andq	%rbx, %rdi
	movq	8(%rsp), %rbx                   # 8-byte Reload
	movabsq	$18014381329608892, %rcx        # imm = 0x3FFFFBFFFFF0BC
	subq	%rax, %rcx
	movq	%rcx, 152(%rsp)
	movabsq	$18014398509481980, %rcx        # imm = 0x3FFFFFFFFFFFFC
	movq	%rcx, %rax
	subq	%rdx, %rax
	movq	%rax, 160(%rsp)
	movq	%rcx, %rax
	subq	%rsi, %rax
	movq	%rax, 168(%rsp)
	movq	%rcx, %rax
	subq	%rdi, %rax
	movq	%rax, 176(%rsp)
	movabsq	$1125899906842620, %rax         # imm = 0x3FFFFFFFFFFFC
	subq	%rbp, %rax
	movq	%rax, 184(%rsp)
.LBB38_7:                               #   in Loop: Header=BB38_2 Depth=1
	movq	%r14, %rdi
	movq	%r14, %rsi
	leaq	112(%rsp), %rdx
	callq	secp256k1_gej_add_var
.LBB38_8:                               #   in Loop: Header=BB38_2 Depth=1
	cmpq	%rbx, %r13
	jg	.LBB38_14
# %bb.9:                                #   in Loop: Header=BB38_2 Depth=1
	movl	%r15d, %eax
	movl	1264(%rsp,%rax,4), %ecx
	testl	%ecx, %ecx
	je	.LBB38_14
# %bb.10:                               #   in Loop: Header=BB38_2 Depth=1
	movq	104(%rsp), %rax                 # 8-byte Reload
	movq	(%rax), %rax
	jle	.LBB38_12
# %bb.11:                               #   in Loop: Header=BB38_2 Depth=1
	addl	$-1, %ecx
	shrl	%ecx
	shlq	$6, %rcx
	movq	(%rax,%rcx), %rdx
	movabsq	$4503599627370495, %rbx         # imm = 0xFFFFFFFFFFFFF
	andq	%rbx, %rdx
	movq	%rdx, 16(%rsp)
	movq	(%rax,%rcx), %rsi
	movq	8(%rax,%rcx), %rdi
	shrq	$52, %rsi
	shlq	$12, %rdi
	leaq	-4095(%rbx), %rdx
	andq	%rdx, %rdi
	orq	%rsi, %rdi
	movq	%rdi, 24(%rsp)
	movq	8(%rax,%rcx), %rdi
	movq	16(%rax,%rcx), %rbp
	shrq	$40, %rdi
	shlq	$24, %rbp
	leaq	-16777215(%rbx), %rsi
	andq	%rsi, %rbp
	orq	%rdi, %rbp
	movq	%rbp, 32(%rsp)
	movq	16(%rax,%rcx), %rdi
	movq	24(%rax,%rcx), %rbp
	shrq	$28, %rdi
	shlq	$36, %rbp
	movabsq	$4503530907893760, %r8          # imm = 0xFFFF000000000
	andq	%r8, %rbp
	orq	%rdi, %rbp
	movq	%rbp, 40(%rsp)
	movq	24(%rax,%rcx), %rdi
	shrq	$16, %rdi
	movq	%rdi, 48(%rsp)
	movq	32(%rax,%rcx), %rdi
	andq	%rbx, %rdi
	movq	8(%rsp), %rbx                   # 8-byte Reload
	movq	%rdi, 56(%rsp)
	movq	32(%rax,%rcx), %rdi
	movq	40(%rax,%rcx), %rbp
	shrq	$52, %rdi
	shlq	$12, %rbp
	andq	%rdx, %rbp
	orq	%rdi, %rbp
	movq	%rbp, 64(%rsp)
	movq	40(%rax,%rcx), %rdx
	movq	48(%rax,%rcx), %rdi
	shrq	$40, %rdx
	shlq	$24, %rdi
	andq	%rsi, %rdi
	orq	%rdx, %rdi
	movq	%rdi, 72(%rsp)
	movq	48(%rax,%rcx), %rdx
	movq	56(%rax,%rcx), %rsi
	shrq	$28, %rdx
	shlq	$36, %rsi
	andq	%r8, %rsi
	orq	%rdx, %rsi
	movq	%rsi, 80(%rsp)
	movq	56(%rax,%rcx), %rax
	shrq	$16, %rax
	movq	%rax, 88(%rsp)
	movl	$0, 96(%rsp)
	jmp	.LBB38_13
.LBB38_15:                              # %._crit_edge
	addq	$3336, %rsp                     # imm = 0xD08
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end38:
	.size	secp256k1_ecmult, .Lfunc_end38-secp256k1_ecmult
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function secp256k1_gej_eq_x_var
	.type	secp256k1_gej_eq_x_var,@function
secp256k1_gej_eq_x_var:                 # @secp256k1_gej_eq_x_var
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rsi, %rbp
	movq	%rdi, -72(%rsp)                 # 8-byte Spill
	addq	$80, %rsi
	leaq	-40(%rsp), %rdi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -48(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -64(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -56(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-64(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-56(%rsp), %rsi
	movq	-48(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	-72(%rsp), %rbx                 # 8-byte Reload
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -48(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -64(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -56(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-64(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-56(%rsp), %rsi
	movq	-48(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movabsq	$4503599627370495, %r8          # imm = 0xFFFFFFFFFFFFF
	movabsq	$281474976710655, %r9           # imm = 0xFFFFFFFFFFFF
	movq	32(%rbp), %rdi
	movq	%rdi, %rax
	shrq	$48, %rax
	movabsq	$4294968273, %r11               # imm = 0x1000003D1
	imulq	%r11, %rax
	addq	(%rbp), %rax
	andq	%r9, %rdi
	movq	%rax, %rcx
	shrq	$52, %rcx
	addq	8(%rbp), %rcx
	andq	%r8, %rax
	movq	%rcx, %rsi
	shrq	$52, %rsi
	addq	16(%rbp), %rsi
	movq	%rsi, %rbx
	shrq	$52, %rbx
	addq	24(%rbp), %rbx
	movq	%rbx, %rdx
	shrq	$52, %rdx
	subq	-40(%rsp), %rax
	subq	-8(%rsp), %rdi
	addq	%rdx, %rdi
	movabsq	$1125899906842620, %r10         # imm = 0x3FFFFFFFFFFFC
	addq	%rdi, %r10
	movq	%r10, %rdx
	shrq	$48, %rdx
	imulq	%r11, %rdx
	addq	%rax, %rdx
	movabsq	$18014381329608892, %rbp        # imm = 0x3FFFFBFFFFF0BC
	addq	%rdx, %rbp
	movq	%rbp, %rdi
	andq	%r8, %rdi
	addq	$-1, %r11
	xorq	%rdi, %r11
	testq	%rdi, %rdi
	je	.LBB39_2
# %bb.1:
	xorl	%eax, %eax
	cmpq	%r8, %r11
	jne	.LBB39_3
.LBB39_2:
	andq	%r8, %rbx
	andq	%r8, %rsi
	andq	%r8, %rcx
	andq	%r9, %r10
	shrq	$52, %rbp
	addq	%rcx, %rbp
	movabsq	$18014398509481980, %rax        # imm = 0x3FFFFFFFFFFFFC
	addq	%rax, %rbp
	subq	-32(%rsp), %rbp
	movq	%rbp, %rcx
	shrq	$52, %rcx
	addq	%rax, %rsi
	subq	-24(%rsp), %rsi
	addq	%rcx, %rsi
	movq	%rbp, %rcx
	andq	%r8, %rcx
	orq	%rdi, %rcx
	andq	%r11, %rbp
	movq	%rsi, %rdx
	shrq	$52, %rdx
	addq	%rax, %rbx
	subq	-16(%rsp), %rbx
	addq	%rdx, %rbx
	andq	%rsi, %rbp
	andq	%r8, %rsi
	orq	%rcx, %rsi
	movq	%rbx, %rax
	shrq	$52, %rax
	addq	%r10, %rax
	andq	%rbx, %rbp
	andq	%r8, %rbx
	orq	%rsi, %rbx
	movabsq	$4222124650659840, %rcx         # imm = 0xF000000000000
	xorq	%rax, %rcx
	andq	%rbp, %rcx
	orq	%rax, %rbx
	sete	%al
	cmpq	%r8, %rcx
	sete	%cl
	orb	%al, %cl
	movzbl	%cl, %eax
.LBB39_3:                               # %secp256k1_fe_equal_var.exit
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end39:
	.size	secp256k1_gej_eq_x_var, .Lfunc_end39-secp256k1_gej_eq_x_var
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function secp256k1_scalar_inverse
	.type	secp256k1_scalar_inverse,@function
secp256k1_scalar_inverse:               # @secp256k1_scalar_inverse
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$440, %rsp                      # imm = 0x1B8
	.cfi_def_cfa_offset 496
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rsi, %rbx
	movq	%rdi, 80(%rsp)                  # 8-byte Spill
	leaq	16(%rsp), %rsi
	movq	%rbx, %rdi
	movq	%rbx, 8(%rsp)                   # 8-byte Spill
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	120(%rsp), %rbp
	movq	%rbp, %rdi
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	%rbx, %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	movq	%rbp, %rdi
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	88(%rsp), %rbx
	movq	%rbx, %rdi
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbx, %rdi
	movq	8(%rsp), %rbp                   # 8-byte Reload
	movq	%rbp, %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	88(%rsp), %rbx
	movq	%rbx, %rdi
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbx, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	152(%rsp), %rbx
	movq	%rbx, %rdi
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbx, %rdi
	movq	%rbp, %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	152(%rsp), %rbx
	movq	%rbx, %rdi
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbx, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	312(%rsp), %rbp
	movq	%rbp, %rdi
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	120(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	movq	%rbp, %rdi
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	184(%rsp), %rbp
	movq	%rbp, %rdi
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	280(%rsp), %rbp
	movq	%rbp, %rdi
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	408(%rsp), %rbp
	movq	%rbp, %rdi
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	184(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	%rbp, %r15
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	376(%rsp), %rbp
	movq	%rbp, %rdi
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	%r15, %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	movq	%rbp, %r15
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	248(%rsp), %rbp
	movq	%rbp, %rdi
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	%r15, %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	16(%rsp), %r15
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	216(%rsp), %rbx
	movq	%rbx, %rdi
	callq	secp256k1_scalar_reduce_512
	movl	$59, %ebp
	.p2align	4, 0x90
.LBB40_1:                               # =>This Inner Loop Header: Depth=1
	movq	%r15, %rsi
	movq	%rbx, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	addl	$-1, %ebp
	jne	.LBB40_1
# %bb.2:
	leaq	16(%rsp), %rsi
	leaq	216(%rsp), %rbp
	movq	%rbp, %rdi
	leaq	248(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	leaq	344(%rsp), %rbp
	movq	%rbp, %rdi
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	184(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	88(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	88(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	120(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	88(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	88(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	120(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	152(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	88(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	88(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	280(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	152(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	120(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	120(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	120(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	120(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	movq	8(%rsp), %rdx                   # 8-byte Reload
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rbx
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	movq	%rbx, %rsi
	movq	%rbp, %rdi
	#APP
	movq	(%rdi), %r11
	movq	8(%rdi), %r12
	movq	16(%rdi), %r13
	movq	24(%rdi), %r14
	movq	%r11, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r11, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r11, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r12, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r14, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	callq	secp256k1_scalar_reduce_512
	leaq	16(%rsp), %rsi
	movq	%rbp, %rdi
	leaq	312(%rsp), %rdx
	#APP
	movq	(%rdi), %r15
	movq	8(%rdi), %rbx
	movq	16(%rdi), %rcx
	movq	(%rdx), %r11
	movq	8(%rdx), %r12
	movq	16(%rdx), %r13
	movq	24(%rdx), %r14
	movq	%r15, %rax
	mulq	%r11
	movq	%rax, (%rsi)
	movq	%rdx, %r8
	xorq	%r9, %r9
	xorq	%r10, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rbx, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 8(%rsi)
	xorq	%r8, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rbx, %rax
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%rcx, %rax
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 16(%rsi)
	xorq	%r9, %r9
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	24(%rdi), %r15
	movq	%rbx, %rax
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%rcx, %rax
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r15, %rax
	mulq	%r11
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movq	%r10, 24(%rsi)
	xorq	%r10, %r10
	movq	%rbx, %rax
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%rcx, %rax
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r15, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movq	%r8, 32(%rsi)
	xorq	%r8, %r8
	movq	%rcx, %rax
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r15, %rax
	mulq	%r13
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 40(%rsi)
	movq	%r15, %rax
	mulq	%r14
	addq	%rax, %r10
	adcq	%rdx, %r8
	movq	%r10, 48(%rsi)
	movq	%r8, 56(%rsi)

	#NO_APP
	movq	80(%rsp), %rdi                  # 8-byte Reload
	callq	secp256k1_scalar_reduce_512
	addq	$440, %rsp                      # imm = 0x1B8
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end40:
	.size	secp256k1_scalar_inverse, .Lfunc_end40-secp256k1_scalar_inverse
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function secp256k1_scalar_reduce_512
.LCPI41_0:
	.byte	255                             # 0xff
	.byte	255                             # 0xff
	.byte	255                             # 0xff
	.byte	255                             # 0xff
	.byte	255                             # 0xff
	.byte	255                             # 0xff
	.byte	255                             # 0xff
	.byte	255                             # 0xff
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.byte	0                               # 0x0
	.text
	.p2align	4, 0x90
	.type	secp256k1_scalar_reduce_512,@function
secp256k1_scalar_reduce_512:            # @secp256k1_scalar_reduce_512
	.cfi_startproc
# %bb.0:
	pushq	%r14
	.cfi_def_cfa_offset 16
	pushq	%r13
	.cfi_def_cfa_offset 24
	pushq	%r12
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	subq	$72, %rsp
	.cfi_def_cfa_offset 112
	.cfi_offset %rbx, -40
	.cfi_offset %r12, -32
	.cfi_offset %r13, -24
	.cfi_offset %r14, -16
	#APP
	movq	32(%rsi), %r11
	movq	40(%rsi), %r12
	movq	48(%rsi), %r13
	movq	56(%rsi), %r14
	movq	(%rsi), %r8
	movq	$0, %r9
	movq	$0, %r10
	movabsq	$4624529908474429119, %rax      # imm = 0x402DA1732FC9BEBF
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, 64(%rsp)
	movq	$0, %r8
	addq	8(%rsi), %r9
	adcq	$0, %r10
	movabsq	$4624529908474429119, %rax      # imm = 0x402DA1732FC9BEBF
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movabsq	$4994812053365940164, %rax      # imm = 0x4551231950B75FC4
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, 56(%rsp)
	movq	$0, %r9
	addq	16(%rsi), %r10
	adcq	$0, %r8
	adcq	$0, %r9
	movabsq	$4624529908474429119, %rax      # imm = 0x402DA1732FC9BEBF
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movabsq	$4994812053365940164, %rax      # imm = 0x4551231950B75FC4
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%r11, %r10
	adcq	$0, %r8
	adcq	$0, %r9
	movq	%r10, 48(%rsp)
	movq	$0, %r10
	addq	24(%rsi), %r8
	adcq	$0, %r9
	adcq	$0, %r10
	movabsq	$4624529908474429119, %rax      # imm = 0x402DA1732FC9BEBF
	mulq	%r14
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	movabsq	$4994812053365940164, %rax      # imm = 0x4551231950B75FC4
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	adcq	$0, %r10
	addq	%r12, %r8
	adcq	$0, %r9
	adcq	$0, %r10
	movq	%r8, 40(%rsp)
	movq	$0, %r8
	movabsq	$4994812053365940164, %rax      # imm = 0x4551231950B75FC4
	mulq	%r14
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	addq	%r13, %r9
	adcq	$0, %r10
	adcq	$0, %r8
	movq	%r9, 32(%rsp)
	addq	%r14, %r10
	adcq	$0, %r8
	movq	%r10, 24(%rsp)
	movq	%r8, 16(%rsp)

	#NO_APP
	movq	64(%rsp), %rax
	movq	56(%rsp), %rcx
	movq	48(%rsp), %rdx
	movq	40(%rsp), %rsi
	movq	32(%rsp), %r8
	movq	24(%rsp), %r9
	movq	16(%rsp), %r10
	movq	%rax, -40(%rsp)
	movq	%rcx, -48(%rsp)
	movq	%rdx, -56(%rsp)
	movq	%rsi, -64(%rsp)
	movq	%r8, -72(%rsp)
	movq	%r9, -80(%rsp)
	movq	%r10, -88(%rsp)
	#APP
	movq	-72(%rsp), %r11
	movq	-80(%rsp), %r12
	movq	-88(%rsp), %r13
	movq	-40(%rsp), %r8
	movq	$0, %r9
	movq	$0, %r10
	movabsq	$4624529908474429119, %rax      # imm = 0x402DA1732FC9BEBF
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, 8(%rsp)
	movq	$0, %r8
	addq	-48(%rsp), %r9
	adcq	$0, %r10
	movabsq	$4624529908474429119, %rax      # imm = 0x402DA1732FC9BEBF
	mulq	%r12
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movabsq	$4994812053365940164, %rax      # imm = 0x4551231950B75FC4
	mulq	%r11
	addq	%rax, %r9
	adcq	%rdx, %r10
	adcq	$0, %r8
	movq	%r9, (%rsp)
	movq	$0, %r9
	addq	-56(%rsp), %r10
	adcq	$0, %r8
	adcq	$0, %r9
	movabsq	$4624529908474429119, %rax      # imm = 0x402DA1732FC9BEBF
	mulq	%r13
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	movabsq	$4994812053365940164, %rax      # imm = 0x4551231950B75FC4
	mulq	%r12
	addq	%rax, %r10
	adcq	%rdx, %r8
	adcq	$0, %r9
	addq	%r11, %r10
	adcq	$0, %r8
	adcq	$0, %r9
	movq	%r10, -8(%rsp)
	addq	-64(%rsp), %r8
	adcq	$0, %r9
	movabsq	$4994812053365940164, %rax      # imm = 0x4551231950B75FC4
	mulq	%r13
	addq	%rax, %r8
	adcq	%rdx, %r9
	addq	%r12, %r8
	adcq	$0, %r9
	movq	%r8, -16(%rsp)
	addq	%r13, %r9
	movq	%r9, -24(%rsp)

	#NO_APP
	movq	8(%rsp), %rax
	movq	(%rsp), %rcx
	movq	-8(%rsp), %rdx
	movq	-16(%rsp), %rsi
	movq	-24(%rsp), %r8
	movq	%rax, -96(%rsp)
	movq	%rcx, -104(%rsp)
	movq	%rdx, -112(%rsp)
	movq	%rsi, -120(%rsp)
	movq	%r8, -128(%rsp)
	#APP
	movq	-128(%rsp), %r10
	movabsq	$4624529908474429119, %rax      # imm = 0x402DA1732FC9BEBF
	mulq	%r10
	addq	-96(%rsp), %rax
	adcq	$0, %rdx
	movq	%rax, (%rdi)
	movq	%rdx, %r8
	movq	$0, %r9
	addq	-104(%rsp), %r8
	adcq	$0, %r9
	movabsq	$4994812053365940164, %rax      # imm = 0x4551231950B75FC4
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, 8(%rdi)
	movq	$0, %r8
	addq	%r10, %r9
	adcq	$0, %r8
	addq	-112(%rsp), %r9
	adcq	$0, %r8
	movq	%r9, 16(%rdi)
	movq	$0, %r9
	addq	-120(%rsp), %r8
	adcq	$0, %r9
	movq	%r8, 24(%rdi)
	movq	%r9, -32(%rsp)

	#NO_APP
	movq	24(%rdi), %r9
	cmpq	$-1, %r9
	setne	%cl
	movq	16(%rdi), %xmm0                 # xmm0 = mem[0],zero
	pcmpeqb	.LCPI41_0(%rip), %xmm0
	pmovmskb	%xmm0, %edx
	xorl	%esi, %esi
	cmpl	$65535, %edx                    # imm = 0xFFFF
	sete	%sil
	movq	16(%rdi), %r8
	cmpq	$-2, %r8
	setb	%dl
	orb	%cl, %dl
	movzbl	%dl, %eax
	movl	%eax, %ecx
	notl	%ecx
	andl	%esi, %ecx
	xorl	%esi, %esi
	movq	(%rdi), %r10
	movq	8(%rdi), %r14
	movabsq	$-4994812053365940165, %r11     # imm = 0xBAAEDCE6AF48A03B
	cmpq	%r14, %r11
	movl	$0, %ebx
	sbbq	%rbx, %rbx
	setb	%bl
	cmpq	%r11, %r14
	sbbq	%rsi, %rsi
	setb	%dl
	orb	%dl, %al
	movzbl	%al, %eax
	notl	%eax
	movzbl	%bl, %edx
	andl	%eax, %edx
	orl	$-2, %eax
	movzbl	%dl, %edx
	orl	%ecx, %edx
	movabsq	$-4624529908474429120, %rcx     # imm = 0xBFD25E8CD0364140
	xorl	%esi, %esi
	cmpq	%rcx, %r10
	seta	%sil
	andl	%eax, %esi
	orl	%edx, %esi
	addl	-32(%rsp), %esi
	movabsq	$4624529908474429119, %rax      # imm = 0x402DA1732FC9BEBF
	imulq	%rsi, %rax
	movabsq	$4994812053365940164, %rcx      # imm = 0x4551231950B75FC4
	imulq	%rsi, %rcx
	addq	%r10, %rax
	adcq	%r14, %rcx
	movq	%rax, (%rdi)
	movq	%rcx, 8(%rdi)
	adcq	%r8, %rsi
	movq	%rsi, 16(%rdi)
	adcq	$0, %r9
	movq	%r9, 24(%rdi)
	addq	$72, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r12
	.cfi_def_cfa_offset 24
	popq	%r13
	.cfi_def_cfa_offset 16
	popq	%r14
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end41:
	.size	secp256k1_scalar_reduce_512, .Lfunc_end41-secp256k1_scalar_reduce_512
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function secp256k1_ecmult_wnaf
	.type	secp256k1_ecmult_wnaf,@function
secp256k1_ecmult_wnaf:                  # @secp256k1_ecmult_wnaf
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$56, %rsp
	.cfi_def_cfa_offset 112
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	%edx, %r10d
	movq	%rdi, %r11
	movdqu	(%rsi), %xmm0
	movdqu	16(%rsi), %xmm1
	movdqa	%xmm1, 32(%rsp)
	movdqa	%xmm0, 16(%rsp)
	movq	40(%rsp), %r15
	movl	$1, 8(%rsp)                     # 4-byte Folded Spill
	testq	%r15, %r15
	js	.LBB42_2
# %bb.1:
	movl	$1, 4(%rsp)                     # 4-byte Folded Spill
	jmp	.LBB42_3
.LBB42_2:
	movq	32(%rsp), %rax
	pcmpeqd	%xmm0, %xmm0
	pxor	16(%rsp), %xmm0
	movq	%xmm0, %rcx
	pshufd	$238, %xmm0, %xmm0              # xmm0 = xmm0[2,3,2,3]
	movq	%xmm0, %rdx
	movabsq	$-4994812053365940165, %rsi     # imm = 0xBAAEDCE6AF48A03B
	xorl	%edi, %edi
	addq	%rdx, %rsi
	setb	%dil
	movabsq	$-4624529908474429118, %rdx     # imm = 0xBFD25E8CD0364142
	xorl	%ebp, %ebp
	addq	%rcx, %rdx
	setb	%bpl
	movq	%rdx, %xmm0
	movq	%rbp, %xmm1
	punpcklqdq	%xmm1, %xmm0            # xmm0 = xmm0[0],xmm1[0]
	pshufd	$238, %xmm0, %xmm0              # xmm0 = xmm0[2,3,2,3]
	movq	%xmm0, %rcx
	movq	%rsi, %xmm0
	movq	%rdi, %xmm1
	punpcklqdq	%xmm1, %xmm0            # xmm0 = xmm0[0],xmm1[0]
	movq	%rdx, 16(%rsp)
	pshufd	$238, %xmm0, %xmm0              # xmm0 = xmm0[2,3,2,3]
	movq	%xmm0, %rdx
	addq	%rsi, %rcx
	movq	%rcx, 24(%rsp)
	notq	%rax
	adcq	%rdx, %rax
	setb	%cl
	movzbl	%cl, %ecx
	addq	$-2, %rax
	notq	%r15
	adcq	%rcx, %r15
	movq	%rax, 32(%rsp)
	addq	$-1, %r15
	movq	%r15, 40(%rsp)
	movl	$-1, 4(%rsp)                    # 4-byte Folded Spill
.LBB42_3:
	leal	-1(%r10), %ecx
                                        # kill: def $cl killed $cl killed $ecx
	shll	%cl, 8(%rsp)                    # 4-byte Folded Spill
	movl	$-1, %eax
	movl	%r10d, %ecx
	shll	%cl, %eax
	movl	%eax, 12(%rsp)                  # 4-byte Spill
	xorl	%eax, %eax
	xorl	%r14d, %r14d
	jmp	.LBB42_4
	.p2align	4, 0x90
.LBB42_14:                              #   in Loop: Header=BB42_4 Depth=1
	addl	$1, %r14d
	cmpl	$256, %r14d                     # imm = 0x100
	jge	.LBB42_13
.LBB42_4:                               # =>This Inner Loop Header: Depth=1
	movl	%r14d, %r12d
	shrl	$6, %r12d
	movq	16(%rsp,%r12,8), %rdx
	movl	%r14d, %ebp
	andl	$63, %ebp
	movq	%rdx, %r13
	movl	%ebp, %ecx
	shrq	%cl, %r13
	btq	%r14, %rdx
	jae	.LBB42_14
# %bb.5:                                # %.preheader
                                        #   in Loop: Header=BB42_4 Depth=1
	cmpl	%r14d, %eax
	jge	.LBB42_7
# %bb.6:                                # %.lr.ph.preheader
                                        #   in Loop: Header=BB42_4 Depth=1
	movslq	%eax, %rcx
	leaq	(%r11,%rcx,4), %rdi
	notl	%eax
	addl	%r14d, %eax
	leaq	4(,%rax,4), %rdx
	xorl	%esi, %esi
	movq	%r10, %rbx
	movq	%r15, 48(%rsp)                  # 8-byte Spill
	movq	%r11, %r15
	callq	memset@PLT
	movq	%r15, %r11
	movq	48(%rsp), %r15                  # 8-byte Reload
	movq	%rbx, %r10
	movl	%r14d, %eax
.LBB42_7:                               # %._crit_edge
                                        #   in Loop: Header=BB42_4 Depth=1
	leal	(%r14,%r10), %r8d
	movl	$256, %edx                      # imm = 0x100
	subl	%r14d, %edx
	cmpl	$257, %r8d                      # imm = 0x101
	cmovll	%r10d, %edx
	leal	(%rdx,%r14), %ecx
	addl	$-1, %ecx
	shrl	$6, %ecx
	cmpl	%r12d, %ecx
	je	.LBB42_9
# %bb.8:                                #   in Loop: Header=BB42_4 Depth=1
	movl	%r12d, %ecx
	movq	24(%rsp,%rcx,8), %rsi
	negb	%bpl
	movl	%ebp, %ecx
	shlq	%cl, %rsi
	orq	%rsi, %r13
.LBB42_9:                               # %secp256k1_scalar_get_bits_var.exit
                                        #   in Loop: Header=BB42_4 Depth=1
	movq	$-1, %rsi
	movl	%edx, %ecx
	shlq	%cl, %rsi
	notl	%esi
	andl	%esi, %r13d
	testl	%r13d, 8(%rsp)                  # 4-byte Folded Reload
	je	.LBB42_11
# %bb.10:                               #   in Loop: Header=BB42_4 Depth=1
	movl	%r8d, %esi
	shrl	$6, %esi
	xorl	%r9d, %r9d
	cmpl	$64, %r8d
	setb	%r9b
	movl	%r8d, %ecx
	shlq	%cl, %r9
	xorl	%ebp, %ebp
	cmpl	$1, %esi
	sete	%bpl
	shlq	%cl, %rbp
	xorl	%ebx, %ebx
	cmpl	$2, %esi
	sete	%bl
	shlq	%cl, %rbx
	xorl	%edi, %edi
	cmpl	$3, %esi
	sete	%dil
	shlq	%cl, %rdi
	addq	%r9, 16(%rsp)
	adcq	%rbp, 24(%rsp)
	adcq	%rbx, 32(%rsp)
	adcq	%rdi, %r15
	movq	%r15, 40(%rsp)
	addl	12(%rsp), %r13d                 # 4-byte Folded Reload
.LBB42_11:                              #   in Loop: Header=BB42_4 Depth=1
	imull	4(%rsp), %r13d                  # 4-byte Folded Reload
	addl	%r14d, %edx
	cltq
	movl	%r13d, (%r11,%rax,4)
	addl	$1, %eax
	movl	%edx, %r14d
	cmpl	$256, %r14d                     # imm = 0x100
	jl	.LBB42_4
.LBB42_13:
                                        # kill: def $eax killed $eax killed $rax
	addq	$56, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end42:
	.size	secp256k1_ecmult_wnaf, .Lfunc_end42-secp256k1_ecmult_wnaf
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function secp256k1_rfc6979_hmac_sha256_initialize
.LCPI43_0:
	.zero	16,1
.LCPI43_1:
	.long	1779033703                      # 0x6a09e667
	.long	3144134277                      # 0xbb67ae85
	.long	1013904242                      # 0x3c6ef372
	.long	2773480762                      # 0xa54ff53a
.LCPI43_2:
	.long	1359893119                      # 0x510e527f
	.long	2600822924                      # 0x9b05688c
	.long	528734635                       # 0x1f83d9ab
	.long	1541459225                      # 0x5be0cd19
.LCPI43_3:
	.zero	16,92
.LCPI43_4:
	.zero	16,106
.LCPI43_5:
	.zero	16
	.text
	.p2align	4, 0x90
	.type	secp256k1_rfc6979_hmac_sha256_initialize,@function
secp256k1_rfc6979_hmac_sha256_initialize: # @secp256k1_rfc6979_hmac_sha256_initialize
	.cfi_startproc
# %bb.0:                                # %iter.check
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$504, %rsp                      # imm = 0x1F8
	.cfi_def_cfa_offset 560
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%r8, 88(%rsp)                   # 8-byte Spill
	movq	%rcx, %r15
	movq	%rdx, %r14
	movq	%rsi, %r12
	movq	%rdi, %r13
	movaps	.LCPI43_0(%rip), %xmm0          # xmm0 = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]
	movups	%xmm0, 16(%rdi)
	movups	%xmm0, (%rdi)
	leaq	32(%rdi), %rax
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	xorps	%xmm0, %xmm0
	movups	%xmm0, 32(%rdi)
	movups	%xmm0, 48(%rdi)
	leaq	296(%rsp), %rdi
	movaps	.LCPI43_1(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movups	%xmm0, 296(%rsp)
	movaps	.LCPI43_2(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movups	%xmm0, 312(%rsp)
	movq	$0, 488(%rsp)
	movaps	.LCPI43_3(%rip), %xmm0          # xmm0 = [92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92]
	movaps	%xmm0, (%rsp)
	movaps	%xmm0, 16(%rsp)
	movaps	%xmm0, 32(%rsp)
	movaps	%xmm0, 48(%rsp)
	movq	%rsp, %rbp
	movl	$64, %edx
	movq	%rbp, %rsi
	callq	secp256k1_sha256_write
	movaps	.LCPI43_1(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	%xmm0, 96(%rsp)
	movaps	.LCPI43_2(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movaps	%xmm0, 112(%rsp)
	movq	$0, 288(%rsp)
	movaps	.LCPI43_4(%rip), %xmm1          # xmm1 = [106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106]
	movaps	(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, (%rsp)
	movaps	16(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 16(%rsp)
	movaps	32(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 32(%rsp)
	movaps	48(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 48(%rsp)
	leaq	96(%rsp), %rbx
	movl	$64, %edx
	movq	%rbx, %rdi
	movq	%rbp, %rsi
	callq	secp256k1_sha256_write
	movl	$32, %edx
	movq	%rbx, %rdi
	movq	%r13, %rsi
	callq	secp256k1_sha256_write
	leaq	.L.str.21(%rip), %rsi
	movl	$1, %edx
	movq	%rbx, %rdi
	callq	secp256k1_sha256_write
	movl	$32, %edx
	movq	%rbx, %rdi
	movq	%r12, 72(%rsp)                  # 8-byte Spill
	movq	%r12, %rsi
	callq	secp256k1_sha256_write
	movl	$32, %edx
	movq	%rbx, %rdi
	movq	%r14, 80(%rsp)                  # 8-byte Spill
	movq	%r14, %rsi
	movq	88(%rsp), %rbp                  # 8-byte Reload
	callq	secp256k1_sha256_write
	testq	%r15, %r15
	je	.LBB43_3
# %bb.1:                                # %iter.check
	testq	%rbp, %rbp
	je	.LBB43_3
# %bb.2:                                # %iter.check150
	movq	%r15, %r14
	leaq	96(%rsp), %r12
	movq	%r12, %rdi
	movq	%r15, %rsi
	movq	%rbp, %rdx
	callq	secp256k1_sha256_write
	movq	%r12, %rdi
	movq	64(%rsp), %rbx                  # 8-byte Reload
	movq	%rbx, %rsi
	callq	secp256k1_hmac_sha256_finalize
	movups	(%rbx), %xmm0
	movups	16(%rbx), %xmm1
	movaps	%xmm0, (%rsp)
	movaps	%xmm1, 16(%rsp)
	xorps	%xmm2, %xmm2
	movaps	%xmm2, 48(%rsp)
	movaps	.LCPI43_1(%rip), %xmm2          # xmm2 = [1779033703,3144134277,1013904242,2773480762]
	movups	%xmm2, 296(%rsp)
	movaps	.LCPI43_2(%rip), %xmm2          # xmm2 = [1359893119,2600822924,528734635,1541459225]
	movups	%xmm2, 312(%rsp)
	movq	$0, 488(%rsp)
	movaps	.LCPI43_3(%rip), %xmm2          # xmm2 = [92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92]
	xorps	%xmm2, %xmm0
	movaps	%xmm0, (%rsp)
	xorps	%xmm2, %xmm1
	movaps	%xmm1, 16(%rsp)
	movaps	%xmm2, 32(%rsp)
	movaps	48(%rsp), %xmm0
	xorps	%xmm2, %xmm0
	movaps	%xmm0, 48(%rsp)
	movq	%rsp, %r15
	movl	$64, %edx
	leaq	296(%rsp), %rdi
	movq	%r15, %rsi
	callq	secp256k1_sha256_write
	movaps	.LCPI43_1(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	%xmm0, 96(%rsp)
	movaps	.LCPI43_2(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movaps	%xmm0, 112(%rsp)
	movq	$0, 288(%rsp)
	movaps	(%rsp), %xmm0
	movaps	.LCPI43_4(%rip), %xmm1          # xmm1 = [106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106]
	xorps	%xmm1, %xmm0
	movaps	%xmm0, (%rsp)
	movaps	16(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 16(%rsp)
	movaps	32(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 32(%rsp)
	movaps	48(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 48(%rsp)
	movl	$64, %edx
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	secp256k1_sha256_write
	movl	$32, %edx
	movq	%r12, %rdi
	movq	%r13, %rsi
	callq	secp256k1_sha256_write
	movq	%r12, %rdi
	movq	%r13, %rsi
	callq	secp256k1_hmac_sha256_finalize
	movups	(%rbx), %xmm0
	movups	16(%rbx), %xmm1
	movaps	%xmm0, (%rsp)
	movaps	%xmm1, 16(%rsp)
	xorps	%xmm2, %xmm2
	movaps	%xmm2, 48(%rsp)
	movaps	.LCPI43_1(%rip), %xmm2          # xmm2 = [1779033703,3144134277,1013904242,2773480762]
	movups	%xmm2, 296(%rsp)
	movaps	.LCPI43_2(%rip), %xmm2          # xmm2 = [1359893119,2600822924,528734635,1541459225]
	movups	%xmm2, 312(%rsp)
	movq	$0, 488(%rsp)
	movaps	.LCPI43_3(%rip), %xmm2          # xmm2 = [92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92]
	xorps	%xmm2, %xmm0
	movaps	%xmm0, (%rsp)
	xorps	%xmm2, %xmm1
	movaps	%xmm1, 16(%rsp)
	movaps	%xmm2, 32(%rsp)
	movaps	48(%rsp), %xmm0
	xorps	%xmm2, %xmm0
	movaps	%xmm0, 48(%rsp)
	movq	%rsp, %r15
	movl	$64, %edx
	leaq	296(%rsp), %rdi
	movq	%r15, %rsi
	callq	secp256k1_sha256_write
	movaps	.LCPI43_1(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	%xmm0, 96(%rsp)
	movaps	.LCPI43_2(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movaps	%xmm0, 112(%rsp)
	movq	$0, 288(%rsp)
	movaps	(%rsp), %xmm0
	movaps	.LCPI43_4(%rip), %xmm1          # xmm1 = [106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106]
	xorps	%xmm1, %xmm0
	movaps	%xmm0, (%rsp)
	movaps	16(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 16(%rsp)
	movaps	32(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 32(%rsp)
	movaps	48(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 48(%rsp)
	movl	$64, %edx
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	secp256k1_sha256_write
	movl	$32, %edx
	movq	%r12, %rdi
	movq	%r13, %rsi
	callq	secp256k1_sha256_write
	leaq	secp256k1_rfc6979_hmac_sha256_initialize.one(%rip), %rsi
	movl	$1, %edx
	movq	%r12, %rdi
	callq	secp256k1_sha256_write
	movl	$32, %edx
	movq	%r12, %rdi
	movq	72(%rsp), %rsi                  # 8-byte Reload
	callq	secp256k1_sha256_write
	movl	$32, %edx
	movq	%r12, %rdi
	movq	80(%rsp), %rsi                  # 8-byte Reload
	callq	secp256k1_sha256_write
	movq	%r12, %rdi
	movq	%r14, %rsi
	movq	%rbp, %rdx
	jmp	.LBB43_4
.LBB43_3:                               # %iter.check64
	leaq	96(%rsp), %rbp
	movq	%rbp, %rdi
	movq	64(%rsp), %rbx                  # 8-byte Reload
	movq	%rbx, %rsi
	callq	secp256k1_hmac_sha256_finalize
	movups	(%rbx), %xmm0
	movups	16(%rbx), %xmm1
	movaps	%xmm0, (%rsp)
	movaps	%xmm1, 16(%rsp)
	xorps	%xmm2, %xmm2
	movaps	%xmm2, 48(%rsp)
	movaps	.LCPI43_1(%rip), %xmm2          # xmm2 = [1779033703,3144134277,1013904242,2773480762]
	movups	%xmm2, 296(%rsp)
	movaps	.LCPI43_2(%rip), %xmm2          # xmm2 = [1359893119,2600822924,528734635,1541459225]
	movups	%xmm2, 312(%rsp)
	movq	$0, 488(%rsp)
	movaps	.LCPI43_3(%rip), %xmm2          # xmm2 = [92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92]
	xorps	%xmm2, %xmm0
	movaps	%xmm0, (%rsp)
	xorps	%xmm2, %xmm1
	movaps	%xmm1, 16(%rsp)
	movaps	%xmm2, 32(%rsp)
	movaps	48(%rsp), %xmm0
	xorps	%xmm2, %xmm0
	movaps	%xmm0, 48(%rsp)
	movq	%rsp, %r15
	movl	$64, %edx
	leaq	296(%rsp), %r14
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	secp256k1_sha256_write
	movaps	.LCPI43_1(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	%xmm0, 96(%rsp)
	movaps	.LCPI43_2(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movaps	%xmm0, 112(%rsp)
	movq	$0, 288(%rsp)
	movaps	(%rsp), %xmm0
	movaps	.LCPI43_4(%rip), %xmm1          # xmm1 = [106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106]
	xorps	%xmm1, %xmm0
	movaps	%xmm0, (%rsp)
	movaps	16(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 16(%rsp)
	movaps	32(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 32(%rsp)
	movaps	48(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 48(%rsp)
	movl	$64, %edx
	movq	%rbp, %rdi
	movq	%r15, %rsi
	callq	secp256k1_sha256_write
	movl	$32, %edx
	movq	%rbp, %rdi
	movq	%r13, %rsi
	callq	secp256k1_sha256_write
	movq	%rbp, %rdi
	movq	%r13, %rsi
	callq	secp256k1_hmac_sha256_finalize
	movups	(%rbx), %xmm0
	movups	16(%rbx), %xmm1
	movaps	%xmm0, (%rsp)
	movaps	%xmm1, 16(%rsp)
	xorps	%xmm2, %xmm2
	movaps	%xmm2, 48(%rsp)
	movaps	.LCPI43_1(%rip), %xmm2          # xmm2 = [1779033703,3144134277,1013904242,2773480762]
	movups	%xmm2, 296(%rsp)
	movaps	.LCPI43_2(%rip), %xmm2          # xmm2 = [1359893119,2600822924,528734635,1541459225]
	movups	%xmm2, 312(%rsp)
	movq	$0, 488(%rsp)
	movaps	.LCPI43_3(%rip), %xmm2          # xmm2 = [92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92]
	xorps	%xmm2, %xmm0
	movaps	%xmm0, (%rsp)
	xorps	%xmm2, %xmm1
	movaps	%xmm1, 16(%rsp)
	movaps	%xmm2, 32(%rsp)
	movaps	48(%rsp), %xmm0
	xorps	%xmm2, %xmm0
	movaps	%xmm0, 48(%rsp)
	movq	%rsp, %r15
	movl	$64, %edx
	movq	%r14, %rdi
	movq	%r15, %rsi
	callq	secp256k1_sha256_write
	movaps	.LCPI43_1(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	%xmm0, 96(%rsp)
	movaps	.LCPI43_2(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movaps	%xmm0, 112(%rsp)
	movq	$0, 288(%rsp)
	movaps	(%rsp), %xmm0
	movaps	.LCPI43_4(%rip), %xmm1          # xmm1 = [106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106]
	xorps	%xmm1, %xmm0
	movaps	%xmm0, (%rsp)
	movaps	16(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 16(%rsp)
	movaps	32(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 32(%rsp)
	movaps	48(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 48(%rsp)
	movl	$64, %edx
	movq	%rbp, %rdi
	movq	%r15, %rsi
	callq	secp256k1_sha256_write
	movl	$32, %edx
	movq	%rbp, %rdi
	movq	%r13, %rsi
	callq	secp256k1_sha256_write
	leaq	secp256k1_rfc6979_hmac_sha256_initialize.one(%rip), %rsi
	movl	$1, %edx
	movq	%rbp, %rdi
	callq	secp256k1_sha256_write
	movl	$32, %edx
	movq	%rbp, %rdi
	movq	72(%rsp), %rsi                  # 8-byte Reload
	callq	secp256k1_sha256_write
	movl	$32, %edx
	movq	%rbp, %rdi
	movq	80(%rsp), %rsi                  # 8-byte Reload
.LBB43_4:                               # %iter.check236
	callq	secp256k1_sha256_write
	leaq	96(%rsp), %rbp
	movq	%rbp, %rdi
	movq	%rbx, %rsi
	callq	secp256k1_hmac_sha256_finalize
	movups	(%rbx), %xmm0
	movups	16(%rbx), %xmm1
	movaps	%xmm0, (%rsp)
	movaps	%xmm1, 16(%rsp)
	xorps	%xmm2, %xmm2
	movaps	%xmm2, 48(%rsp)
	movaps	.LCPI43_1(%rip), %xmm2          # xmm2 = [1779033703,3144134277,1013904242,2773480762]
	movups	%xmm2, 296(%rsp)
	movaps	.LCPI43_2(%rip), %xmm2          # xmm2 = [1359893119,2600822924,528734635,1541459225]
	movups	%xmm2, 312(%rsp)
	movq	$0, 488(%rsp)
	movaps	.LCPI43_3(%rip), %xmm2          # xmm2 = [92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92]
	xorps	%xmm2, %xmm0
	movaps	%xmm0, (%rsp)
	xorps	%xmm2, %xmm1
	movaps	%xmm1, 16(%rsp)
	movaps	%xmm2, 32(%rsp)
	xorps	48(%rsp), %xmm2
	movaps	%xmm2, 48(%rsp)
	movq	%rsp, %rbx
	movl	$64, %edx
	leaq	296(%rsp), %rdi
	movq	%rbx, %rsi
	callq	secp256k1_sha256_write
	movaps	.LCPI43_1(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	%xmm0, 96(%rsp)
	movaps	.LCPI43_2(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movaps	%xmm0, 112(%rsp)
	movq	$0, 288(%rsp)
	movaps	(%rsp), %xmm0
	movaps	.LCPI43_4(%rip), %xmm1          # xmm1 = [106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106]
	xorps	%xmm1, %xmm0
	movaps	%xmm0, (%rsp)
	movaps	16(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 16(%rsp)
	movaps	32(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 32(%rsp)
	xorps	48(%rsp), %xmm1
	movaps	%xmm1, 48(%rsp)
	movl	$64, %edx
	movq	%rbp, %rdi
	movq	%rbx, %rsi
	callq	secp256k1_sha256_write
	movl	$32, %edx
	movq	%rbp, %rdi
	movq	%r13, %rsi
	callq	secp256k1_sha256_write
	movq	%rbp, %rdi
	movq	%r13, %rsi
	callq	secp256k1_hmac_sha256_finalize
	movl	$0, 64(%r13)
	addq	$504, %rsp                      # imm = 0x1F8
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end43:
	.size	secp256k1_rfc6979_hmac_sha256_initialize, .Lfunc_end43-secp256k1_rfc6979_hmac_sha256_initialize
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function secp256k1_rfc6979_hmac_sha256_generate
.LCPI44_0:
	.long	1779033703                      # 0x6a09e667
	.long	3144134277                      # 0xbb67ae85
	.long	1013904242                      # 0x3c6ef372
	.long	2773480762                      # 0xa54ff53a
.LCPI44_1:
	.long	1359893119                      # 0x510e527f
	.long	2600822924                      # 0x9b05688c
	.long	528734635                       # 0x1f83d9ab
	.long	1541459225                      # 0x5be0cd19
.LCPI44_2:
	.zero	16,92
.LCPI44_3:
	.zero	16,106
.LCPI44_4:
	.zero	16
	.text
	.p2align	4, 0x90
	.type	secp256k1_rfc6979_hmac_sha256_generate,@function
secp256k1_rfc6979_hmac_sha256_generate: # @secp256k1_rfc6979_hmac_sha256_generate
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$472, %rsp                      # imm = 0x1D8
	.cfi_def_cfa_offset 528
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rsi, %r14
	movq	%rdi, %rbx
	cmpl	$0, 64(%rdi)
	je	.LBB44_2
# %bb.1:                                # %iter.check
	leaq	32(%rbx), %r13
	movups	32(%rbx), %xmm0
	movups	48(%rbx), %xmm1
	movaps	%xmm0, (%rsp)
	movaps	%xmm1, 16(%rsp)
	leaq	264(%rsp), %r15
	movaps	.LCPI44_0(%rip), %xmm2          # xmm2 = [1779033703,3144134277,1013904242,2773480762]
	movups	%xmm2, 264(%rsp)
	movaps	.LCPI44_1(%rip), %xmm2          # xmm2 = [1359893119,2600822924,528734635,1541459225]
	movups	%xmm2, 280(%rsp)
	movq	$0, 456(%rsp)
	movaps	.LCPI44_2(%rip), %xmm2          # xmm2 = [92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92]
	xorps	%xmm2, %xmm0
	movaps	%xmm0, (%rsp)
	xorps	%xmm2, %xmm1
	movaps	%xmm1, 16(%rsp)
	movaps	%xmm2, 32(%rsp)
	movaps	%xmm2, 48(%rsp)
	movq	%rsp, %rbp
	movl	$64, %edx
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	secp256k1_sha256_write
	movaps	.LCPI44_0(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	%xmm0, 64(%rsp)
	movaps	.LCPI44_1(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movaps	%xmm0, 80(%rsp)
	movq	$0, 256(%rsp)
	movaps	.LCPI44_3(%rip), %xmm1          # xmm1 = [106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106]
	movaps	(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, (%rsp)
	movaps	16(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 16(%rsp)
	movaps	32(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 32(%rsp)
	movaps	48(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 48(%rsp)
	leaq	64(%rsp), %r12
	movl	$64, %edx
	movq	%r12, %rdi
	movq	%rbp, %rsi
	callq	secp256k1_sha256_write
	movl	$32, %edx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	secp256k1_sha256_write
	leaq	secp256k1_rfc6979_hmac_sha256_generate.zero(%rip), %rsi
	movl	$1, %edx
	movq	%r12, %rdi
	callq	secp256k1_sha256_write
	movq	%r12, %rdi
	movq	%r13, %rsi
	callq	secp256k1_hmac_sha256_finalize
	movups	32(%rbx), %xmm0
	movups	48(%rbx), %xmm1
	movaps	%xmm0, (%rsp)
	movaps	%xmm1, 16(%rsp)
	xorps	%xmm2, %xmm2
	movaps	%xmm2, 48(%rsp)
	movaps	.LCPI44_0(%rip), %xmm2          # xmm2 = [1779033703,3144134277,1013904242,2773480762]
	movups	%xmm2, 264(%rsp)
	movaps	.LCPI44_1(%rip), %xmm2          # xmm2 = [1359893119,2600822924,528734635,1541459225]
	movups	%xmm2, 280(%rsp)
	movq	$0, 456(%rsp)
	movaps	.LCPI44_2(%rip), %xmm2          # xmm2 = [92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92]
	xorps	%xmm2, %xmm0
	movaps	%xmm0, (%rsp)
	xorps	%xmm2, %xmm1
	movaps	%xmm1, 16(%rsp)
	movaps	%xmm2, 32(%rsp)
	xorps	48(%rsp), %xmm2
	movaps	%xmm2, 48(%rsp)
	movq	%rsp, %rbp
	movl	$64, %edx
	movq	%r15, %rdi
	movq	%rbp, %rsi
	callq	secp256k1_sha256_write
	movaps	.LCPI44_0(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	%xmm0, 64(%rsp)
	movaps	.LCPI44_1(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movaps	%xmm0, 80(%rsp)
	movq	$0, 256(%rsp)
	movaps	(%rsp), %xmm0
	movaps	.LCPI44_3(%rip), %xmm1          # xmm1 = [106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106]
	xorps	%xmm1, %xmm0
	movaps	%xmm0, (%rsp)
	movaps	16(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 16(%rsp)
	movaps	32(%rsp), %xmm0
	xorps	%xmm1, %xmm0
	movaps	%xmm0, 32(%rsp)
	xorps	48(%rsp), %xmm1
	movaps	%xmm1, 48(%rsp)
	movl	$64, %edx
	movq	%r12, %rdi
	movq	%rbp, %rsi
	callq	secp256k1_sha256_write
	movl	$32, %edx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	secp256k1_sha256_write
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	secp256k1_hmac_sha256_finalize
.LBB44_2:                               # %iter.check89
	leaq	264(%rsp), %rdi
	movups	32(%rbx), %xmm0
	movups	48(%rbx), %xmm1
	movaps	%xmm0, (%rsp)
	movaps	%xmm1, 16(%rsp)
	movaps	.LCPI44_0(%rip), %xmm2          # xmm2 = [1779033703,3144134277,1013904242,2773480762]
	movups	%xmm2, 264(%rsp)
	movaps	.LCPI44_1(%rip), %xmm2          # xmm2 = [1359893119,2600822924,528734635,1541459225]
	movups	%xmm2, 280(%rsp)
	movq	$0, 456(%rsp)
	movaps	.LCPI44_2(%rip), %xmm2          # xmm2 = [92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92]
	xorps	%xmm2, %xmm0
	movaps	%xmm0, (%rsp)
	xorps	%xmm2, %xmm1
	movaps	%xmm1, 16(%rsp)
	movaps	%xmm2, 32(%rsp)
	movaps	%xmm2, 48(%rsp)
	movq	%rsp, %r15
	movl	$64, %edx
	movq	%r15, %rsi
	callq	secp256k1_sha256_write
	movaps	.LCPI44_0(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	%xmm0, 64(%rsp)
	movaps	.LCPI44_1(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movaps	%xmm0, 80(%rsp)
	movq	$0, 256(%rsp)
	movaps	.LCPI44_3(%rip), %xmm0          # xmm0 = [106,106,106,106,106,106,106,106,106,106,106,106,106,106,106,106]
	movaps	(%rsp), %xmm1
	xorps	%xmm0, %xmm1
	movaps	%xmm1, (%rsp)
	movaps	16(%rsp), %xmm1
	xorps	%xmm0, %xmm1
	movaps	%xmm1, 16(%rsp)
	movaps	32(%rsp), %xmm1
	xorps	%xmm0, %xmm1
	movaps	%xmm1, 32(%rsp)
	xorps	48(%rsp), %xmm0
	movaps	%xmm0, 48(%rsp)
	leaq	64(%rsp), %rbp
	movl	$64, %edx
	movq	%rbp, %rdi
	movq	%r15, %rsi
	callq	secp256k1_sha256_write
	movl	$32, %edx
	movq	%rbp, %rdi
	movq	%rbx, %rsi
	callq	secp256k1_sha256_write
	movq	%rbp, %rdi
	movq	%rbx, %rsi
	callq	secp256k1_hmac_sha256_finalize
	movups	(%rbx), %xmm0
	movups	16(%rbx), %xmm1
	movups	%xmm1, 16(%r14)
	movups	%xmm0, (%r14)
	movl	$1, 64(%rbx)
	addq	$472, %rsp                      # imm = 0x1D8
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end44:
	.size	secp256k1_rfc6979_hmac_sha256_generate, .Lfunc_end44-secp256k1_rfc6979_hmac_sha256_generate
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function secp256k1_hmac_sha256_finalize
	.type	secp256k1_hmac_sha256_finalize,@function
secp256k1_hmac_sha256_finalize:         # @secp256k1_hmac_sha256_finalize
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$56, %rsp
	.cfi_def_cfa_offset 112
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rsi, %r14
	movq	%rdi, %r12
	movq	192(%rdi), %rax
	movq	%rax, %rcx
	movq	%rax, %rsi
	movq	%rax, %r8
	movl	%eax, %r9d
	movl	%eax, %ebp
	movl	%eax, %edi
	movl	%eax, %ebx
	movl	$55, %edx
	subl	%eax, %edx
	shrq	$29, %rax
	shll	$24, %eax
	shrq	$21, %rcx
	andl	$16711680, %ecx                 # imm = 0xFF0000
	orl	%eax, %ecx
	shrq	$37, %rsi
	andl	$65280, %esi                    # imm = 0xFF00
	orl	%ecx, %esi
	shrq	$53, %r8
	movzbl	%r8b, %eax
	orl	%esi, %eax
	movl	%eax, 8(%rsp)
	shll	$27, %r9d
	shll	$11, %ebp
	andl	$16711680, %ebp                 # imm = 0xFF0000
	orl	%r9d, %ebp
	shrl	$5, %edi
	andl	$65280, %edi                    # imm = 0xFF00
	orl	%ebp, %edi
	shrl	$21, %ebx
	movzbl	%bl, %eax
	orl	%edi, %eax
	movl	%eax, 12(%rsp)
	movl	$55, %r15d
	andl	$63, %edx
	addq	$1, %rdx
	leaq	secp256k1_sha256_finalize.pad(%rip), %rbx
	movq	%r12, %rdi
	movq	%rbx, %rsi
	callq	secp256k1_sha256_write
	leaq	8(%rsp), %rsi
	movl	$8, %edx
	movq	%r12, %rdi
	callq	secp256k1_sha256_write
	pxor	%xmm3, %xmm3
	movdqu	(%r12), %xmm0
	movdqu	16(%r12), %xmm1
	movdqa	%xmm0, %xmm2
	punpckhbw	%xmm3, %xmm2            # xmm2 = xmm2[8],xmm3[8],xmm2[9],xmm3[9],xmm2[10],xmm3[10],xmm2[11],xmm3[11],xmm2[12],xmm3[12],xmm2[13],xmm3[13],xmm2[14],xmm3[14],xmm2[15],xmm3[15]
	pshuflw	$27, %xmm2, %xmm2               # xmm2 = xmm2[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm2, %xmm2               # xmm2 = xmm2[0,1,2,3,7,6,5,4]
	punpcklbw	%xmm3, %xmm0            # xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1],xmm0[2],xmm3[2],xmm0[3],xmm3[3],xmm0[4],xmm3[4],xmm0[5],xmm3[5],xmm0[6],xmm3[6],xmm0[7],xmm3[7]
	pshuflw	$27, %xmm0, %xmm0               # xmm0 = xmm0[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm0, %xmm0               # xmm0 = xmm0[0,1,2,3,7,6,5,4]
	packuswb	%xmm2, %xmm0
	movdqu	%xmm3, (%r12)
	movdqa	%xmm1, %xmm2
	punpckhbw	%xmm3, %xmm2            # xmm2 = xmm2[8],xmm3[8],xmm2[9],xmm3[9],xmm2[10],xmm3[10],xmm2[11],xmm3[11],xmm2[12],xmm3[12],xmm2[13],xmm3[13],xmm2[14],xmm3[14],xmm2[15],xmm3[15]
	pshuflw	$27, %xmm2, %xmm2               # xmm2 = xmm2[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm2, %xmm2               # xmm2 = xmm2[0,1,2,3,7,6,5,4]
	punpcklbw	%xmm3, %xmm1            # xmm1 = xmm1[0],xmm3[0],xmm1[1],xmm3[1],xmm1[2],xmm3[2],xmm1[3],xmm3[3],xmm1[4],xmm3[4],xmm1[5],xmm3[5],xmm1[6],xmm3[6],xmm1[7],xmm3[7]
	pshuflw	$27, %xmm1, %xmm1               # xmm1 = xmm1[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm1, %xmm1               # xmm1 = xmm1[0,1,2,3,7,6,5,4]
	packuswb	%xmm2, %xmm1
	movdqu	%xmm3, 16(%r12)
	movdqa	%xmm0, 16(%rsp)
	movdqa	%xmm1, 32(%rsp)
	leaq	200(%r12), %r13
	leaq	16(%rsp), %rsi
	movl	$32, %edx
	movq	%r13, %rdi
	callq	secp256k1_sha256_write
	movq	392(%r12), %rax
	movq	%rax, %rcx
	shrq	$29, %rcx
	shll	$24, %ecx
	movq	%rax, %rdx
	shrq	$21, %rdx
	andl	$16711680, %edx                 # imm = 0xFF0000
	orl	%ecx, %edx
	movq	%rax, %rcx
	shrq	$37, %rcx
	andl	$65280, %ecx                    # imm = 0xFF00
	orl	%edx, %ecx
	movq	%rax, %rdx
	shrq	$53, %rdx
	movzbl	%dl, %edx
	orl	%ecx, %edx
	movl	%edx, 8(%rsp)
	movl	%eax, %ecx
	shll	$27, %ecx
	movl	%eax, %edx
	shll	$11, %edx
	andl	$16711680, %edx                 # imm = 0xFF0000
	orl	%ecx, %edx
	movl	%eax, %ecx
	shrl	$5, %ecx
	andl	$65280, %ecx                    # imm = 0xFF00
	orl	%edx, %ecx
	subl	%eax, %r15d
	shrl	$21, %eax
	movzbl	%al, %eax
	orl	%ecx, %eax
	movl	%eax, 12(%rsp)
	andl	$63, %r15d
	addq	$1, %r15
	movq	%r13, %rdi
	movq	%rbx, %rsi
	movq	%r15, %rdx
	callq	secp256k1_sha256_write
	leaq	8(%rsp), %rsi
	movl	$8, %edx
	movq	%r13, %rdi
	callq	secp256k1_sha256_write
	movdqu	200(%r12), %xmm0
	movdqu	216(%r12), %xmm1
	movdqa	%xmm0, %xmm2
	pxor	%xmm3, %xmm3
	punpckhbw	%xmm3, %xmm2            # xmm2 = xmm2[8],xmm3[8],xmm2[9],xmm3[9],xmm2[10],xmm3[10],xmm2[11],xmm3[11],xmm2[12],xmm3[12],xmm2[13],xmm3[13],xmm2[14],xmm3[14],xmm2[15],xmm3[15]
	pshuflw	$27, %xmm2, %xmm2               # xmm2 = xmm2[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm2, %xmm2               # xmm2 = xmm2[0,1,2,3,7,6,5,4]
	punpcklbw	%xmm3, %xmm0            # xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1],xmm0[2],xmm3[2],xmm0[3],xmm3[3],xmm0[4],xmm3[4],xmm0[5],xmm3[5],xmm0[6],xmm3[6],xmm0[7],xmm3[7]
	pshuflw	$27, %xmm0, %xmm0               # xmm0 = xmm0[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm0, %xmm0               # xmm0 = xmm0[0,1,2,3,7,6,5,4]
	packuswb	%xmm2, %xmm0
	movdqu	%xmm3, 200(%r12)
	movdqa	%xmm1, %xmm2
	punpckhbw	%xmm3, %xmm2            # xmm2 = xmm2[8],xmm3[8],xmm2[9],xmm3[9],xmm2[10],xmm3[10],xmm2[11],xmm3[11],xmm2[12],xmm3[12],xmm2[13],xmm3[13],xmm2[14],xmm3[14],xmm2[15],xmm3[15]
	pshuflw	$27, %xmm2, %xmm2               # xmm2 = xmm2[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm2, %xmm2               # xmm2 = xmm2[0,1,2,3,7,6,5,4]
	punpcklbw	%xmm3, %xmm1            # xmm1 = xmm1[0],xmm3[0],xmm1[1],xmm3[1],xmm1[2],xmm3[2],xmm1[3],xmm3[3],xmm1[4],xmm3[4],xmm1[5],xmm3[5],xmm1[6],xmm3[6],xmm1[7],xmm3[7]
	pxor	%xmm3, %xmm3
	pshuflw	$27, %xmm1, %xmm1               # xmm1 = xmm1[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm1, %xmm1               # xmm1 = xmm1[0,1,2,3,7,6,5,4]
	packuswb	%xmm2, %xmm1
	movdqu	%xmm3, 216(%r12)
	movdqu	%xmm0, (%r14)
	movdqu	%xmm1, 16(%r14)
	addq	$56, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end45:
	.size	secp256k1_hmac_sha256_finalize, .Lfunc_end45-secp256k1_hmac_sha256_finalize
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function secp256k1_sha256_write
	.type	secp256k1_sha256_write,@function
secp256k1_sha256_write:                 # @secp256k1_sha256_write
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$296, %rsp                      # imm = 0x128
	.cfi_def_cfa_offset 352
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rdx, %rbp
	movq	%rsi, %r14
	movq	%rdi, %r15
	movq	192(%rdi), %rcx
	movl	%ecx, %eax
	andl	$63, %eax
	addq	%rdx, %rcx
	movq	%rcx, 192(%rdi)
	leaq	(%rax,%rdx), %rcx
	cmpq	$64, %rcx
	jb	.LBB46_1
# %bb.2:                                # %.lr.ph
	leaq	128(%r15), %rcx
	movq	%rcx, 232(%rsp)                 # 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 208(%rsp)                 # 8-byte Spill
	movq	%r15, 64(%rsp)                  # 8-byte Spill
	.p2align	4, 0x90
.LBB46_3:                               # =>This Inner Loop Header: Depth=1
	movq	232(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax), %rdi
	movl	$64, %ebx
	subq	%rax, %rbx
	movq	%r14, %rsi
	movq	%rbx, %rdx
	callq	memcpy@PLT
	addq	%rbx, %r14
	movq	%r14, 280(%rsp)                 # 8-byte Spill
	subq	%rbx, %rbp
	movq	%rbp, 288(%rsp)                 # 8-byte Spill
	movl	(%r15), %r13d
	movl	4(%r15), %r9d
	movl	8(%r15), %ebp
	movl	12(%r15), %r10d
	movq	%r10, 272(%rsp)                 # 8-byte Spill
	movl	16(%r15), %r8d
	movl	20(%r15), %r14d
	movl	24(%r15), %r12d
	movl	28(%r15), %edi
	movl	%edi, 204(%rsp)                 # 4-byte Spill
	movl	%r8d, %eax
	roll	$26, %eax
	movl	%r8d, %edx
	roll	$21, %edx
	movl	%r8d, %ecx
	roll	$7, %ecx
	xorl	%eax, %edx
	movl	%r12d, %eax
	xorl	%r14d, %eax
	andl	%r8d, %eax
	movq	%r8, 216(%rsp)                  # 8-byte Spill
	xorl	%r12d, %eax
	movq	%r12, 264(%rsp)                 # 8-byte Spill
	movl	128(%r15), %ebx
	movl	%r13d, %esi
	roll	$30, %esi
	xorl	%edx, %ecx
	bswapl	%ebx
	movl	%r13d, %edx
	roll	$19, %edx
	addl	%edi, %ecx
	xorl	%esi, %edx
	movl	%r13d, %esi
	roll	$10, %esi
	addl	%eax, %ecx
	xorl	%edx, %esi
	movq	%r9, %rax
	movq	%r9, 224(%rsp)                  # 8-byte Spill
	movl	%eax, %edx
	andl	%r13d, %edx
                                        # kill: def $eax killed $eax killed $rax def $rax
	orl	%r13d, %eax
	movq	%rbx, 160(%rsp)                 # 8-byte Spill
	leal	(%rbx,%rcx), %edi
	addl	$1116352408, %edi               # imm = 0x428A2F98
                                        # kill: def $ecx killed $ecx killed $rcx def $rcx
	andl	%ebp, %eax
	movq	%rbp, %r11
	movq	%rbp, 256(%rsp)                 # 8-byte Spill
	orl	%edx, %eax
	addl	%esi, %eax
	addl	%ebx, %ecx
	addl	%edi, %eax
	leal	(%r10,%rcx), %r9d
	addl	$1116352408, %r9d               # imm = 0x428A2F98
	movl	%r9d, %ecx
	roll	$26, %ecx
	movl	%r9d, %edx
	roll	$21, %edx
	movl	%r9d, %r10d
	roll	$7, %r10d
	xorl	%ecx, %edx
	movl	%r14d, %ecx
	xorl	%r8d, %ecx
	andl	%r9d, %ecx
	xorl	%edx, %r10d
	movl	132(%r15), %ebp
	movl	%eax, %edx
	roll	$30, %edx
	xorl	%r14d, %ecx
	movq	%r14, %rsi
	movq	%r14, 248(%rsp)                 # 8-byte Spill
	movq	%rbp, %rdi
	bswapl	%edi
	movq	%rdi, 120(%rsp)                 # 8-byte Spill
	movl	%eax, %ebp
	roll	$19, %ebp
	addl	%r12d, %ecx
	addl	%edi, %ecx
	xorl	%edx, %ebp
	leal	(%rcx,%r10), %edx
	leal	(%r10,%rcx), %edi
	addl	$1899447441, %edi               # imm = 0x71374491
	movl	%eax, %ebx
	roll	$10, %ebx
	xorl	%ebp, %ebx
	movl	%eax, %ebp
	andl	%r13d, %ebp
	movl	%eax, %ecx
	orl	%r13d, %ecx
	movq	%r13, %r14
	movq	224(%rsp), %r13                 # 8-byte Reload
	andl	%r13d, %ecx
	orl	%ebp, %ecx
	addl	%ebx, %ecx
	addl	%edi, %ecx
	leal	(%r11,%rdx), %r10d
	addl	$1899447441, %r10d              # imm = 0x71374491
	movl	%r10d, %edx
	roll	$26, %edx
	movl	%r10d, %edi
	roll	$21, %edi
	movl	%r10d, %r8d
	roll	$7, %r8d
	xorl	%edx, %edi
	xorl	%edi, %r8d
	movl	%r9d, %edx
	movq	216(%rsp), %r12                 # 8-byte Reload
	xorl	%r12d, %edx
	andl	%r10d, %edx
	xorl	%r12d, %edx
	movl	%ecx, %edi
	roll	$30, %edi
	movl	136(%r15), %ebp
	bswapl	%ebp
	movq	%rbp, 104(%rsp)                 # 8-byte Spill
	movl	%ecx, %ebx
	roll	$19, %ebx
	addl	%ebp, %esi
	addl	%esi, %edx
	xorl	%edi, %ebx
	leal	(%rdx,%r8), %esi
	addl	%r8d, %edx
	addl	$-1245643825, %edx              # imm = 0xB5C0FBCF
	movl	%ecx, %edi
	roll	$10, %edi
	xorl	%ebx, %edi
	movl	%ecx, %ebp
	andl	%eax, %ebp
	movl	%ecx, %r11d
	orl	%eax, %r11d
	andl	%r14d, %r11d
	movq	%r14, 240(%rsp)                 # 8-byte Spill
	orl	%ebp, %r11d
	addl	%edi, %r11d
	addl	%edx, %r11d
	leal	(%rsi,%r13), %r15d
	addl	$-1245643825, %r15d             # imm = 0xB5C0FBCF
	movl	%r15d, %esi
	roll	$26, %esi
	movl	%r15d, %edi
	roll	$21, %edi
	movl	%r15d, %r8d
	roll	$7, %r8d
	xorl	%esi, %edi
	xorl	%edi, %r8d
	movl	%r10d, %esi
	xorl	%r9d, %esi
	andl	%r15d, %esi
	xorl	%r9d, %esi
	movl	%r11d, %edi
	roll	$30, %edi
	movq	64(%rsp), %rbp                  # 8-byte Reload
	movl	140(%rbp), %edx
	bswapl	%edx
	movq	%rdx, 72(%rsp)                  # 8-byte Spill
	movl	%r11d, %ebp
	roll	$19, %ebp
	addl	%r12d, %esi
	addl	%edx, %esi
	xorl	%edi, %ebp
	leal	(%rsi,%r8), %edi
	leal	(%r8,%rsi), %edx
	addl	$-373957723, %edx               # imm = 0xE9B5DBA5
	movl	%r11d, %esi
	roll	$10, %esi
	xorl	%ebp, %esi
	movl	%r11d, %ebx
	andl	%ecx, %ebx
	movl	%r11d, %r8d
	orl	%ecx, %r8d
	andl	%eax, %r8d
	orl	%ebx, %r8d
	addl	%esi, %r8d
	addl	%edx, %r8d
	leal	(%r14,%rdi), %r12d
	addl	$-373957723, %r12d              # imm = 0xE9B5DBA5
	movl	%r12d, %edx
	roll	$26, %edx
	movl	%r12d, %esi
	roll	$21, %esi
	movl	%r12d, %edi
	roll	$7, %edi
	xorl	%edx, %esi
	xorl	%esi, %edi
	movl	%r15d, %edx
	xorl	%r10d, %edx
	andl	%r12d, %edx
	movq	64(%rsp), %rsi                  # 8-byte Reload
	movl	144(%rsi), %esi
	bswapl	%esi
	movl	%esi, 32(%rsp)                  # 4-byte Spill
	addl	%esi, %r9d
	xorl	%r10d, %edx
	addl	%edx, %r9d
	leal	(%rdi,%r9), %edx
	addl	$961987163, %edx                # imm = 0x3956C25B
	movl	%r8d, %esi
	roll	$30, %esi
	movl	%r8d, %edi
	roll	$19, %edi
	movl	%r8d, %ebp
	roll	$10, %ebp
	xorl	%esi, %edi
	xorl	%edi, %ebp
	movl	%r8d, %esi
	andl	%r11d, %esi
	movl	%r8d, %edi
	orl	%r11d, %edi
	andl	%ecx, %edi
	orl	%esi, %edi
	addl	%ebp, %edi
	addl	%edx, %eax
	movl	%eax, %esi
	roll	$26, %esi
	movl	%eax, %ebp
	roll	$21, %ebp
	addl	%edx, %edi
	xorl	%esi, %ebp
	movl	%r12d, %edx
	xorl	%r15d, %edx
	andl	%eax, %edx
	movq	64(%rsp), %rsi                  # 8-byte Reload
	movl	148(%rsi), %esi
	bswapl	%esi
	movl	%esi, 88(%rsp)                  # 4-byte Spill
	addl	%esi, %r10d
	xorl	%r15d, %edx
	addl	%edx, %r10d
	movl	%eax, %edx
	roll	$7, %edx
	xorl	%ebp, %edx
	movl	%edi, %esi
	roll	$30, %esi
	movl	%edi, %ebp
	roll	$19, %ebp
	addl	%r10d, %edx
	addl	$1508970993, %edx               # imm = 0x59F111F1
	xorl	%esi, %ebp
	movl	%edi, %esi
	roll	$10, %esi
	xorl	%ebp, %esi
	movl	%edi, %ebx
	andl	%r8d, %ebx
	movl	%edi, %r9d
	orl	%r8d, %r9d
	andl	%r11d, %r9d
	orl	%ebx, %r9d
	addl	%esi, %r9d
	addl	%edx, %ecx
	addl	%edx, %r9d
	movl	%ecx, %edx
	roll	$26, %edx
	movl	%ecx, %esi
	roll	$21, %esi
	xorl	%edx, %esi
	movl	%eax, %edx
	xorl	%r12d, %edx
	andl	%ecx, %edx
	xorl	%r12d, %edx
	movq	64(%rsp), %rbp                  # 8-byte Reload
	movl	152(%rbp), %ebp
	bswapl	%ebp
	movl	%ebp, 96(%rsp)                  # 4-byte Spill
	addl	%ebp, %r15d
	addl	%edx, %r15d
	movl	%r9d, %edx
	roll	$30, %edx
	movl	%r9d, %ebx
	roll	$19, %ebx
	xorl	%edx, %ebx
	movl	%ecx, %edx
	roll	$7, %edx
	xorl	%esi, %edx
	movl	%r9d, %esi
	roll	$10, %esi
	xorl	%ebx, %esi
	movl	%r9d, %ebx
	andl	%edi, %ebx
	movl	%r9d, %r14d
	orl	%edi, %r14d
	andl	%r8d, %r14d
	orl	%ebx, %r14d
	addl	%r15d, %edx
	addl	$-1841331548, %edx              # imm = 0x923F82A4
	addl	%esi, %r14d
	addl	%edx, %r11d
	movl	%r11d, %esi
	roll	$26, %esi
	addl	%edx, %r14d
	movl	%r11d, %edx
	roll	$21, %edx
	xorl	%esi, %edx
	movl	%r11d, %esi
	roll	$7, %esi
	xorl	%edx, %esi
	movl	%ecx, %edx
	xorl	%eax, %edx
	andl	%r11d, %edx
	movq	64(%rsp), %rbp                  # 8-byte Reload
	movl	156(%rbp), %ebp
	bswapl	%ebp
	movl	%ebp, 12(%rsp)                  # 4-byte Spill
	xorl	%eax, %edx
	addl	%ebp, %r12d
	addl	%edx, %r12d
	movl	%r14d, %edx
	roll	$30, %edx
	movl	%r14d, %ebx
	roll	$19, %ebx
	xorl	%edx, %ebx
	leal	(%rsi,%r12), %edx
	addl	$-1424204075, %edx              # imm = 0xAB1C5ED5
	movl	%r14d, %esi
	roll	$10, %esi
	xorl	%ebx, %esi
	movl	%r14d, %ebx
	andl	%r9d, %ebx
	movl	%r14d, %ebp
	orl	%r9d, %ebp
	andl	%edi, %ebp
	orl	%ebx, %ebp
	addl	%esi, %ebp
	addl	%edx, %r8d
	addl	%edx, %ebp
	movq	%rbp, %r15
	movl	%r8d, %edx
	roll	$26, %edx
	movl	%r8d, %esi
	roll	$21, %esi
	movl	%r8d, %ebx
	roll	$7, %ebx
	xorl	%edx, %esi
	xorl	%esi, %ebx
	movl	%r11d, %edx
	xorl	%ecx, %edx
	andl	%r8d, %edx
	movq	64(%rsp), %rsi                  # 8-byte Reload
	movl	160(%rsi), %esi
	bswapl	%esi
	movl	%esi, 16(%rsp)                  # 4-byte Spill
	addl	%esi, %eax
	xorl	%ecx, %edx
	addl	%edx, %eax
	addl	%ebx, %eax
	addl	$-670586216, %eax               # imm = 0xD807AA98
	movl	%r15d, %edx
	roll	$30, %edx
	movl	%r15d, %esi
	roll	$19, %esi
	movl	%r15d, %ebx
	roll	$10, %ebx
	xorl	%edx, %esi
	xorl	%esi, %ebx
	movl	%r15d, %esi
	andl	%r14d, %esi
	movl	%r15d, %edx
	orl	%r14d, %edx
	andl	%r9d, %edx
	orl	%esi, %edx
	addl	%ebx, %edx
	movq	%rdx, %rbp
	addl	%eax, %edi
	movl	%edi, %esi
	roll	$26, %esi
	movl	%edi, %ebx
	roll	$21, %ebx
	addl	%eax, %ebp
	xorl	%esi, %ebx
	movl	%r8d, %eax
	xorl	%r11d, %eax
	andl	%edi, %eax
	movq	64(%rsp), %rdx                  # 8-byte Reload
	movl	164(%rdx), %edx
	bswapl	%edx
	movl	%edx, 48(%rsp)                  # 4-byte Spill
	addl	%edx, %ecx
	xorl	%r11d, %eax
	addl	%eax, %ecx
	movl	%edi, %eax
	roll	$7, %eax
	xorl	%ebx, %eax
	movl	%ebp, %esi
	roll	$30, %esi
	movl	%ebp, %ebx
	roll	$19, %ebx
	addl	%ecx, %eax
	addl	$310598401, %eax                # imm = 0x12835B01
	xorl	%esi, %ebx
	movl	%ebp, %esi
	roll	$10, %esi
	xorl	%ebx, %esi
	movl	%ebp, %ebx
	andl	%r15d, %ebx
	movl	%ebp, %r12d
	movq	%rbp, %rdx
	orl	%r15d, %r12d
	andl	%r14d, %r12d
	orl	%ebx, %r12d
	addl	%esi, %r12d
	addl	%eax, %r9d
	addl	%eax, %r12d
	movl	%r9d, %eax
	roll	$26, %eax
	movl	%r9d, %esi
	roll	$21, %esi
	xorl	%eax, %esi
	movl	%edi, %eax
	xorl	%r8d, %eax
	andl	%r9d, %eax
	xorl	%r8d, %eax
	movq	64(%rsp), %rcx                  # 8-byte Reload
	movl	168(%rcx), %ecx
	bswapl	%ecx
	movl	%ecx, 24(%rsp)                  # 4-byte Spill
	addl	%ecx, %r11d
	addl	%eax, %r11d
	movl	%r12d, %eax
	roll	$30, %eax
	movl	%r12d, %ebx
	roll	$19, %ebx
	xorl	%eax, %ebx
	movl	%r9d, %ebp
	roll	$7, %ebp
	xorl	%esi, %ebp
	movl	%r12d, %esi
	roll	$10, %esi
	xorl	%ebx, %esi
	movl	%r12d, %ebx
	andl	%edx, %ebx
	movl	%r12d, %r13d
	orl	%edx, %r13d
	andl	%r15d, %r13d
	orl	%ebx, %r13d
	addl	%r11d, %ebp
	addl	$607225278, %ebp                # imm = 0x243185BE
	addl	%esi, %r13d
	addl	%ebp, %r14d
	movl	%r14d, %esi
	roll	$26, %esi
	addl	%ebp, %r13d
	movl	%r14d, %ebp
	roll	$21, %ebp
	xorl	%esi, %ebp
	movl	%r14d, %esi
	roll	$7, %esi
	xorl	%ebp, %esi
	movl	%r9d, %ebp
	xorl	%edi, %ebp
	andl	%r14d, %ebp
	movq	64(%rsp), %rax                  # 8-byte Reload
	movl	172(%rax), %eax
	bswapl	%eax
	movl	%eax, 20(%rsp)                  # 4-byte Spill
	xorl	%edi, %ebp
	addl	%eax, %r8d
	addl	%ebp, %r8d
	movl	%r13d, %ebp
	roll	$30, %ebp
	movl	%r13d, %ebx
	roll	$19, %ebx
	xorl	%ebp, %ebx
	addl	%r8d, %esi
	addl	$1426881987, %esi               # imm = 0x550C7DC3
	movl	%r13d, %ebp
	roll	$10, %ebp
	xorl	%ebx, %ebp
	movl	%r13d, %ebx
	andl	%r12d, %ebx
	movl	%r13d, %eax
	orl	%r12d, %eax
	andl	%edx, %eax
	orl	%ebx, %eax
	addl	%ebp, %eax
	addl	%esi, %r15d
	addl	%esi, %eax
	movq	%rax, %rcx
	movl	%r15d, %esi
	roll	$26, %esi
	movl	%r15d, %ebp
	roll	$21, %ebp
	movl	%r15d, %ebx
	movq	%r15, %r8
	roll	$7, %ebx
	xorl	%esi, %ebp
	xorl	%ebp, %ebx
	movl	%r14d, %esi
	xorl	%r9d, %esi
	andl	%r8d, %esi
	movq	64(%rsp), %rax                  # 8-byte Reload
	movl	176(%rax), %eax
	bswapl	%eax
	movl	%eax, 4(%rsp)                   # 4-byte Spill
	addl	%eax, %edi
	xorl	%r9d, %esi
	addl	%esi, %edi
	leal	(%rbx,%rdi), %esi
	addl	$1925078388, %esi               # imm = 0x72BE5D74
	movl	%ecx, %edi
	roll	$30, %edi
	movl	%ecx, %ebp
	roll	$19, %ebp
	movl	%ecx, %ebx
	roll	$10, %ebx
	xorl	%edi, %ebp
	xorl	%ebp, %ebx
	movl	%ecx, %edi
	andl	%r13d, %edi
	movl	%ecx, %eax
	movq	%rcx, %r11
	orl	%r13d, %eax
	andl	%r12d, %eax
	orl	%edi, %eax
	addl	%ebx, %eax
	movq	%rax, %rcx
	addl	%esi, %edx
	movl	%edx, %edi
	roll	$26, %edi
	movl	%edx, %ebp
	movq	%rdx, %r10
	roll	$21, %ebp
	addl	%esi, %ecx
	xorl	%edi, %ebp
	movl	%r8d, %esi
	xorl	%r14d, %esi
	andl	%r10d, %esi
	movq	64(%rsp), %rax                  # 8-byte Reload
	movl	180(%rax), %eax
	bswapl	%eax
	movl	%eax, 8(%rsp)                   # 4-byte Spill
	addl	%eax, %r9d
	xorl	%r14d, %esi
	addl	%esi, %r9d
	movl	%r10d, %esi
	roll	$7, %esi
	xorl	%ebp, %esi
	movl	%ecx, %edi
	roll	$30, %edi
	movl	%ecx, %ebp
	roll	$19, %ebp
	addl	%r9d, %esi
	addl	$-2132889090, %esi              # imm = 0x80DEB1FE
	xorl	%edi, %ebp
	movl	%ecx, %edi
	roll	$10, %edi
	xorl	%ebp, %edi
	movl	%ecx, %ebp
	movq	%rcx, %r9
	andl	%r11d, %ebp
	movl	%r9d, %eax
	orl	%r11d, %eax
	movq	%r11, 152(%rsp)                 # 8-byte Spill
	andl	%r13d, %eax
	orl	%ebp, %eax
	addl	%edi, %eax
	addl	%esi, %r12d
	addl	%esi, %eax
	movq	%rax, %rcx
	movl	%r12d, %esi
	roll	$26, %esi
	movl	%r12d, %edi
	roll	$21, %edi
	xorl	%esi, %edi
	movl	%r10d, %esi
	xorl	%r8d, %esi
	andl	%r12d, %esi
	xorl	%r8d, %esi
	movq	64(%rsp), %rax                  # 8-byte Reload
	movl	184(%rax), %eax
	bswapl	%eax
	addl	%eax, %r14d
	addl	%esi, %r14d
	movl	%ecx, %esi
	roll	$30, %esi
	movl	%ecx, %ebx
	roll	$19, %ebx
	xorl	%esi, %ebx
	movl	%r12d, %esi
	roll	$7, %esi
	xorl	%edi, %esi
	movl	%ecx, %ebp
	roll	$10, %ebp
	xorl	%ebx, %ebp
	movq	%rcx, %r15
	movq	%rcx, 56(%rsp)                  # 8-byte Spill
	movl	%r15d, %edi
	andl	%r9d, %edi
                                        # kill: def $r15d killed $r15d killed $r15 def $r15
	orl	%r9d, %r15d
	andl	%r11d, %r15d
	orl	%edi, %r15d
	leal	(%rsi,%r14), %edi
	addl	$-1680079193, %edi              # imm = 0x9BDC06A7
	movl	%eax, 52(%rsp)                  # 4-byte Spill
	movl	%eax, %esi
	roll	$13, %esi
	movl	%eax, %ebx
	roll	$15, %ebx
	xorl	%ebx, %esi
	shrl	$10, %eax
	xorl	%esi, %eax
	movl	%eax, 144(%rsp)                 # 4-byte Spill
	addl	%ebp, %r15d
	movq	120(%rsp), %rax                 # 8-byte Reload
	movl	%eax, %ecx
	roll	$14, %ecx
	movl	%ecx, %ebx
	movq	104(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, %esi
	roll	$25, %esi
	movl	%ecx, %edx
	roll	$14, %edx
	movl	%eax, %ebp
	roll	$25, %ebp
	xorl	%ebp, %ebx
	movl	%ebx, 192(%rsp)                 # 4-byte Spill
	xorl	%esi, %edx
	movl	%edx, 184(%rsp)                 # 4-byte Spill
	addl	%edi, %r13d
	addl	%edi, %r15d
	movq	72(%rsp), %rax                  # 8-byte Reload
	movl	%eax, %ecx
	roll	$14, %ecx
	movl	%ecx, %edx
	movl	32(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, %esi
	roll	$25, %esi
	roll	$14, %ecx
	movl	%eax, %edi
	roll	$25, %edi
	xorl	%edi, %edx
	movl	%edx, 136(%rsp)                 # 4-byte Spill
	xorl	%esi, %ecx
	movl	%ecx, 132(%rsp)                 # 4-byte Spill
	movl	88(%rsp), %eax                  # 4-byte Reload
	movl	%eax, %esi
	roll	$25, %esi
	movl	%eax, %ecx
	roll	$14, %ecx
	movl	%r13d, %edi
	roll	$26, %edi
	xorl	%esi, %ecx
	movl	%ecx, 140(%rsp)                 # 4-byte Spill
	movl	%r13d, %eax
	roll	$21, %eax
	xorl	%edi, %eax
	movl	%eax, 28(%rsp)                  # 4-byte Spill
	movl	96(%rsp), %esi                  # 4-byte Reload
	movl	%esi, %ecx
	roll	$14, %ecx
	roll	$25, %esi
	xorl	%esi, %ecx
	movl	%ecx, 84(%rsp)                  # 4-byte Spill
	movl	%r12d, %esi
	movq	%r10, 176(%rsp)                 # 8-byte Spill
	xorl	%r10d, %esi
	andl	%r13d, %esi
	xorl	%r10d, %esi
	movq	64(%rsp), %rax                  # 8-byte Reload
	movl	188(%rax), %eax
	bswapl	%eax
	movl	%eax, 112(%rsp)                 # 4-byte Spill
	addl	%eax, %r8d
	addl	%esi, %r8d
	movq	%r8, 168(%rsp)                  # 8-byte Spill
	movl	12(%rsp), %r11d                 # 4-byte Reload
	movl	%r11d, %ebp
	roll	$14, %ebp
	movl	16(%rsp), %r10d                 # 4-byte Reload
	movl	%r10d, %esi
	roll	$25, %esi
	movl	%r10d, %ebx
	roll	$14, %ebx
	movl	%r11d, %edi
	roll	$25, %edi
	xorl	%edi, %ebp
	xorl	%esi, %ebx
	movl	48(%rsp), %r8d                  # 4-byte Reload
	movl	%r8d, %esi
	roll	$25, %esi
	movl	%r8d, %eax
	roll	$14, %eax
	movq	%r15, %rdi
	movl	%edi, %ecx
	roll	$30, %ecx
	xorl	%esi, %eax
	roll	$19, %r15d
	xorl	%ecx, %r15d
	movl	%edi, %edx
	movq	56(%rsp), %r14                  # 8-byte Reload
	andl	%r14d, %edx
	movl	%edi, %esi
	orl	%r14d, %esi
	andl	%r9d, %esi
	orl	%edx, %esi
	movl	%r8d, %ecx
	shrl	$3, %ecx
	xorl	%eax, %ecx
	addl	%r10d, %ecx
	movl	%ecx, 44(%rsp)                  # 4-byte Spill
	shrl	$3, %r10d
	xorl	%ebx, %r10d
	movl	%r13d, %eax
	roll	$7, %eax
	xorl	28(%rsp), %eax                  # 4-byte Folded Reload
	addl	%r11d, %r10d
	movl	%r10d, 28(%rsp)                 # 4-byte Spill
	shrl	$3, %r11d
	xorl	%ebp, %r11d
	movl	96(%rsp), %ecx                  # 4-byte Reload
	addl	%ecx, %r11d
	movl	%r11d, 116(%rsp)                # 4-byte Spill
	shrl	$3, %ecx
	xorl	84(%rsp), %ecx                  # 4-byte Folded Reload
	movl	88(%rsp), %edx                  # 4-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 84(%rsp)                  # 4-byte Spill
	shrl	$3, %edx
	xorl	140(%rsp), %edx                 # 4-byte Folded Reload
	movl	%edi, %ecx
	roll	$10, %ecx
	xorl	%r15d, %ecx
	movl	32(%rsp), %ebx                  # 4-byte Reload
	addl	%ebx, %edx
	movl	%edx, 140(%rsp)                 # 4-byte Spill
	shrl	$3, %ebx
	xorl	132(%rsp), %ebx                 # 4-byte Folded Reload
	movq	168(%rsp), %rdx                 # 8-byte Reload
	addl	%edx, %eax
	addl	$-1046744716, %eax              # imm = 0xC19BF174
	movq	72(%rsp), %rbp                  # 8-byte Reload
	addl	%ebp, %ebx
	movl	%ebx, 16(%rsp)                  # 4-byte Spill
	movl	%ebp, %edx
	shrl	$3, %edx
	xorl	136(%rsp), %edx                 # 4-byte Folded Reload
	movq	%rsi, %rbx
	addl	%ecx, %ebx
	movq	104(%rsp), %rcx                 # 8-byte Reload
	addl	%ecx, %edx
	movl	%edx, 12(%rsp)                  # 4-byte Spill
	movl	%ecx, %r10d
	shrl	$3, %r10d
	xorl	184(%rsp), %r10d                # 4-byte Folded Reload
	movq	120(%rsp), %rcx                 # 8-byte Reload
	addl	%ecx, %r10d
	movl	%ecx, %ebp
	shrl	$3, %ebp
	xorl	192(%rsp), %ebp                 # 4-byte Folded Reload
	movq	152(%rsp), %r15                 # 8-byte Reload
	addl	%eax, %r15d
	addl	%eax, %ebx
	addl	160(%rsp), %ebp                 # 4-byte Folded Reload
	addl	%r8d, %ebp
	movl	%r15d, %eax
	roll	$26, %eax
	addl	144(%rsp), %ebp                 # 4-byte Folded Reload
	movl	%ebp, 104(%rsp)                 # 4-byte Spill
	movl	%r15d, %ecx
	roll	$21, %ecx
	xorl	%eax, %ecx
	movl	%r13d, %eax
	xorl	%r12d, %eax
	andl	%r15d, %eax
	movq	176(%rsp), %rdx                 # 8-byte Reload
	addl	%ebp, %edx
	xorl	%r12d, %eax
	addl	%eax, %edx
	movl	%r15d, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movl	%ebx, %ecx
	roll	$30, %ecx
	movl	%ebx, %esi
	roll	$19, %esi
	addl	%edx, %eax
	addl	$-459576895, %eax               # imm = 0xE49B69C1
	xorl	%ecx, %esi
	movl	%ebx, %ecx
	roll	$10, %ecx
	xorl	%esi, %ecx
	movl	%ebx, %esi
	movq	%rdi, %r8
	andl	%r8d, %esi
	movl	%ebx, %ebp
	movq	%rbx, %rdx
	orl	%r8d, %ebp
	movq	%rdi, %rbx
	andl	%r14d, %ebp
	orl	%esi, %ebp
	addl	%ecx, %ebp
	movq	%r9, %r11
	addl	%eax, %r11d
	addl	%eax, %ebp
	movl	%r11d, %eax
	roll	$26, %eax
	movl	%r11d, %ecx
	roll	$21, %ecx
	xorl	%eax, %ecx
	movl	%r11d, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movl	112(%rsp), %edi                 # 4-byte Reload
	movl	%edi, %ecx
	roll	$13, %ecx
	movl	%edi, %esi
	roll	$15, %esi
	xorl	%esi, %ecx
	movl	%edi, %esi
	shrl	$10, %esi
	xorl	%ecx, %esi
	addl	24(%rsp), %r10d                 # 4-byte Folded Reload
	addl	%esi, %r10d
	movl	%r15d, %ecx
	xorl	%r13d, %ecx
	andl	%r11d, %ecx
	xorl	%r13d, %ecx
	addl	%r10d, %r12d
	addl	%ecx, %r12d
	movl	%ebp, %ecx
	roll	$30, %ecx
	movl	%ebp, %esi
	roll	$19, %esi
	xorl	%ecx, %esi
	leal	(%rax,%r12), %r9d
	addl	$-272742522, %r9d               # imm = 0xEFBE4786
	movl	%ebp, %ecx
	roll	$10, %ecx
	xorl	%esi, %ecx
	movl	%ebp, %esi
	movq	%rdx, %rdi
	andl	%edi, %esi
	movl	%ebp, %eax
	movq	%rbp, %r12
	orl	%edi, %eax
	movq	%rdx, 184(%rsp)                 # 8-byte Spill
	andl	%ebx, %eax
	orl	%esi, %eax
	addl	%ecx, %eax
	movq	%r14, %rsi
	addl	%r9d, %esi
	addl	%r9d, %eax
	movl	104(%rsp), %r8d                 # 4-byte Reload
	movl	%r8d, %ebp
	roll	$15, %ebp
	movl	%r8d, %ecx
	roll	$13, %ecx
	xorl	%ebp, %ecx
	movl	%r8d, %ebp
	shrl	$10, %ebp
	xorl	%ecx, %ebp
	movl	12(%rsp), %edx                  # 4-byte Reload
	addl	20(%rsp), %edx                  # 4-byte Folded Reload
	movl	%esi, %ecx
	roll	$26, %ecx
	addl	%ebp, %edx
	movl	%edx, 12(%rsp)                  # 4-byte Spill
	movl	%esi, %ebp
	roll	$21, %ebp
	xorl	%ecx, %ebp
	movl	%r11d, %ecx
	xorl	%r15d, %ecx
	andl	%esi, %ecx
	addl	%edx, %r13d
	xorl	%r15d, %ecx
	movq	%r15, %r14
	addl	%ecx, %r13d
	movl	%esi, %ecx
	movq	%rsi, %r8
	movq	%rsi, 56(%rsp)                  # 8-byte Spill
	roll	$7, %ecx
	xorl	%ebp, %ecx
	movl	%eax, %ebp
	roll	$30, %ebp
	movl	%eax, %esi
	roll	$19, %esi
	leal	(%rcx,%r13), %r9d
	addl	$264347078, %r9d                # imm = 0xFC19DC6
	xorl	%ebp, %esi
	movl	%eax, %ecx
	roll	$10, %ecx
	xorl	%esi, %ecx
	movl	%eax, %edx
	movq	%r12, %r13
	andl	%r13d, %edx
	movl	%eax, %ebp
	movq	%rax, %r12
	orl	%r13d, %ebp
	movq	%r13, 96(%rsp)                  # 8-byte Spill
	andl	%edi, %ebp
	orl	%edx, %ebp
	addl	%ecx, %ebp
	addl	%r9d, %ebx
	addl	%r9d, %ebp
	movl	%r10d, %eax
	roll	$13, %eax
	movl	%r10d, %ecx
	roll	$15, %ecx
	xorl	%ecx, %eax
	movl	%r10d, %edx
	movl	%r10d, %r15d
	shrl	$10, %edx
	movl	%ebx, %esi
	roll	$26, %esi
	xorl	%eax, %edx
	movl	%ebx, %ecx
	roll	$21, %ecx
	xorl	%esi, %ecx
	movl	4(%rsp), %r9d                   # 4-byte Reload
	movl	16(%rsp), %esi                  # 4-byte Reload
	addl	%r9d, %esi
	addl	%edx, %esi
	movl	%esi, 16(%rsp)                  # 4-byte Spill
	movl	%r8d, %eax
	movq	%r11, 192(%rsp)                 # 8-byte Spill
	xorl	%r11d, %eax
	andl	%ebx, %eax
	movq	%rbx, 32(%rsp)                  # 8-byte Spill
	addl	%esi, %r14d
	movl	%ebp, %edx
	roll	$30, %edx
	xorl	%r11d, %eax
	addl	%eax, %r14d
	movl	%ebp, %eax
	roll	$19, %eax
	xorl	%edx, %eax
	movl	%ebp, %edx
	movq	%r12, 72(%rsp)                  # 8-byte Spill
	andl	%r12d, %edx
	movl	%ebp, %esi
	movq	%rbp, 144(%rsp)                 # 8-byte Spill
	orl	%r12d, %esi
	andl	%r13d, %esi
	orl	%edx, %esi
	movq	%rsi, 88(%rsp)                  # 8-byte Spill
	movl	24(%rsp), %edx                  # 4-byte Reload
	movl	%edx, %esi
	roll	$14, %esi
	roll	$25, %edx
	xorl	%edx, %esi
	movl	%esi, 136(%rsp)                 # 4-byte Spill
                                        # kill: def $ebx killed $ebx killed $rbx def $rbx
	roll	$7, %ebx
	xorl	%ecx, %ebx
	movl	20(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, %esi
	roll	$14, %esi
	roll	$25, %ecx
	xorl	%ecx, %esi
	movl	%esi, 132(%rsp)                 # 4-byte Spill
	movl	%ebp, %ecx
	roll	$10, %ecx
	xorl	%eax, %ecx
	movl	%ecx, 176(%rsp)                 # 4-byte Spill
	movl	%r9d, %edx
	roll	$14, %r9d
	movl	8(%rsp), %r13d                  # 4-byte Reload
	movl	%r13d, %eax
	roll	$25, %eax
	roll	$14, %r13d
	movl	%edx, %ecx
	roll	$25, %ecx
	xorl	%ecx, %r9d
	xorl	%eax, %r13d
	movl	52(%rsp), %edx                  # 4-byte Reload
	movl	%edx, %eax
	roll	$25, %eax
	roll	$14, %edx
	movl	12(%rsp), %r12d                 # 4-byte Reload
	movl	%r12d, %ecx
	roll	$15, %ecx
	xorl	%eax, %edx
	movl	%r12d, %eax
	roll	$13, %eax
	xorl	%ecx, %eax
	movl	%eax, 168(%rsp)                 # 4-byte Spill
	movl	112(%rsp), %edi                 # 4-byte Reload
	movl	%edi, %ecx
	roll	$25, %ecx
	movl	%edi, %eax
	roll	$14, %eax
	movl	104(%rsp), %r11d                # 4-byte Reload
	movl	%r11d, %esi
	roll	$25, %esi
	movl	%r11d, %r10d
	roll	$14, %r10d
	xorl	%ecx, %eax
	xorl	%esi, %r10d
	movl	%r15d, %ecx
	roll	$25, %ecx
	movl	%r15d, %esi
	roll	$14, %esi
	xorl	%ecx, %esi
	leal	(%rbx,%r14), %ecx
	addl	$604807628, %ecx                # imm = 0x240CA1CC
	movl	%r12d, %ebx
	roll	$25, %ebx
	movl	%r12d, %ebp
	roll	$14, %ebp
	xorl	%ebx, %ebp
	movl	%r12d, %ebx
	shrl	$3, %ebx
	xorl	%ebp, %ebx
	addl	%r15d, 44(%rsp)                 # 4-byte Folded Spill
	addl	%r15d, %ebx
	movl	%ebx, 160(%rsp)                 # 4-byte Spill
	movl	%r15d, %r8d
	shrl	$3, %r8d
	xorl	%esi, %r8d
	addl	%r11d, 28(%rsp)                 # 4-byte Folded Spill
	addl	%r11d, %r8d
	movl	%r8d, 120(%rsp)                 # 4-byte Spill
	shrl	$3, %r11d
	xorl	%r10d, %r11d
	movl	%edi, %esi
	addl	%edi, 116(%rsp)                 # 4-byte Folded Spill
	addl	%edi, %r11d
	movl	%r11d, 152(%rsp)                # 4-byte Spill
	shrl	$3, %esi
	xorl	%eax, %esi
	movl	52(%rsp), %eax                  # 4-byte Reload
	movl	84(%rsp), %r14d                 # 4-byte Reload
	addl	%eax, %r14d
	addl	%eax, %esi
	movl	%esi, 112(%rsp)                 # 4-byte Spill
	movl	%eax, %esi
	shrl	$3, %esi
	xorl	%edx, %esi
	movl	8(%rsp), %eax                   # 4-byte Reload
	movl	140(%rsp), %ebp                 # 4-byte Reload
	addl	%eax, %ebp
	addl	%eax, %esi
	movl	%esi, 104(%rsp)                 # 4-byte Spill
	movl	%eax, %edx
	shrl	$3, %edx
	xorl	%r13d, %edx
	movq	88(%rsp), %r11                  # 8-byte Reload
	addl	176(%rsp), %r11d                # 4-byte Folded Reload
	movl	4(%rsp), %eax                   # 4-byte Reload
	addl	%eax, %edx
	movl	%edx, 8(%rsp)                   # 4-byte Spill
	shrl	$3, %eax
	xorl	%r9d, %eax
	movl	20(%rsp), %edx                  # 4-byte Reload
	addl	%edx, %eax
	movl	%eax, 4(%rsp)                   # 4-byte Spill
	movl	%edx, %eax
	shrl	$3, %eax
	xorl	132(%rsp), %eax                 # 4-byte Folded Reload
	movl	24(%rsp), %r10d                 # 4-byte Reload
	addl	%r10d, %eax
	movl	%eax, 84(%rsp)                  # 4-byte Spill
	shrl	$3, %r10d
	xorl	136(%rsp), %r10d                # 4-byte Folded Reload
	addl	48(%rsp), %r10d                 # 4-byte Folded Reload
	movq	184(%rsp), %r15                 # 8-byte Reload
	addl	%ecx, %r15d
	addl	%ecx, %r11d
	movl	16(%rsp), %edi                  # 4-byte Reload
	movl	%edi, %ecx
	roll	$25, %ecx
	movl	%edi, %edx
	roll	$14, %edx
	xorl	%ecx, %edx
	movl	%edi, %eax
	shrl	$3, %eax
	xorl	%edx, %eax
	addl	%r12d, %r10d
	addl	%r12d, %eax
	movl	%eax, 52(%rsp)                  # 4-byte Spill
	shrl	$10, %r12d
	movl	%r15d, %ecx
	roll	$26, %ecx
	xorl	168(%rsp), %r12d                # 4-byte Folded Reload
	movl	%r15d, %edx
	roll	$21, %edx
	xorl	%ecx, %edx
	movl	%ebp, %eax
	addl	%r12d, %eax
	movl	%r15d, %ecx
	roll	$7, %ecx
	xorl	%edx, %ecx
	movq	32(%rsp), %rdx                  # 8-byte Reload
                                        # kill: def $edx killed $edx killed $rdx
	movq	56(%rsp), %rbp                  # 8-byte Reload
	xorl	%ebp, %edx
	andl	%r15d, %edx
	movq	192(%rsp), %rsi                 # 8-byte Reload
	addl	%eax, %esi
	movl	%eax, %r12d
	xorl	%ebp, %edx
	addl	%edx, %esi
	addl	%esi, %ecx
	addl	$770255983, %ecx                # imm = 0x2DE92C6F
	movl	%r11d, %edx
	roll	$30, %edx
	movl	%r11d, %esi
	roll	$19, %esi
	movl	%r11d, %ebx
	roll	$10, %ebx
	xorl	%edx, %esi
	xorl	%esi, %ebx
	movl	%r11d, %edx
	movq	144(%rsp), %r8                  # 8-byte Reload
	andl	%r8d, %edx
	movl	%r11d, %ebp
	orl	%r8d, %ebp
	movq	72(%rsp), %r9                   # 8-byte Reload
	andl	%r9d, %ebp
	orl	%edx, %ebp
	addl	%ebx, %ebp
	movq	96(%rsp), %r13                  # 8-byte Reload
	addl	%ecx, %r13d
	movl	%r13d, %edx
	roll	$26, %edx
	movl	%r13d, %esi
	roll	$21, %esi
	addl	%ecx, %ebp
	xorl	%edx, %esi
	movl	%edi, %ebx
	roll	$15, %ebx
	movl	%edi, %edx
	roll	$13, %edx
	movl	%r13d, %ecx
	movq	%r13, 96(%rsp)                  # 8-byte Spill
	roll	$7, %ecx
	xorl	%esi, %ecx
	xorl	%ebx, %edx
	movl	%eax, %esi
	roll	$25, %esi
	movl	%eax, %ebx
	roll	$14, %ebx
	xorl	%esi, %ebx
	shrl	$3, %eax
	xorl	%ebx, %eax
	addl	%edi, 84(%rsp)                  # 4-byte Folded Spill
	addl	%edi, %eax
	movl	%eax, 24(%rsp)                  # 4-byte Spill
	shrl	$10, %edi
	xorl	%edx, %edi
	addl	%edi, %r14d
	movl	%ebp, %edx
	roll	$30, %edx
	movl	%ebp, %esi
	roll	$19, %esi
	xorl	%edx, %esi
	movl	%r15d, %edx
	movq	32(%rsp), %rax                  # 8-byte Reload
	xorl	%eax, %edx
	andl	%r13d, %edx
	xorl	%eax, %edx
	movq	56(%rsp), %rax                  # 8-byte Reload
	addl	%r14d, %eax
	addl	%edx, %eax
	movl	%ebp, %edx
	roll	$10, %edx
	xorl	%esi, %edx
	movl	%ebp, %esi
	andl	%r11d, %esi
	movl	%ebp, %ebx
	orl	%r11d, %ebx
	andl	%r8d, %ebx
	orl	%esi, %ebx
	addl	%eax, %ecx
	addl	$1249150122, %ecx               # imm = 0x4A7484AA
	addl	%edx, %ebx
	movq	%r9, %rdi
	addl	%ecx, %edi
	movl	%edi, %edx
	roll	$26, %edx
	addl	%ecx, %ebx
	movl	%edi, %esi
	roll	$21, %esi
	xorl	%edx, %esi
	movl	%r12d, %edx
	roll	$13, %edx
	movl	%r12d, %ecx
	roll	$15, %ecx
	xorl	%ecx, %edx
	movl	%edi, %r8d
	roll	$7, %r8d
	xorl	%esi, %r8d
	movl	%r14d, %esi
	roll	$25, %esi
	movl	%r14d, %ecx
	roll	$14, %ecx
	xorl	%esi, %ecx
	movl	%r14d, %eax
	shrl	$3, %eax
	xorl	%ecx, %eax
	addl	%r12d, 4(%rsp)                  # 4-byte Folded Spill
	addl	%r12d, %eax
	movl	%eax, 48(%rsp)                  # 4-byte Spill
	movl	%r12d, %ecx
	shrl	$10, %ecx
	xorl	%edx, %ecx
	movl	116(%rsp), %r9d                 # 4-byte Reload
	addl	%ecx, %r9d
	movq	96(%rsp), %rax                  # 8-byte Reload
	movl	%eax, %ecx
	xorl	%r15d, %ecx
	andl	%edi, %ecx
	movq	%rdi, 72(%rsp)                  # 8-byte Spill
	xorl	%r15d, %ecx
	movq	32(%rsp), %rax                  # 8-byte Reload
	addl	%r9d, %eax
	addl	%ecx, %eax
	addl	%eax, %r8d
	addl	$1555081692, %r8d               # imm = 0x5CB0A9DC
	movl	%ebx, %edx
	roll	$30, %edx
	movl	%ebx, %esi
	roll	$19, %esi
	movl	%ebx, %ecx
	roll	$10, %ecx
	xorl	%edx, %esi
	xorl	%esi, %ecx
	movl	%ebx, %edx
	andl	%ebp, %edx
	movl	%ebx, %esi
	orl	%ebp, %esi
	andl	%r11d, %esi
	orl	%edx, %esi
	addl	%ecx, %esi
	movq	144(%rsp), %r13                 # 8-byte Reload
	addl	%r8d, %r13d
	movl	%r13d, %ecx
	roll	$26, %ecx
	movl	%r13d, %edx
	roll	$21, %edx
	addl	%r8d, %esi
	xorl	%ecx, %edx
	movl	%r13d, %r8d
	roll	$7, %r8d
	movl	%r14d, %r12d
	roll	$13, %r12d
	movl	%r14d, %ecx
	roll	$15, %ecx
	xorl	%edx, %r8d
	xorl	%ecx, %r12d
	movl	%r9d, %ecx
	roll	$25, %ecx
	movl	%r9d, %edx
	roll	$14, %edx
	xorl	%ecx, %edx
	movl	%r9d, %eax
	shrl	$3, %eax
	xorl	%edx, %eax
	addl	%r14d, 8(%rsp)                  # 4-byte Folded Spill
	addl	%r14d, %eax
	movl	%eax, 20(%rsp)                  # 4-byte Spill
	movl	%r14d, %ecx
	shrl	$10, %ecx
	xorl	%r12d, %ecx
	movl	28(%rsp), %r14d                 # 4-byte Reload
	addl	%ecx, %r14d
	movq	%rsi, %r12
	movq	%rsi, 56(%rsp)                  # 8-byte Spill
	movl	%r12d, %ecx
	roll	$30, %ecx
	movl	%r12d, %edx
	roll	$19, %edx
	xorl	%ecx, %edx
	movl	%edi, %ecx
	movq	96(%rsp), %rsi                  # 8-byte Reload
	xorl	%esi, %ecx
	andl	%r13d, %ecx
	xorl	%esi, %ecx
	addl	%r14d, %r15d
	addl	%ecx, %r15d
	movl	%r12d, %ecx
	roll	$10, %ecx
	xorl	%edx, %ecx
	movl	%r12d, %edx
	andl	%ebx, %edx
                                        # kill: def $r12d killed $r12d killed $r12 def $r12
	orl	%ebx, %r12d
	andl	%ebp, %r12d
	orl	%edx, %r12d
	leal	(%r8,%r15), %edx
	addl	$1996064986, %edx               # imm = 0x76F988DA
	addl	%ecx, %r12d
	addl	%edx, %r11d
	movl	%r11d, %ecx
	roll	$26, %ecx
	addl	%edx, %r12d
	movl	%r11d, %edi
	roll	$21, %edi
	xorl	%ecx, %edi
	movl	%r9d, %edx
	roll	$13, %edx
	movl	%r9d, %ecx
	roll	$15, %ecx
	xorl	%ecx, %edx
	movl	%r11d, %r8d
	movq	%r11, %r15
	movq	%r11, 88(%rsp)                  # 8-byte Spill
	roll	$7, %r8d
	xorl	%edi, %r8d
	movl	%r14d, %edi
	roll	$25, %edi
	movl	%r14d, %ecx
	roll	$14, %ecx
	xorl	%edi, %ecx
	movl	%r14d, %eax
	shrl	$3, %eax
	xorl	%ecx, %eax
	addl	%r9d, 104(%rsp)                 # 4-byte Folded Spill
	addl	%r9d, %eax
	movl	%eax, 32(%rsp)                  # 4-byte Spill
	movl	%r9d, %ecx
	shrl	$10, %ecx
	xorl	%edx, %ecx
	movl	44(%rsp), %r9d                  # 4-byte Reload
	addl	%ecx, %r9d
	movl	%r13d, %ecx
	movq	72(%rsp), %r11                  # 8-byte Reload
	xorl	%r11d, %ecx
	andl	%r15d, %ecx
	xorl	%r11d, %ecx
	addl	%r9d, %esi
	addl	%ecx, %esi
	addl	%esi, %r8d
	addl	$-1740746414, %r8d              # imm = 0x983E5152
	movl	%r12d, %edx
	roll	$30, %edx
	movl	%r12d, %edi
	roll	$19, %edi
	movl	%r12d, %ecx
	roll	$10, %ecx
	xorl	%edx, %edi
	xorl	%edi, %ecx
	movl	%r12d, %edx
	movq	56(%rsp), %rsi                  # 8-byte Reload
	andl	%esi, %edx
	movl	%r12d, %edi
	orl	%esi, %edi
	andl	%ebx, %edi
	orl	%edx, %edi
	addl	%ecx, %edi
	addl	%r8d, %ebp
	movl	%ebp, %ecx
	roll	$26, %ecx
	movl	%ebp, %edx
	roll	$21, %edx
	addl	%r8d, %edi
	xorl	%ecx, %edx
	movl	%ebp, %r8d
	roll	$7, %r8d
	movl	%r14d, %r15d
	roll	$13, %r15d
	movl	%r14d, %ecx
	roll	$15, %ecx
	xorl	%edx, %r8d
	xorl	%ecx, %r15d
	movl	%r9d, %ecx
	roll	$25, %ecx
	movl	%r9d, %edx
	roll	$14, %edx
	xorl	%ecx, %edx
	movl	%r9d, %eax
	shrl	$3, %eax
	xorl	%edx, %eax
	addl	%r14d, 112(%rsp)                # 4-byte Folded Spill
	addl	%r14d, %eax
	movl	%eax, 16(%rsp)                  # 4-byte Spill
	shrl	$10, %r14d
	xorl	%r15d, %r14d
	addl	%r14d, %r10d
	movl	%edi, %ecx
	roll	$30, %ecx
	movl	%edi, %edx
	roll	$19, %edx
	xorl	%ecx, %edx
	movq	88(%rsp), %r14                  # 8-byte Reload
	movl	%r14d, %ecx
	xorl	%r13d, %ecx
	andl	%ebp, %ecx
	xorl	%r13d, %ecx
	movq	%r11, %rax
	addl	%r10d, %eax
	addl	%ecx, %eax
	movl	%edi, %ecx
	roll	$10, %ecx
	xorl	%edx, %ecx
	movl	%edi, %edx
	andl	%r12d, %edx
	movl	%edi, %r15d
	orl	%r12d, %r15d
	andl	%esi, %r15d
	orl	%edx, %r15d
	leal	(%r8,%rax), %edx
	addl	$-1473132947, %edx              # imm = 0xA831C66D
	addl	%ecx, %r15d
	addl	%edx, %ebx
	movl	%ebx, %ecx
	roll	$26, %ecx
	addl	%edx, %r15d
	movl	%ebx, %edx
	roll	$21, %edx
	xorl	%ecx, %edx
	movl	%r9d, %ecx
	roll	$13, %ecx
	movl	%r9d, %eax
	roll	$15, %eax
	xorl	%eax, %ecx
	movl	%ebx, %r8d
	roll	$7, %r8d
	xorl	%edx, %r8d
	movl	%r10d, %eax
	roll	$25, %eax
	movl	%r10d, %edx
	roll	$14, %edx
	xorl	%eax, %edx
	movl	%r10d, %eax
	shrl	$3, %eax
	xorl	%edx, %eax
	addl	%r9d, 152(%rsp)                 # 4-byte Folded Spill
	addl	%r9d, %eax
	movl	%eax, 44(%rsp)                  # 4-byte Spill
	shrl	$10, %r9d
	xorl	%ecx, %r9d
	movl	84(%rsp), %r11d                 # 4-byte Reload
	addl	%r9d, %r11d
	movl	%ebp, %eax
	movq	%r14, %rcx
	xorl	%ecx, %eax
	andl	%ebx, %eax
	xorl	%ecx, %eax
	addl	%r11d, %r13d
	addl	%eax, %r13d
	leal	(%r8,%r13), %r9d
	addl	$-1341970488, %r9d              # imm = 0xB00327C8
	movl	%r15d, %ecx
	roll	$30, %ecx
	movl	%r15d, %edx
	roll	$19, %edx
	movl	%r15d, %eax
	roll	$10, %eax
	xorl	%ecx, %edx
	xorl	%edx, %eax
	movl	%r15d, %ecx
	andl	%edi, %ecx
	movl	%r15d, %r8d
	orl	%edi, %r8d
	andl	%r12d, %r8d
	orl	%ecx, %r8d
	addl	%eax, %r8d
	movq	%rsi, %rcx
	addl	%r9d, %ecx
	movl	%ecx, %eax
	roll	$26, %eax
	movl	%ecx, %edx
	roll	$21, %edx
	addl	%r9d, %r8d
	xorl	%eax, %edx
	movl	%ecx, %r9d
	movq	%rcx, %rsi
	roll	$7, %r9d
	movl	%r10d, %ecx
	roll	$13, %ecx
	movl	%r10d, %eax
	roll	$15, %eax
	xorl	%edx, %r9d
	xorl	%eax, %ecx
	movl	%r11d, %eax
	roll	$25, %eax
	movl	%r11d, %edx
	roll	$14, %edx
	xorl	%eax, %edx
	movl	%r11d, %eax
	shrl	$3, %eax
	xorl	%edx, %eax
	addl	%r10d, 120(%rsp)                # 4-byte Folded Spill
	addl	%r10d, %eax
	movl	%eax, 12(%rsp)                  # 4-byte Spill
	shrl	$10, %r10d
	xorl	%ecx, %r10d
	movl	4(%rsp), %r14d                  # 4-byte Reload
	addl	%r10d, %r14d
	movl	%r8d, %eax
	roll	$30, %eax
	movl	%r8d, %ecx
	roll	$19, %ecx
	xorl	%eax, %ecx
	movl	%ebx, %eax
	xorl	%ebp, %eax
	andl	%esi, %eax
	xorl	%ebp, %eax
	movq	88(%rsp), %rdx                  # 8-byte Reload
	addl	%r14d, %edx
	addl	%eax, %edx
	movl	%r8d, %eax
	roll	$10, %eax
	xorl	%ecx, %eax
	movl	%r8d, %ecx
	andl	%r15d, %ecx
	movl	%r8d, %r10d
	orl	%r15d, %r10d
	andl	%edi, %r10d
	orl	%ecx, %r10d
	leal	(%r9,%rdx), %ecx
	addl	$-1084653625, %ecx              # imm = 0xBF597FC7
	addl	%eax, %r10d
	addl	%ecx, %r12d
	movl	%r12d, %eax
	roll	$26, %eax
	addl	%ecx, %r10d
	movl	%r12d, %edx
	roll	$21, %edx
	xorl	%eax, %edx
	movl	%r11d, %ecx
	roll	$13, %ecx
	movl	%r11d, %eax
	roll	$15, %eax
	xorl	%eax, %ecx
	movl	%r12d, %r9d
	roll	$7, %r9d
	xorl	%edx, %r9d
	movl	%r14d, %edx
	roll	$25, %edx
	movl	%r14d, %eax
	roll	$14, %eax
	xorl	%edx, %eax
	movl	%r14d, %edx
	shrl	$3, %edx
	xorl	%eax, %edx
	addl	%r11d, 160(%rsp)                # 4-byte Folded Spill
	addl	%r11d, %edx
	movl	%edx, 28(%rsp)                  # 4-byte Spill
	shrl	$10, %r11d
	xorl	%ecx, %r11d
	movl	8(%rsp), %r13d                  # 4-byte Reload
	addl	%r11d, %r13d
	movl	%esi, %eax
	xorl	%ebx, %eax
	andl	%r12d, %eax
	xorl	%ebx, %eax
	addl	%r13d, %ebp
	addl	%eax, %ebp
	leal	(%r9,%rbp), %eax
	addl	$-958395405, %eax               # imm = 0xC6E00BF3
	movl	%r10d, %ecx
	roll	$30, %ecx
	movl	%r10d, %edx
	roll	$19, %edx
	movl	%r10d, %ebp
	roll	$10, %ebp
	xorl	%ecx, %edx
	xorl	%edx, %ebp
	movl	%r10d, %ecx
	andl	%r8d, %ecx
	movl	%r10d, %r9d
	orl	%r8d, %r9d
	andl	%r15d, %r9d
	orl	%ecx, %r9d
	addl	%ebp, %r9d
	addl	%eax, %edi
	movl	%edi, %ecx
	roll	$26, %ecx
	movl	%edi, %edx
	roll	$21, %edx
	addl	%eax, %r9d
	xorl	%ecx, %edx
	movl	%edi, %eax
	roll	$7, %eax
	movl	%r14d, %ecx
	roll	$13, %ecx
	movl	%r14d, %ebp
	roll	$15, %ebp
	xorl	%edx, %eax
	xorl	%ebp, %ecx
	movl	%r13d, %edx
	roll	$25, %edx
	movl	%r13d, %ebp
	roll	$14, %ebp
	xorl	%edx, %ebp
	movl	%r13d, %edx
	shrl	$3, %edx
	xorl	%ebp, %edx
	addl	%r14d, 52(%rsp)                 # 4-byte Folded Spill
	addl	%r14d, %edx
	movl	%edx, 116(%rsp)                 # 4-byte Spill
	shrl	$10, %r14d
	xorl	%ecx, %r14d
	movl	104(%rsp), %ebp                 # 4-byte Reload
	addl	%r14d, %ebp
	movl	%r9d, %ecx
	roll	$30, %ecx
	movl	%r9d, %edx
	roll	$19, %edx
	xorl	%ecx, %edx
	movl	%r12d, %ecx
	xorl	%esi, %ecx
	andl	%edi, %ecx
	xorl	%esi, %ecx
	addl	%ebp, %ebx
	addl	%ecx, %ebx
	movl	%r9d, %ecx
	roll	$10, %ecx
	xorl	%edx, %ecx
	movl	%r9d, %edx
	andl	%r10d, %edx
	movl	%r9d, %r11d
	orl	%r10d, %r11d
	andl	%r8d, %r11d
	orl	%edx, %r11d
	addl	%ebx, %eax
	addl	$-710438585, %eax               # imm = 0xD5A79147
	addl	%ecx, %r11d
	addl	%eax, %r15d
	movl	%r15d, %ecx
	roll	$26, %ecx
	addl	%eax, %r11d
	movl	%r15d, %edx
	roll	$21, %edx
	xorl	%ecx, %edx
	movl	%r13d, %ecx
	roll	$13, %ecx
	movl	%r13d, %eax
	roll	$15, %eax
	xorl	%eax, %ecx
	movl	%r15d, %eax
	roll	$7, %eax
	xorl	%edx, %eax
	movl	%ebp, %edx
	roll	$25, %edx
	movl	%ebp, %ebx
	roll	$14, %ebx
	xorl	%edx, %ebx
	movl	%ebp, %edx
	shrl	$3, %edx
	xorl	%ebx, %edx
	addl	%r13d, 24(%rsp)                 # 4-byte Folded Spill
	addl	%r13d, %edx
	movl	%edx, 4(%rsp)                   # 4-byte Spill
	movl	%r13d, %edx
	shrl	$10, %edx
	xorl	%ecx, %edx
	movl	112(%rsp), %r14d                # 4-byte Reload
	addl	%edx, %r14d
	movl	%edi, %ecx
	xorl	%r12d, %ecx
	andl	%r15d, %ecx
	xorl	%r12d, %ecx
	addl	%r14d, %esi
	addl	%ecx, %esi
	addl	%esi, %eax
	addl	$113926993, %eax                # imm = 0x6CA6351
	movl	%r11d, %ecx
	roll	$30, %ecx
	movl	%r11d, %edx
	roll	$19, %edx
	movl	%r11d, %esi
	roll	$10, %esi
	xorl	%ecx, %edx
	xorl	%edx, %esi
	movl	%r11d, %ecx
	andl	%r9d, %ecx
	movl	%r11d, %r13d
	orl	%r9d, %r13d
	andl	%r10d, %r13d
	orl	%ecx, %r13d
	addl	%esi, %r13d
	addl	%eax, %r8d
	movl	%r8d, %ecx
	roll	$26, %ecx
	movl	%r8d, %edx
	roll	$21, %edx
	addl	%eax, %r13d
	xorl	%ecx, %edx
	movl	%r8d, %eax
	roll	$7, %eax
	movl	%ebp, %ebx
	movl	%ebp, %ecx
	roll	$13, %ecx
	movl	%ebp, %esi
	roll	$15, %esi
	xorl	%edx, %eax
	xorl	%esi, %ecx
	movl	%r14d, %edx
	roll	$25, %edx
	movl	%r14d, %esi
	roll	$14, %esi
	xorl	%edx, %esi
	movl	%r14d, %edx
	shrl	$3, %edx
	xorl	%esi, %edx
	addl	%ebp, 48(%rsp)                  # 4-byte Folded Spill
	addl	%ebp, %edx
	movl	%edx, 104(%rsp)                 # 4-byte Spill
	movl	%ebp, %edx
	shrl	$10, %edx
	xorl	%ecx, %edx
	movl	152(%rsp), %ebp                 # 4-byte Reload
	addl	%edx, %ebp
	movl	%r13d, %ecx
	roll	$30, %ecx
	movl	%r13d, %edx
	roll	$19, %edx
	xorl	%ecx, %edx
	movl	%r15d, %ecx
	xorl	%edi, %ecx
	andl	%r8d, %ecx
	xorl	%edi, %ecx
	addl	%ebp, %r12d
	addl	%ecx, %r12d
	movl	%r13d, %ecx
	roll	$10, %ecx
	xorl	%edx, %ecx
	movl	%r13d, %edx
	andl	%r11d, %edx
	movl	%r13d, %ebx
	orl	%r11d, %ebx
	andl	%r9d, %ebx
	orl	%edx, %ebx
	addl	%r12d, %eax
	addl	$338241895, %eax                # imm = 0x14292967
	addl	%ecx, %ebx
	addl	%eax, %r10d
	movl	%r10d, %ecx
	roll	$26, %ecx
	addl	%eax, %ebx
	movl	%r10d, %edx
	roll	$21, %edx
	xorl	%ecx, %edx
	movl	%r14d, %ecx
	roll	$13, %ecx
	movl	%r14d, %eax
	roll	$15, %eax
	xorl	%eax, %ecx
	movl	%r10d, %eax
	roll	$7, %eax
	xorl	%edx, %eax
	movl	%ebp, %edx
	roll	$25, %edx
	movl	%ebp, %esi
	roll	$14, %esi
	xorl	%edx, %esi
	movl	%ebp, %edx
	shrl	$3, %edx
	xorl	%esi, %edx
	addl	%r14d, 20(%rsp)                 # 4-byte Folded Spill
	addl	%r14d, %edx
	movl	%edx, 8(%rsp)                   # 4-byte Spill
	shrl	$10, %r14d
	xorl	%ecx, %r14d
	movl	120(%rsp), %r12d                # 4-byte Reload
	addl	%r14d, %r12d
	movl	%r8d, %ecx
	xorl	%r15d, %ecx
	andl	%r10d, %ecx
	xorl	%r15d, %ecx
	addl	%r12d, %edi
	addl	%ecx, %edi
	addl	%edi, %eax
	addl	$666307205, %eax                # imm = 0x27B70A85
	movl	%ebx, %ecx
	roll	$30, %ecx
	movl	%ebx, %edx
	roll	$19, %edx
	movl	%ebx, %esi
	roll	$10, %esi
	xorl	%ecx, %edx
	xorl	%edx, %esi
	movl	%ebx, %ecx
	andl	%r13d, %ecx
	movl	%ebx, %edx
	orl	%r13d, %edx
	andl	%r11d, %edx
	orl	%ecx, %edx
	addl	%esi, %edx
	addl	%eax, %r9d
	movl	%r9d, %ecx
	roll	$26, %ecx
	movl	%r9d, %esi
	roll	$21, %esi
	addl	%eax, %edx
	xorl	%ecx, %esi
	movl	%r9d, %eax
	roll	$7, %eax
	movl	%ebp, %ecx
	roll	$13, %ecx
	movl	%ebp, %edi
	roll	$15, %edi
	xorl	%esi, %eax
	xorl	%edi, %ecx
	movl	%r12d, %esi
	roll	$25, %esi
	movl	%r12d, %edi
	roll	$14, %edi
	xorl	%esi, %edi
	movl	%r12d, %esi
	shrl	$3, %esi
	xorl	%edi, %esi
	movl	%ebp, %edi
	addl	%ebp, 32(%rsp)                  # 4-byte Folded Spill
	addl	%ebp, %esi
	movl	%esi, 96(%rsp)                  # 4-byte Spill
	movl	%ebp, %esi
	shrl	$10, %esi
	xorl	%ecx, %esi
	movl	160(%rsp), %ebp                 # 4-byte Reload
	addl	%esi, %ebp
	movl	%edx, %ecx
	roll	$30, %ecx
	movl	%edx, %esi
	roll	$19, %esi
	xorl	%ecx, %esi
	movl	%r10d, %ecx
	xorl	%r8d, %ecx
	andl	%r9d, %ecx
	xorl	%r8d, %ecx
	addl	%ebp, %r15d
	movl	%ebp, %edi
	addl	%ecx, %r15d
	movl	%edx, %ecx
	roll	$10, %ecx
	xorl	%esi, %ecx
	movl	%edx, %esi
	andl	%ebx, %esi
	movl	%edx, %r14d
	orl	%ebx, %r14d
	andl	%r13d, %r14d
	orl	%esi, %r14d
	addl	%r15d, %eax
	addl	$773529912, %eax                # imm = 0x2E1B2138
	addl	%ecx, %r14d
	addl	%eax, %r11d
	movl	%r11d, %ecx
	roll	$26, %ecx
	addl	%eax, %r14d
	movl	%r11d, %esi
	roll	$21, %esi
	xorl	%ecx, %esi
	movl	%r12d, %ecx
	roll	$13, %ecx
	movl	%r12d, %eax
	movl	%r12d, %ebp
	roll	$15, %eax
	xorl	%eax, %ecx
	movl	%r11d, %eax
	roll	$7, %eax
	xorl	%esi, %eax
	movl	%edi, %r12d
	movl	%edi, %esi
	roll	$25, %esi
	roll	$14, %edi
	xorl	%esi, %edi
	movl	%r12d, %esi
	shrl	$3, %esi
	xorl	%edi, %esi
	addl	%ebp, 16(%rsp)                  # 4-byte Folded Spill
	addl	%ebp, %esi
	movq	%rsi, 56(%rsp)                  # 8-byte Spill
	movl	%ebp, %esi
	shrl	$10, %esi
	xorl	%ecx, %esi
	movl	52(%rsp), %ebp                  # 4-byte Reload
	addl	%esi, %ebp
	movl	%r9d, %ecx
	xorl	%r10d, %ecx
	andl	%r11d, %ecx
	xorl	%r10d, %ecx
	addl	%ebp, %r8d
	addl	%ecx, %r8d
	leal	(%rax,%r8), %ecx
	addl	$1294757372, %ecx               # imm = 0x4D2C6DFC
	movl	%r14d, %eax
	roll	$30, %eax
	movl	%r14d, %esi
	roll	$19, %esi
	movl	%r14d, %edi
	roll	$10, %edi
	xorl	%eax, %esi
	xorl	%esi, %edi
	movl	%r14d, %esi
	andl	%edx, %esi
	movl	%r14d, %r8d
	orl	%edx, %r8d
	andl	%ebx, %r8d
	orl	%esi, %r8d
	addl	%edi, %r8d
	addl	%ecx, %r13d
	movl	%r13d, %esi
	roll	$26, %esi
	movl	%r13d, %edi
	roll	$21, %edi
	addl	%ecx, %r8d
	xorl	%esi, %edi
	movl	%r13d, %ecx
	roll	$7, %ecx
	movl	%r12d, %r15d
	roll	$13, %r15d
	movl	%r12d, %eax
	roll	$15, %eax
	xorl	%edi, %ecx
	xorl	%eax, %r15d
	movl	%ebp, %eax
	roll	$25, %eax
	movl	%ebp, %edi
	roll	$14, %edi
	xorl	%eax, %edi
	movl	%ebp, %esi
	shrl	$3, %esi
	xorl	%edi, %esi
	movl	%r12d, %eax
	addl	%r12d, 44(%rsp)                 # 4-byte Folded Spill
	addl	%r12d, %esi
	movl	%esi, 88(%rsp)                  # 4-byte Spill
	shrl	$10, %eax
	xorl	%r15d, %eax
	movl	24(%rsp), %r15d                 # 4-byte Reload
	addl	%eax, %r15d
	movl	%r8d, %eax
	roll	$30, %eax
	movl	%r8d, %esi
	roll	$19, %esi
	xorl	%eax, %esi
	movl	%r11d, %eax
	xorl	%r9d, %eax
	andl	%r13d, %eax
	xorl	%r9d, %eax
	addl	%r15d, %r10d
	addl	%eax, %r10d
	movl	%r8d, %eax
	roll	$10, %eax
	xorl	%esi, %eax
	movl	%r8d, %esi
	andl	%r14d, %esi
	movl	%r8d, %edi
	orl	%r14d, %edi
	andl	%edx, %edi
	orl	%esi, %edi
	addl	%r10d, %ecx
	addl	$1396182291, %ecx               # imm = 0x53380D13
	addl	%eax, %edi
	addl	%ecx, %ebx
	movl	%ebx, %eax
	roll	$26, %eax
	addl	%ecx, %edi
	movq	%rdi, %r10
	movl	%ebx, %edi
	roll	$21, %edi
	xorl	%eax, %edi
	movl	%ebp, %esi
	roll	$13, %esi
	movl	%ebp, %eax
	roll	$15, %eax
	xorl	%eax, %esi
	movl	%ebx, %ecx
	roll	$7, %ecx
	xorl	%edi, %ecx
	movl	%r15d, %eax
	roll	$25, %eax
	movl	%r15d, %edi
	roll	$14, %edi
	xorl	%eax, %edi
	movl	%r15d, %eax
	shrl	$3, %eax
	xorl	%edi, %eax
	addl	%ebp, 12(%rsp)                  # 4-byte Folded Spill
	addl	%ebp, %eax
	movl	%eax, 24(%rsp)                  # 4-byte Spill
	movl	%ebp, %eax
	shrl	$10, %eax
	xorl	%esi, %eax
	movl	48(%rsp), %r12d                 # 4-byte Reload
	addl	%eax, %r12d
	movl	%r13d, %eax
	xorl	%r11d, %eax
	andl	%ebx, %eax
	xorl	%r11d, %eax
	addl	%r12d, %r9d
	addl	%eax, %r9d
	leal	(%rcx,%r9), %eax
	addl	$1695183700, %eax               # imm = 0x650A7354
	movl	%r10d, %ecx
	roll	$30, %ecx
	movl	%r10d, %esi
	roll	$19, %esi
	movl	%r10d, %edi
	roll	$10, %edi
	xorl	%ecx, %esi
	xorl	%esi, %edi
	movl	%r10d, %esi
	andl	%r8d, %esi
	movl	%r10d, %ecx
	orl	%r8d, %ecx
	andl	%r14d, %ecx
	orl	%esi, %ecx
	addl	%edi, %ecx
	addl	%eax, %edx
	movl	%edx, %esi
	roll	$26, %esi
	movl	%edx, %ebp
	roll	$21, %ebp
	addl	%eax, %ecx
	xorl	%esi, %ebp
	movl	%edx, %r9d
	roll	$7, %r9d
	movl	%r15d, %edi
	roll	$13, %edi
	movl	%r15d, %eax
	roll	$15, %eax
	xorl	%ebp, %r9d
	xorl	%eax, %edi
	movl	%r12d, %eax
	roll	$25, %eax
	movl	%r12d, %ebp
	roll	$14, %ebp
	xorl	%eax, %ebp
	movl	%r12d, %esi
	shrl	$3, %esi
	xorl	%ebp, %esi
	movl	%r15d, %eax
	addl	%r15d, 28(%rsp)                 # 4-byte Folded Spill
	addl	%r15d, %esi
	movl	%esi, 52(%rsp)                  # 4-byte Spill
	shrl	$10, %eax
	xorl	%edi, %eax
	movl	20(%rsp), %r15d                 # 4-byte Reload
	addl	%eax, %r15d
	movl	%ecx, %eax
	roll	$30, %eax
	movl	%ecx, %edi
	roll	$19, %edi
	xorl	%eax, %edi
	movl	%ebx, %eax
	xorl	%r13d, %eax
	andl	%edx, %eax
	xorl	%r13d, %eax
	addl	%r15d, %r11d
	addl	%eax, %r11d
	movl	%ecx, %eax
	roll	$10, %eax
	xorl	%edi, %eax
	movl	%ecx, %edi
	movq	%r10, %rsi
	movq	%r10, 72(%rsp)                  # 8-byte Spill
	andl	%esi, %edi
	movl	%ecx, %r10d
	orl	%esi, %r10d
	andl	%r8d, %r10d
	orl	%edi, %r10d
	leal	(%r9,%r11), %esi
	addl	$1986661051, %esi               # imm = 0x766A0ABB
	addl	%eax, %r10d
	addl	%esi, %r14d
	movl	%r14d, %eax
	roll	$26, %eax
	addl	%esi, %r10d
	movl	%r14d, %ebp
	roll	$21, %ebp
	xorl	%eax, %ebp
	movl	%r12d, %r9d
	movl	%r12d, %edi
	roll	$13, %edi
	movl	%r12d, %eax
	roll	$15, %eax
	xorl	%eax, %edi
	movl	%r14d, %esi
	roll	$7, %esi
	xorl	%ebp, %esi
	movl	%r15d, %eax
	roll	$25, %eax
	movl	%r15d, %ebp
	roll	$14, %ebp
	xorl	%eax, %ebp
	movl	%r15d, %eax
	shrl	$3, %eax
	xorl	%ebp, %eax
	addl	%r12d, 116(%rsp)                # 4-byte Folded Spill
	addl	%r12d, %eax
	movl	%eax, 48(%rsp)                  # 4-byte Spill
	movl	%r12d, %eax
	shrl	$10, %eax
	xorl	%edi, %eax
	movl	32(%rsp), %r11d                 # 4-byte Reload
	addl	%eax, %r11d
	movl	%edx, %eax
	xorl	%ebx, %eax
	andl	%r14d, %eax
	xorl	%ebx, %eax
	addl	%r11d, %r13d
	addl	%eax, %r13d
	leal	(%rsi,%r13), %eax
	addl	$-2117940946, %eax              # imm = 0x81C2C92E
	movl	%r10d, %esi
	roll	$30, %esi
	movl	%r10d, %edi
	roll	$19, %edi
	movl	%r10d, %ebp
	roll	$10, %ebp
	xorl	%esi, %edi
	xorl	%edi, %ebp
	movl	%r10d, %esi
	andl	%ecx, %esi
	movl	%r10d, %r9d
	orl	%ecx, %r9d
	movq	72(%rsp), %r12                  # 8-byte Reload
	andl	%r12d, %r9d
	orl	%esi, %r9d
	addl	%ebp, %r9d
	addl	%eax, %r8d
	movl	%r8d, %esi
	roll	$26, %esi
	movl	%r8d, %ebp
	roll	$21, %ebp
	addl	%eax, %r9d
	xorl	%esi, %ebp
	movl	%r8d, %r13d
	roll	$7, %r13d
	movl	%r15d, %esi
	roll	$13, %esi
	movl	%r15d, %eax
	roll	$15, %eax
	xorl	%ebp, %r13d
	xorl	%eax, %esi
	movl	%r11d, %eax
	roll	$25, %eax
	movl	%r11d, %ebp
	roll	$14, %ebp
	xorl	%eax, %ebp
	movl	%r11d, %edi
	shrl	$3, %edi
	xorl	%ebp, %edi
	movl	%r15d, %eax
	addl	%r15d, 4(%rsp)                  # 4-byte Folded Spill
	addl	%r15d, %edi
	movl	%edi, 120(%rsp)                 # 4-byte Spill
	shrl	$10, %eax
	xorl	%esi, %eax
	movl	16(%rsp), %r15d                 # 4-byte Reload
	addl	%eax, %r15d
	movl	%r9d, %eax
	roll	$30, %eax
	movl	%r9d, %esi
	roll	$19, %esi
	xorl	%eax, %esi
	movl	%r14d, %eax
	xorl	%edx, %eax
	andl	%r8d, %eax
	xorl	%edx, %eax
	addl	%r15d, %ebx
	addl	%eax, %ebx
	movl	%r9d, %eax
	roll	$10, %eax
	xorl	%esi, %eax
	movl	%r9d, %esi
	andl	%r10d, %esi
	movl	%r9d, %edi
	orl	%r10d, %edi
	andl	%ecx, %edi
	orl	%esi, %edi
	leal	(%rbx,%r13), %esi
	addl	$-1838011259, %esi              # imm = 0x92722C85
	addl	%eax, %edi
	movq	%r12, %rbx
	addl	%esi, %ebx
	movl	%ebx, %eax
	roll	$26, %eax
	addl	%esi, %edi
	movq	%rdi, %r12
	movl	%ebx, %esi
	roll	$21, %esi
	xorl	%eax, %esi
	movl	%r11d, %ebp
	roll	$13, %ebp
	movl	%r11d, %eax
	roll	$15, %eax
	xorl	%eax, %ebp
	movl	%ebx, %edi
	movq	%rbx, %r13
	roll	$7, %edi
	xorl	%esi, %edi
	movl	%r15d, %eax
	roll	$25, %eax
	movl	%r15d, %esi
	roll	$14, %esi
	xorl	%eax, %esi
	movl	%r15d, %ebx
	shrl	$3, %ebx
	xorl	%esi, %ebx
	movl	%r11d, %eax
	addl	%r11d, 104(%rsp)                # 4-byte Folded Spill
	addl	%r11d, %ebx
	movl	%ebx, 16(%rsp)                  # 4-byte Spill
	shrl	$10, %eax
	xorl	%ebp, %eax
	movl	44(%rsp), %ebx                  # 4-byte Reload
	addl	%eax, %ebx
	movl	%r8d, %eax
	xorl	%r14d, %eax
	movq	%r13, %r11
	movq	%r13, 72(%rsp)                  # 8-byte Spill
	andl	%r11d, %eax
	xorl	%r14d, %eax
	addl	%ebx, %edx
	movl	%ebx, %r13d
	addl	%eax, %edx
	leal	(%rdi,%rdx), %eax
	addl	$-1564481375, %eax              # imm = 0xA2BFE8A1
	movl	%r12d, %edx
	roll	$30, %edx
	movl	%r12d, %esi
	roll	$19, %esi
	movl	%r12d, %ebp
	roll	$10, %ebp
	xorl	%edx, %esi
	xorl	%esi, %ebp
	movl	%r12d, %edx
	andl	%r9d, %edx
	movl	%r12d, %edi
	orl	%r9d, %edi
	andl	%r10d, %edi
	orl	%edx, %edi
	addl	%ebp, %edi
	addl	%eax, %ecx
	movl	%ecx, %edx
	roll	$26, %edx
	movl	%ecx, %ebp
	roll	$21, %ebp
	addl	%eax, %edi
	xorl	%edx, %ebp
	movl	%ecx, %edx
	roll	$7, %edx
	movl	%r15d, %esi
	roll	$13, %esi
	movl	%r15d, %eax
	roll	$15, %eax
	xorl	%ebp, %edx
	xorl	%eax, %esi
	movl	%ebx, %eax
	roll	$25, %eax
	movl	%ebx, %ebp
	roll	$14, %ebp
	xorl	%eax, %ebp
	shrl	$3, %ebx
	xorl	%ebp, %ebx
	movl	%r15d, %eax
	addl	%r15d, 8(%rsp)                  # 4-byte Folded Spill
	addl	%r15d, %ebx
	movl	%ebx, 44(%rsp)                  # 4-byte Spill
	shrl	$10, %eax
	xorl	%esi, %eax
	movl	12(%rsp), %r15d                 # 4-byte Reload
	addl	%eax, %r15d
	movl	%edi, %eax
	roll	$30, %eax
	movl	%edi, %esi
	roll	$19, %esi
	xorl	%eax, %esi
	movl	%r11d, %eax
	xorl	%r8d, %eax
	andl	%ecx, %eax
	xorl	%r8d, %eax
	addl	%r15d, %r14d
	addl	%eax, %r14d
	movl	%edi, %eax
	roll	$10, %eax
	xorl	%esi, %eax
	movl	%edi, %esi
	movq	%r12, 32(%rsp)                  # 8-byte Spill
	andl	%r12d, %esi
	movl	%edi, %r11d
	orl	%r12d, %r11d
	andl	%r9d, %r11d
	orl	%esi, %r11d
	addl	%r14d, %edx
	addl	$-1474664885, %edx              # imm = 0xA81A664B
	addl	%eax, %r11d
	addl	%edx, %r10d
	movl	%r10d, %eax
	roll	$26, %eax
	addl	%edx, %r11d
	movl	%r10d, %esi
	roll	$21, %esi
	xorl	%eax, %esi
	movl	%r13d, %ebx
	movl	%r13d, %ebp
	roll	$13, %ebp
	movl	%r13d, %eax
	movl	%r13d, %r14d
	roll	$15, %eax
	xorl	%eax, %ebp
	movl	%r10d, %edx
	roll	$7, %edx
	xorl	%esi, %edx
	movl	%r15d, %eax
	roll	$25, %eax
	movl	%r15d, %esi
	roll	$14, %esi
	xorl	%eax, %esi
	movl	%r15d, %ebx
	shrl	$3, %ebx
	xorl	%esi, %ebx
	movl	%r13d, %eax
	addl	%r13d, 96(%rsp)                 # 4-byte Folded Spill
	addl	%r13d, %ebx
	movl	%ebx, 12(%rsp)                  # 4-byte Spill
	shrl	$10, %eax
	xorl	%ebp, %eax
	movl	28(%rsp), %ebx                  # 4-byte Reload
	addl	%eax, %ebx
	movl	%ecx, %eax
	movq	72(%rsp), %r12                  # 8-byte Reload
	xorl	%r12d, %eax
	andl	%r10d, %eax
	xorl	%r12d, %eax
	addl	%ebx, %r8d
	addl	%eax, %r8d
	leal	(%rdx,%r8), %eax
	addl	$-1035236496, %eax              # imm = 0xC24B8B70
	movl	%r11d, %edx
	roll	$30, %edx
	movl	%r11d, %esi
	roll	$19, %esi
	movl	%r11d, %ebp
	roll	$10, %ebp
	xorl	%edx, %esi
	xorl	%esi, %ebp
	movl	%r11d, %edx
	andl	%edi, %edx
	movl	%r11d, %r13d
	orl	%edi, %r13d
	andl	32(%rsp), %r13d                 # 4-byte Folded Reload
	orl	%edx, %r13d
	addl	%ebp, %r13d
	addl	%eax, %r9d
	movl	%r9d, %edx
	roll	$26, %edx
	movl	%r9d, %esi
	roll	$21, %esi
	addl	%eax, %r13d
	xorl	%edx, %esi
	movl	%r9d, %r8d
	roll	$7, %r8d
	movl	%r15d, %edx
	roll	$13, %edx
	movl	%r15d, %ebp
	roll	$15, %ebp
	xorl	%esi, %r8d
	xorl	%ebp, %edx
	movl	%ebx, %esi
	roll	$25, %esi
	movl	%ebx, %ebp
	roll	$14, %ebp
	xorl	%esi, %ebp
	movl	%ebx, %eax
	shrl	$3, %eax
	xorl	%ebp, %eax
	movl	%r15d, %esi
	movq	56(%rsp), %rbp                  # 8-byte Reload
	addl	%r15d, %ebp
	movq	%rbp, 56(%rsp)                  # 8-byte Spill
	addl	%r15d, %eax
	movl	%eax, 20(%rsp)                  # 4-byte Spill
	shrl	$10, %esi
	xorl	%edx, %esi
	movl	116(%rsp), %r14d                # 4-byte Reload
	addl	%esi, %r14d
	movl	%r13d, %edx
	roll	$30, %edx
	movl	%r13d, %esi
	roll	$19, %esi
	xorl	%edx, %esi
	movl	%r10d, %edx
	xorl	%ecx, %edx
	andl	%r9d, %edx
	xorl	%ecx, %edx
	movq	%r12, %rax
	addl	%r14d, %eax
	addl	%edx, %eax
	movl	%r13d, %edx
	roll	$10, %edx
	xorl	%esi, %edx
	movl	%r13d, %ebp
	andl	%r11d, %ebp
	movl	%r13d, %r12d
	orl	%r11d, %r12d
	andl	%edi, %r12d
	orl	%ebp, %r12d
	addl	%r8d, %eax
	addl	$-949202525, %eax               # imm = 0xC76C51A3
	addl	%edx, %r12d
	movq	32(%rsp), %rsi                  # 8-byte Reload
	addl	%eax, %esi
	movl	%esi, %edx
	roll	$26, %edx
	addl	%eax, %r12d
	movl	%esi, %ebp
	roll	$21, %ebp
	xorl	%edx, %ebp
	movl	%ebx, %r8d
	movl	%ebx, %edx
	roll	$13, %edx
	movl	%ebx, %eax
	roll	$15, %eax
	xorl	%eax, %edx
	movl	%esi, %eax
	movq	%rsi, %r15
	roll	$7, %eax
	xorl	%ebp, %eax
	movl	%r14d, %ebp
	roll	$25, %ebp
	movl	%r14d, %ebx
	roll	$14, %ebx
	xorl	%ebp, %ebx
	movl	%r14d, %esi
	shrl	$3, %esi
	xorl	%ebx, %esi
	movl	%r8d, %ebp
	addl	%r8d, 88(%rsp)                  # 4-byte Folded Spill
	addl	%r8d, %esi
	movl	%esi, 72(%rsp)                  # 4-byte Spill
	shrl	$10, %ebp
	xorl	%edx, %ebp
	movl	4(%rsp), %ebx                   # 4-byte Reload
	addl	%ebp, %ebx
	movl	%r9d, %edx
	xorl	%r10d, %edx
	movq	%r15, %rsi
	movq	%r15, 32(%rsp)                  # 8-byte Spill
	andl	%esi, %edx
	xorl	%r10d, %edx
	addl	%ebx, %ecx
	movl	%ebx, %ebp
	addl	%edx, %ecx
	addl	%ecx, %eax
	addl	$-778901479, %eax               # imm = 0xD192E819
	movl	%r12d, %ecx
	roll	$30, %ecx
	movl	%r12d, %edx
	roll	$19, %edx
	movl	%r12d, %ebx
	roll	$10, %ebx
	xorl	%ecx, %edx
	xorl	%edx, %ebx
	movl	%r12d, %ecx
	andl	%r13d, %ecx
	movl	%r12d, %r8d
	orl	%r13d, %r8d
	andl	%r11d, %r8d
	orl	%ecx, %r8d
	addl	%ebx, %r8d
	addl	%eax, %edi
	movl	%edi, %ecx
	roll	$26, %ecx
	movl	%edi, %edx
	roll	$21, %edx
	addl	%eax, %r8d
	xorl	%ecx, %edx
	movl	%edi, %eax
	roll	$7, %eax
	movl	%r14d, %ecx
	roll	$13, %ecx
	movl	%r14d, %ebx
	roll	$15, %ebx
	xorl	%edx, %eax
	xorl	%ebx, %ecx
	movl	%ebp, %edx
	roll	$25, %edx
	movl	%ebp, %ebx
	roll	$14, %ebx
	xorl	%edx, %ebx
	movl	%ebp, %edx
	shrl	$3, %edx
	xorl	%ebx, %edx
	addl	%r14d, 24(%rsp)                 # 4-byte Folded Spill
	addl	%r14d, %edx
	movl	%edx, 28(%rsp)                  # 4-byte Spill
	movl	%r14d, %edx
	shrl	$10, %edx
	xorl	%ecx, %edx
	movl	104(%rsp), %r15d                # 4-byte Reload
	addl	%edx, %r15d
	movl	%r8d, %ecx
	roll	$30, %ecx
	movl	%r8d, %edx
	roll	$19, %edx
	xorl	%ecx, %edx
	movl	%esi, %ecx
	xorl	%r9d, %ecx
	andl	%edi, %ecx
	xorl	%r9d, %ecx
	addl	%r15d, %r10d
	addl	%ecx, %r10d
	movl	%r8d, %ecx
	roll	$10, %ecx
	xorl	%edx, %ecx
	movl	%r8d, %ebx
	andl	%r12d, %ebx
	movl	%r8d, %r14d
	orl	%r12d, %r14d
	andl	%r13d, %r14d
	orl	%ebx, %r14d
	addl	%r10d, %eax
	addl	$-694614492, %eax               # imm = 0xD6990624
	addl	%ecx, %r14d
	addl	%eax, %r11d
	movl	%r11d, %ecx
	roll	$26, %ecx
	addl	%eax, %r14d
	movl	%r11d, %ebx
	roll	$21, %ebx
	xorl	%ecx, %ebx
	movl	%ebp, %esi
	movl	%ebp, %ecx
	roll	$13, %ecx
	movl	%ebp, %eax
	roll	$15, %eax
	xorl	%eax, %ecx
	movl	%r11d, %eax
	roll	$7, %eax
	xorl	%ebx, %eax
	movl	%r15d, %r10d
	movl	%r15d, %ebx
	roll	$25, %ebx
	movl	%r15d, %ebp
	roll	$14, %ebp
	xorl	%ebx, %ebp
	movl	%r15d, %edx
	shrl	$3, %edx
	xorl	%ebp, %edx
	movl	%esi, %ebx
	addl	%esi, 52(%rsp)                  # 4-byte Folded Spill
	addl	%esi, %edx
	movl	%edx, 4(%rsp)                   # 4-byte Spill
	shrl	$10, %ebx
	xorl	%ecx, %ebx
	movl	8(%rsp), %esi                   # 4-byte Reload
	addl	%ebx, %esi
	movl	%edi, %ecx
	movq	32(%rsp), %rbx                  # 8-byte Reload
	xorl	%ebx, %ecx
	andl	%r11d, %ecx
	xorl	%ebx, %ecx
	addl	%esi, %r9d
	addl	%ecx, %r9d
	addl	%eax, %r9d
	addl	$-200395387, %r9d               # imm = 0xF40E3585
	movl	%r14d, %ecx
	roll	$30, %ecx
	movl	%r14d, %ebp
	roll	$19, %ebp
	movl	%r14d, %eax
	roll	$10, %eax
	xorl	%ecx, %ebp
	xorl	%ebp, %eax
	movl	%r14d, %ecx
	andl	%r8d, %ecx
	movl	%r14d, %r15d
	orl	%r8d, %r15d
	andl	%r12d, %r15d
	orl	%ecx, %r15d
	addl	%eax, %r15d
	addl	%r9d, %r13d
	movl	%r13d, %eax
	roll	$26, %eax
	movl	%r13d, %ebp
	roll	$21, %ebp
	addl	%r9d, %r15d
	xorl	%eax, %ebp
	movl	%r13d, %r9d
	roll	$7, %r9d
	movl	%r10d, %ecx
	roll	$13, %ecx
	movl	%r10d, %eax
	roll	$15, %eax
	xorl	%ebp, %r9d
	xorl	%eax, %ecx
	movl	%esi, %ebp
	roll	$25, %ebp
	movl	%esi, %eax
	roll	$14, %eax
	xorl	%ebp, %eax
	movl	%esi, %edx
	shrl	$3, %edx
	xorl	%eax, %edx
	addl	%r10d, 48(%rsp)                 # 4-byte Folded Spill
	addl	%r10d, %edx
	movl	%edx, 8(%rsp)                   # 4-byte Spill
	movl	%r10d, %eax
	shrl	$10, %eax
	xorl	%ecx, %eax
	movl	96(%rsp), %edx                  # 4-byte Reload
	addl	%eax, %edx
	movl	%r15d, %eax
	roll	$30, %eax
	movl	%r15d, %ecx
	roll	$19, %ecx
	xorl	%eax, %ecx
	movl	%r11d, %eax
	xorl	%edi, %eax
	andl	%r13d, %eax
	xorl	%edi, %eax
	movq	%rbx, %rbp
	addl	%edx, %ebp
	addl	%eax, %ebp
	movl	%r15d, %eax
	roll	$10, %eax
	xorl	%ecx, %eax
	movl	%r15d, %ecx
	andl	%r14d, %ecx
	movl	%r15d, %ebx
	orl	%r14d, %ebx
	andl	%r8d, %ebx
	orl	%ecx, %ebx
	leal	(%r9,%rbp), %ecx
	addl	$275423344, %ecx                # imm = 0x106AA070
	addl	%eax, %ebx
	addl	%ecx, %r12d
	movl	%r12d, %eax
	roll	$26, %eax
	addl	%ecx, %ebx
	movl	%r12d, %ecx
	roll	$21, %ecx
	xorl	%eax, %ecx
	movl	%esi, %r10d
	roll	$13, %r10d
	movl	%esi, %ebp
	roll	$15, %ebp
	xorl	%ebp, %r10d
	movl	%r12d, %r9d
	roll	$7, %r9d
	xorl	%ecx, %r9d
	movl	%edx, %ecx
	roll	$25, %ecx
	movl	%edx, %ebp
	roll	$14, %ebp
	xorl	%ecx, %ebp
	movl	%edx, %eax
	shrl	$3, %eax
	xorl	%ebp, %eax
	movl	%esi, %ecx
	addl	%esi, 120(%rsp)                 # 4-byte Folded Spill
	addl	%esi, %eax
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	shrl	$10, %ecx
	xorl	%r10d, %ecx
	movq	56(%rsp), %rsi                  # 8-byte Reload
	addl	%ecx, %esi
	movl	%r13d, %eax
	xorl	%r11d, %eax
	andl	%r12d, %eax
	xorl	%r11d, %eax
	addl	%esi, %edi
	addl	%eax, %edi
	leal	(%r9,%rdi), %eax
	addl	$430227734, %eax                # imm = 0x19A4C116
	movl	%ebx, %ecx
	roll	$30, %ecx
	movl	%ebx, %edi
	roll	$19, %edi
	movl	%ebx, %ebp
	roll	$10, %ebp
	xorl	%ecx, %edi
	xorl	%edi, %ebp
	movl	%ebx, %ecx
	andl	%r15d, %ecx
	movl	%ebx, %edi
	orl	%r15d, %edi
	andl	%r14d, %edi
	orl	%ecx, %edi
	addl	%ebp, %edi
	addl	%eax, %r8d
	movl	%r8d, %ecx
	roll	$26, %ecx
	movl	%r8d, %ebp
	roll	$21, %ebp
	addl	%eax, %edi
	xorl	%ecx, %ebp
	movl	%r8d, %r10d
	roll	$7, %r10d
	movl	%edx, %ecx
	movl	%edx, 96(%rsp)                  # 4-byte Spill
	movl	%edx, %r9d
	roll	$13, %r9d
	roll	$15, %ecx
	xorl	%ebp, %r10d
	xorl	%ecx, %r9d
	movq	%rsi, %rax
	movl	%eax, %ecx
	roll	$25, %ecx
	movl	%eax, %ebp
	roll	$14, %ebp
	xorl	%ecx, %ebp
	addl	%eax, 44(%rsp)                  # 4-byte Folded Spill
	movl	%eax, %ecx
	movl	%eax, %edx
	shrl	$3, %eax
	xorl	%ebp, %eax
	movl	96(%rsp), %ebp                  # 4-byte Reload
	addl	%ebp, 16(%rsp)                  # 4-byte Folded Spill
	addl	%ebp, %eax
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	shrl	$10, %ebp
	xorl	%r9d, %ebp
	movl	88(%rsp), %eax                  # 4-byte Reload
	addl	%ebp, %eax
	movl	%eax, 88(%rsp)                  # 4-byte Spill
	movl	%r12d, %ebp
	xorl	%r13d, %ebp
	andl	%r8d, %ebp
	xorl	%r13d, %ebp
	addl	%eax, %r11d
	addl	%ebp, %r11d
	movl	%edi, %r9d
	roll	$30, %r9d
	movl	%edi, %ebp
	roll	$19, %ebp
	addl	%r11d, %r10d
	addl	$506948616, %r10d               # imm = 0x1E376C08
	xorl	%r9d, %ebp
	movl	%edi, %r9d
	roll	$10, %r9d
	xorl	%ebp, %r9d
	movl	%edi, %ebp
	andl	%ebx, %ebp
	movl	%edi, %r11d
	orl	%ebx, %r11d
	andl	%r15d, %r11d
	orl	%ebp, %r11d
	addl	%r9d, %r11d
	addl	%r10d, %r14d
	addl	%r10d, %r11d
	roll	$15, %esi
	roll	$13, %ecx
	xorl	%esi, %ecx
	shrl	$10, %edx
	movl	%r14d, %esi
	roll	$26, %esi
	xorl	%ecx, %edx
	movl	%r14d, %eax
	roll	$21, %eax
	xorl	%esi, %eax
	movl	24(%rsp), %esi                  # 4-byte Reload
	addl	%edx, %esi
	movl	%esi, 24(%rsp)                  # 4-byte Spill
	movl	%r14d, %ecx
	roll	$7, %ecx
	xorl	%eax, %ecx
	movl	%r8d, %eax
	xorl	%r12d, %eax
	andl	%r14d, %eax
	addl	%esi, %r13d
	xorl	%r12d, %eax
	addl	%eax, %r13d
	addl	%r13d, %ecx
	addl	$659060556, %ecx                # imm = 0x2748774C
	movl	%r11d, %eax
	roll	$30, %eax
	movl	%r11d, %esi
	roll	$19, %esi
	movl	%r11d, %ebp
	roll	$10, %ebp
	xorl	%eax, %esi
	xorl	%esi, %ebp
	movl	%r11d, %esi
	andl	%edi, %esi
	movl	%r11d, %eax
	orl	%edi, %eax
	andl	%ebx, %eax
	orl	%esi, %eax
	addl	%ebp, %eax
	addl	%ecx, %r15d
	movl	%r15d, %esi
	roll	$26, %esi
	movl	%r15d, %ebp
	roll	$21, %ebp
	addl	%ecx, %eax
	xorl	%esi, %ebp
	movl	88(%rsp), %edx                  # 4-byte Reload
	movl	%edx, %ecx
	roll	$15, %ecx
	movl	%edx, %esi
	roll	$13, %esi
	xorl	%ecx, %esi
	addl	%edx, 12(%rsp)                  # 4-byte Folded Spill
	movl	%edx, %ecx
	shrl	$10, %ecx
	xorl	%esi, %ecx
	movl	52(%rsp), %r13d                 # 4-byte Reload
	addl	%ecx, %r13d
	movl	%r14d, %ecx
	xorl	%r8d, %ecx
	andl	%r15d, %ecx
	xorl	%r8d, %ecx
	addl	%r13d, %r12d
	addl	%ecx, %r12d
	movl	%eax, %ecx
	roll	$30, %ecx
	movl	%eax, %esi
	roll	$19, %esi
	xorl	%ecx, %esi
	movl	%r15d, %ecx
	roll	$7, %ecx
	xorl	%ebp, %ecx
	movl	%eax, %ebp
	roll	$10, %ebp
	xorl	%esi, %ebp
	movl	%eax, %esi
	andl	%r11d, %esi
	movl	%eax, %r10d
	orl	%r11d, %r10d
	andl	%edi, %r10d
	orl	%esi, %r10d
	leal	(%rcx,%r12), %r9d
	addl	$883997877, %r9d                # imm = 0x34B0BCB5
	movl	24(%rsp), %edx                  # 4-byte Reload
	movl	%edx, %esi
	roll	$15, %esi
	movl	%edx, %ecx
	roll	$13, %ecx
	addl	%ebp, %r10d
	xorl	%esi, %ecx
	addl	%r9d, %ebx
	addl	%r9d, %r10d
	addl	%edx, 20(%rsp)                  # 4-byte Folded Spill
	movl	%edx, %esi
	shrl	$10, %esi
	movl	%ebx, %ebp
	roll	$26, %ebp
	xorl	%ecx, %esi
	movl	%ebx, %ecx
	roll	$21, %ecx
	xorl	%ebp, %ecx
	movl	48(%rsp), %r12d                 # 4-byte Reload
	addl	%esi, %r12d
	movl	%ebx, %esi
	roll	$7, %esi
	xorl	%ecx, %esi
	movl	%r15d, %ecx
	xorl	%r14d, %ecx
	andl	%ebx, %ecx
	addl	%r12d, %r8d
	xorl	%r14d, %ecx
	addl	%ecx, %r8d
	leal	(%rsi,%r8), %r9d
	addl	$958139571, %r9d                # imm = 0x391C0CB3
	movl	%r10d, %esi
	roll	$30, %esi
	movl	%r10d, %ebp
	roll	$19, %ebp
	movl	%r10d, %ecx
	roll	$10, %ecx
	xorl	%esi, %ebp
	xorl	%ebp, %ecx
	movl	%r10d, %esi
	andl	%eax, %esi
	movl	%r10d, %r8d
	orl	%eax, %r8d
	andl	%r11d, %r8d
	orl	%esi, %r8d
	addl	%ecx, %r8d
	addl	%r9d, %edi
	movl	%edi, %ecx
	roll	$26, %ecx
	movl	%edi, %esi
	roll	$21, %esi
	addl	%r9d, %r8d
	xorl	%ecx, %esi
	movl	%r13d, %edx
	movl	%r13d, %ecx
	roll	$15, %ecx
	movl	%r13d, %ebp
	roll	$13, %ebp
	xorl	%ecx, %ebp
	addl	%r13d, 72(%rsp)                 # 4-byte Folded Spill
	movl	%r13d, %ecx
	shrl	$10, %ecx
	xorl	%ebp, %ecx
	movl	120(%rsp), %r13d                # 4-byte Reload
	addl	%ecx, %r13d
	movl	%ebx, %ecx
	xorl	%r15d, %ecx
	andl	%edi, %ecx
	xorl	%r15d, %ecx
	addl	%r13d, %r14d
	addl	%ecx, %r14d
	movl	%r8d, %ecx
	roll	$30, %ecx
	movl	%r8d, %ebp
	roll	$19, %ebp
	xorl	%ecx, %ebp
	movl	%edi, %ecx
	roll	$7, %ecx
	xorl	%esi, %ecx
	movl	%r8d, %esi
	roll	$10, %esi
	xorl	%ebp, %esi
	movl	%r8d, %ebp
	andl	%r10d, %ebp
	movl	%r8d, %r9d
	orl	%r10d, %r9d
	andl	%eax, %r9d
	orl	%ebp, %r9d
	addl	%ecx, %r14d
	addl	$1322822218, %r14d              # imm = 0x4ED8AA4A
	movl	%r12d, %edx
	movl	%r12d, %ebp
	roll	$13, %ebp
	movl	%r12d, %ecx
	roll	$15, %ecx
	addl	%esi, %r9d
	xorl	%ecx, %ebp
	addl	%r14d, %r11d
	addl	%r14d, %r9d
	addl	%r12d, 28(%rsp)                 # 4-byte Folded Spill
	movl	%r12d, %ecx
	shrl	$10, %ecx
	movl	%r11d, %esi
	roll	$26, %esi
	xorl	%ebp, %ecx
	movl	%r11d, %ebp
	roll	$21, %ebp
	xorl	%esi, %ebp
	movl	16(%rsp), %r12d                 # 4-byte Reload
	addl	%ecx, %r12d
	movl	%r11d, %ecx
	roll	$7, %ecx
	xorl	%ebp, %ecx
	movl	%edi, %esi
	xorl	%ebx, %esi
	andl	%r11d, %esi
	addl	%r12d, %r15d
	xorl	%ebx, %esi
	addl	%esi, %r15d
	addl	%r15d, %ecx
	addl	$1537002063, %ecx               # imm = 0x5B9CCA4F
	movl	%r9d, %edx
	roll	$30, %edx
	movl	%r9d, %esi
	roll	$19, %esi
	movl	%r9d, %ebp
	roll	$10, %ebp
	xorl	%edx, %esi
	xorl	%esi, %ebp
	movl	%r9d, %edx
	andl	%r8d, %edx
	movl	%r9d, %esi
	orl	%r8d, %esi
	andl	%r10d, %esi
	orl	%edx, %esi
	addl	%ebp, %esi
	addl	%ecx, %eax
	movl	%eax, %edx
	roll	$26, %edx
	movl	%eax, %ebp
	roll	$21, %ebp
	addl	%ecx, %esi
	xorl	%edx, %ebp
	movl	%r13d, %ecx
	movl	%r13d, %r14d
	roll	$15, %r14d
	movl	%r13d, %edx
	roll	$13, %edx
	xorl	%r14d, %edx
	addl	%r13d, 4(%rsp)                  # 4-byte Folded Spill
	shrl	$10, %ecx
	xorl	%edx, %ecx
	movl	44(%rsp), %r13d                 # 4-byte Reload
	addl	%ecx, %r13d
	movl	%r11d, %ecx
	xorl	%edi, %ecx
	andl	%eax, %ecx
	xorl	%edi, %ecx
	addl	%r13d, %ebx
	addl	%ecx, %ebx
	movl	%esi, %ecx
	roll	$30, %ecx
	movl	%esi, %edx
	roll	$19, %edx
	xorl	%ecx, %edx
	movl	%eax, %ecx
	roll	$7, %ecx
	xorl	%ebp, %ecx
	movl	%esi, %ebp
	roll	$10, %ebp
	xorl	%edx, %ebp
	movl	%esi, %r14d
	andl	%r9d, %r14d
	movl	%esi, %r15d
	orl	%r9d, %r15d
	andl	%r8d, %r15d
	orl	%r14d, %r15d
	leal	(%rcx,%rbx), %r14d
	addl	$1747873779, %r14d              # imm = 0x682E6FF3
	movl	%r12d, %edx
	movl	%r12d, %ebx
	roll	$13, %ebx
	movl	%r12d, %ecx
	roll	$15, %ecx
	addl	%ebp, %r15d
	xorl	%ecx, %ebx
	addl	%r14d, %r10d
	addl	%r14d, %r15d
	addl	%r12d, 8(%rsp)                  # 4-byte Folded Spill
	movl	%r12d, %ecx
	shrl	$10, %ecx
	movl	%r10d, %ebp
	roll	$26, %ebp
	xorl	%ebx, %ecx
	movl	%r10d, %ebx
	roll	$21, %ebx
	xorl	%ebp, %ebx
	movl	12(%rsp), %edx                  # 4-byte Reload
	addl	%ecx, %edx
	movl	%r10d, %ecx
	roll	$7, %ecx
	xorl	%ebx, %ecx
	movl	%eax, %ebx
	xorl	%r11d, %ebx
	andl	%r10d, %ebx
	addl	%edx, %edi
	xorl	%r11d, %ebx
	addl	%ebx, %edi
	addl	%ecx, %edi
	addl	$1955562222, %edi               # imm = 0x748F82EE
	movl	%r15d, %ecx
	roll	$30, %ecx
	movl	%r15d, %ebx
	roll	$19, %ebx
	movl	%r15d, %ebp
	roll	$10, %ebp
	xorl	%ecx, %ebx
	xorl	%ebx, %ebp
	movl	%r15d, %ebx
	andl	%esi, %ebx
	movl	%r15d, %r12d
	orl	%esi, %r12d
	andl	%r9d, %r12d
	orl	%ebx, %r12d
	addl	%ebp, %r12d
	addl	%edi, %r8d
	movl	%r8d, %ebx
	roll	$26, %ebx
	movl	%r8d, %r14d
	roll	$21, %r14d
	addl	%edi, %r12d
	xorl	%ebx, %r14d
	movl	%r13d, %ecx
	movl	%r13d, %edi
	roll	$15, %edi
	movl	%r13d, %ebx
	roll	$13, %ebx
	xorl	%edi, %ebx
	movq	32(%rsp), %rdi                  # 8-byte Reload
	addl	%r13d, %edi
	movq	%rdi, 32(%rsp)                  # 8-byte Spill
	movl	%r13d, %edi
	shrl	$10, %edi
	xorl	%ebx, %edi
	movl	20(%rsp), %ecx                  # 4-byte Reload
	addl	%edi, %ecx
	movl	%ecx, 20(%rsp)                  # 4-byte Spill
	movl	%r10d, %edi
	xorl	%eax, %edi
	andl	%r8d, %edi
	xorl	%eax, %edi
	addl	%ecx, %r11d
	addl	%edi, %r11d
	movl	%r12d, %edi
	roll	$30, %edi
	movl	%r12d, %ebx
	roll	$19, %ebx
	xorl	%edi, %ebx
	movl	%r8d, %ebp
	roll	$7, %ebp
	xorl	%r14d, %ebp
	movl	%r12d, %ecx
	roll	$10, %ecx
	xorl	%ebx, %ecx
	movl	%r12d, %ebx
	andl	%r15d, %ebx
	movl	%r12d, %r14d
	orl	%r15d, %r14d
	andl	%esi, %r14d
	orl	%ebx, %r14d
	addl	%ebp, %r11d
	addl	$2024104815, %r11d              # imm = 0x78A5636F
	movl	%edx, %ebx
	roll	$13, %ebx
	movl	%edx, %ebp
	roll	$15, %ebp
	addl	%ecx, %r14d
	xorl	%ebp, %ebx
	addl	%r11d, %r9d
	addl	%r11d, %r14d
	movq	56(%rsp), %rcx                  # 8-byte Reload
	addl	%edx, %ecx
	movq	%rcx, 56(%rsp)                  # 8-byte Spill
	movl	%edx, %ecx
	shrl	$10, %ecx
	movl	%r9d, %ebp
	roll	$26, %ebp
	xorl	%ebx, %ecx
	movl	%r9d, %ebx
	roll	$21, %ebx
	xorl	%ebp, %ebx
	movl	72(%rsp), %r13d                 # 4-byte Reload
	addl	%ecx, %r13d
	movl	%r9d, %ecx
	roll	$7, %ecx
	xorl	%ebx, %ecx
	movl	%r8d, %ebp
	xorl	%r10d, %ebp
	andl	%r9d, %ebp
	addl	%r13d, %eax
	xorl	%r10d, %ebp
	addl	%ebp, %eax
	addl	%ecx, %eax
	addl	$-2067236844, %eax              # imm = 0x84C87814
	movl	%r14d, %ecx
	roll	$30, %ecx
	movl	%r14d, %edi
	roll	$19, %edi
	movl	%r14d, %ebx
	roll	$10, %ebx
	xorl	%ecx, %edi
	xorl	%edi, %ebx
	movl	%r14d, %ecx
	andl	%r12d, %ecx
	movl	%r14d, %r11d
	orl	%r12d, %r11d
	andl	%r15d, %r11d
	orl	%ecx, %r11d
	addl	%ebx, %r11d
	addl	%eax, %esi
	movl	%esi, %ecx
	roll	$26, %ecx
	movl	%esi, %edi
	roll	$21, %edi
	addl	%eax, %r11d
	xorl	%ecx, %edi
	movl	20(%rsp), %edx                  # 4-byte Reload
	movl	%edx, %eax
	roll	$15, %eax
	movl	%edx, %ecx
	roll	$13, %ecx
	xorl	%eax, %ecx
	shrl	$10, %edx
	xorl	%ecx, %edx
	movl	%esi, %eax
	roll	$7, %eax
	xorl	%edi, %eax
	movl	28(%rsp), %ebx                  # 4-byte Reload
	addl	%edx, %ebx
	movl	%r9d, %ecx
	xorl	%r8d, %ecx
	andl	%esi, %ecx
	xorl	%r8d, %ecx
	addl	%ebx, %r10d
	addl	%ecx, %r10d
	movl	%r11d, %ecx
	roll	$30, %ecx
	movl	%r11d, %edi
	roll	$19, %edi
	xorl	%ecx, %edi
	addl	%r10d, %eax
	addl	$-1933114872, %eax              # imm = 0x8CC70208
	movl	%r11d, %ecx
	roll	$10, %ecx
	xorl	%edi, %ecx
	movl	%r11d, %edi
	andl	%r14d, %edi
	movl	%r11d, %r10d
	orl	%r14d, %r10d
	andl	%r12d, %r10d
	orl	%edi, %r10d
	addl	%ecx, %r10d
	addl	%eax, %r15d
	addl	%eax, %r10d
	movl	%r13d, %edx
	movl	%r13d, %eax
	roll	$15, %eax
	movl	%r13d, %ecx
	roll	$13, %ecx
	movl	%r15d, %edi
	roll	$26, %edi
	xorl	%eax, %ecx
	movl	%r15d, %eax
	roll	$21, %eax
	xorl	%edi, %eax
	shrl	$10, %edx
	xorl	%ecx, %edx
	movl	4(%rsp), %r13d                  # 4-byte Reload
	addl	%edx, %r13d
	movl	%esi, %ecx
	xorl	%r9d, %ecx
	andl	%r15d, %ecx
	xorl	%r9d, %ecx
	addl	%r13d, %r8d
	addl	%ecx, %r8d
	movl	%r10d, %ecx
	roll	$30, %ecx
	movl	%r10d, %edi
	roll	$19, %edi
	xorl	%ecx, %edi
	movl	%r15d, %ecx
	roll	$7, %ecx
	xorl	%eax, %ecx
	movl	%r10d, %eax
	roll	$10, %eax
	xorl	%edi, %eax
	movl	%r10d, %edi
	andl	%r11d, %edi
	movl	%r10d, %edx
	orl	%r11d, %edx
	andl	%r14d, %edx
	orl	%edi, %edx
	addl	%r8d, %ecx
	addl	$-1866530822, %ecx              # imm = 0x90BEFFFA
	movl	%ebx, %edi
	roll	$15, %ebx
	movl	%edi, %ebp
	roll	$13, %ebp
	addl	%eax, %edx
	xorl	%ebx, %ebp
	addl	%ecx, %r12d
	addl	%ecx, %edx
	shrl	$10, %edi
	xorl	%ebp, %edi
	movl	%r12d, %eax
	roll	$26, %eax
	movl	8(%rsp), %r8d                   # 4-byte Reload
	addl	%edi, %r8d
	movl	%r12d, %ecx
	roll	$21, %ecx
	xorl	%eax, %ecx
	movl	%r15d, %eax
	xorl	%esi, %eax
	andl	%r12d, %eax
	addl	%r8d, %r9d
	xorl	%esi, %eax
	addl	%eax, %r9d
	movl	%r12d, %eax
	roll	$7, %eax
	xorl	%ecx, %eax
	movl	%edx, %ecx
	roll	$30, %ecx
	movl	%edx, %edi
	roll	$19, %edi
	leal	(%rax,%r9), %ebp
	addl	$-1538233109, %ebp              # imm = 0xA4506CEB
	xorl	%ecx, %edi
	movl	%edx, %ecx
	roll	$10, %ecx
	xorl	%edi, %ecx
	movl	%edx, %edi
	andl	%r10d, %edi
	movl	%edx, %ebx
	orl	%r10d, %ebx
	andl	%r11d, %ebx
	orl	%edi, %ebx
	addl	%ecx, %ebx
	addl	%ebp, %r14d
	addl	%ebp, %ebx
	movl	%r13d, %eax
	movl	%r13d, %ecx
	roll	$15, %ecx
	movl	%r13d, %edi
	roll	$13, %edi
	xorl	%ecx, %edi
	shrl	$10, %eax
	movl	%r14d, %ecx
	roll	$26, %ecx
	xorl	%edi, %eax
	movl	%r14d, %edi
	roll	$21, %edi
	xorl	%ecx, %edi
	movq	32(%rsp), %rbp                  # 8-byte Reload
	addl	%eax, %ebp
	movl	%r14d, %ecx
	roll	$7, %ecx
	xorl	%edi, %ecx
	addl	%esi, %ebp
	movl	%r12d, %esi
	xorl	%r15d, %esi
	andl	%r14d, %esi
	xorl	%r15d, %esi
	addl	%esi, %ebp
	addl	%ebp, %ecx
	addl	$-1090935817, %ecx              # imm = 0xBEF9A3F7
	movl	%ebx, %esi
	roll	$30, %esi
	movl	%ebx, %edi
	roll	$19, %edi
	movl	%ebx, %ebp
	roll	$10, %ebp
	xorl	%esi, %edi
	xorl	%edi, %ebp
	movl	%ebx, %edi
	andl	%edx, %edi
	movl	%ebx, %eax
	orl	%edx, %eax
	andl	%r10d, %eax
	orl	%edi, %eax
	addl	%ebp, %eax
	addl	%ecx, %r11d
	movl	%r11d, %edi
	roll	$26, %edi
	movl	%r11d, %ebp
	roll	$21, %ebp
	addl	%ecx, %eax
	xorl	%edi, %ebp
	movl	%r8d, %esi
	movl	%r8d, %ecx
	roll	$15, %ecx
	movl	%r8d, %edi
	roll	$13, %edi
	xorl	%ecx, %edi
	shrl	$10, %esi
	xorl	%edi, %esi
	movl	%r11d, %r8d
	roll	$7, %r8d
	xorl	%ebp, %r8d
	movq	56(%rsp), %rcx                  # 8-byte Reload
	addl	%esi, %ecx
	addl	%r15d, %ecx
	movq	64(%rsp), %r15                  # 8-byte Reload
	movl	%r14d, %edi
	xorl	%r12d, %edi
	andl	%r11d, %edi
	xorl	%r12d, %edi
	addl	%edi, %ecx
	movq	%rcx, %r9
	movl	%eax, %edi
	roll	$30, %edi
	movl	%eax, %ebp
	roll	$19, %ebp
	movl	%eax, %ecx
	roll	$10, %ecx
	xorl	%edi, %ebp
	xorl	%ebp, %ecx
	movl	%eax, %edi
	andl	%ebx, %edi
	movl	%eax, %esi
	orl	%ebx, %esi
	andl	%edx, %esi
	orl	%edi, %esi
	addl	240(%rsp), %esi                 # 4-byte Folded Reload
	leal	(%r8,%r9), %edi
	addl	$-965641998, %edi               # imm = 0xC67178F2
	addl	%ecx, %esi
	addl	224(%rsp), %eax                 # 4-byte Folded Reload
	addl	%edi, %esi
	movl	%esi, (%r15)
	movl	%eax, 4(%r15)
	addl	256(%rsp), %ebx                 # 4-byte Folded Reload
	addl	272(%rsp), %edx                 # 4-byte Folded Reload
	movl	%ebx, 8(%r15)
	movl	%edx, 12(%r15)
	addl	216(%rsp), %r10d                # 4-byte Folded Reload
	addl	%edi, %r10d
	movl	%r10d, 16(%r15)
	addl	248(%rsp), %r11d                # 4-byte Folded Reload
	movl	%r11d, 20(%r15)
	addl	264(%rsp), %r14d                # 4-byte Folded Reload
	movl	%r14d, 24(%r15)
	movq	280(%rsp), %r14                 # 8-byte Reload
	movq	288(%rsp), %rbp                 # 8-byte Reload
	addl	204(%rsp), %r12d                # 4-byte Folded Reload
	movl	%r12d, 28(%r15)
	movl	$0, %eax
	cmpq	$63, %rbp
	ja	.LBB46_3
# %bb.4:                                # %._crit_edge
	testq	%rbp, %rbp
	je	.LBB46_5
.LBB46_6:
	movq	208(%rsp), %rax                 # 8-byte Reload
	leaq	(%r15,%rax), %rdi
	addq	$128, %rdi
	movq	%r14, %rsi
	movq	%rbp, %rdx
	addq	$296, %rsp                      # imm = 0x128
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	jmp	memcpy@PLT                      # TAILCALL
.LBB46_1:
	.cfi_def_cfa_offset 352
	movq	%rax, 208(%rsp)                 # 8-byte Spill
	testq	%rbp, %rbp
	jne	.LBB46_6
.LBB46_5:
	addq	$296, %rsp                      # imm = 0x128
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end46:
	.size	secp256k1_sha256_write, .Lfunc_end46-secp256k1_sha256_write
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function secp256k1_gej_add_ge
.LCPI47_0:
	.quad	4503599627370495                # 0xfffffffffffff
	.quad	4503599627370495                # 0xfffffffffffff
.LCPI47_1:
	.quad	18014381329608892               # 0x3ffffbfffff0bc
	.quad	18014398509481980               # 0x3ffffffffffffc
.LCPI47_2:
	.quad	18014398509481980               # 0x3ffffffffffffc
	.quad	18014398509481980               # 0x3ffffffffffffc
	.text
	.p2align	4, 0x90
	.type	secp256k1_gej_add_ge,@function
secp256k1_gej_add_ge:                   # @secp256k1_gej_add_ge
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$392, %rsp                      # imm = 0x188
	.cfi_def_cfa_offset 448
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rdx, -24(%rsp)                 # 8-byte Spill
	movq	%rsi, %rbp
	movq	%rsi, -56(%rsp)                 # 8-byte Spill
	movq	%rdi, 32(%rsp)                  # 8-byte Spill
	addq	$80, %rsi
	movq	%rsi, -32(%rsp)                 # 8-byte Spill
	leaq	352(%rsp), %rdi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -16(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 256(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 304(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	256(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	304(%rsp), %rsi
	movq	-16(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movups	(%rbp), %xmm0
	movups	16(%rbp), %xmm1
	movaps	%xmm0, -16(%rsp)
	movaps	%xmm1, (%rsp)
	movq	32(%rbp), %rcx
	movq	%rcx, 16(%rsp)
	movq	%rcx, %rax
	shrq	$48, %rax
	movabsq	$4294968273, %rdx               # imm = 0x1000003D1
	imulq	%rdx, %rax
	addq	-16(%rsp), %rax
	movabsq	$281474976710655, %rsi          # imm = 0xFFFFFFFFFFFF
	movq	%rax, %rdx
	shrq	$52, %rdx
	addq	-8(%rsp), %rdx
	andq	%rsi, %rcx
	movabsq	$4503599627370495, %rbx         # imm = 0xFFFFFFFFFFFFF
	andq	%rbx, %rax
	movq	%rdx, %rsi
	shrq	$52, %rsi
	addq	(%rsp), %rsi
	andq	%rbx, %rdx
	movq	%rsi, %rdi
	shrq	$52, %rdi
	addq	8(%rsp), %rdi
	andq	%rbx, %rsi
	movq	%rdi, %rbp
	shrq	$52, %rbp
	addq	%rcx, %rbp
	andq	%rbx, %rdi
	movq	%rax, -16(%rsp)
	movq	%rdx, -8(%rsp)
	movq	%rsi, (%rsp)
	movq	%rdi, 8(%rsp)
	movq	%rbp, 16(%rsp)
	leaq	304(%rsp), %rdi
	leaq	352(%rsp), %rbx
	movq	-24(%rsp), %rsi                 # 8-byte Reload
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 256(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -112(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 208(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-112(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	208(%rsp), %rsi
	movq	256(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	-56(%rsp), %rdx                 # 8-byte Reload
	movq	72(%rdx), %rsi
	movq	%rsi, %rbp
	shrq	$48, %rbp
	movabsq	$281474976710655, %rcx          # imm = 0xFFFFFFFFFFFF
	andq	%rcx, %rsi
	movq	%rsi, -40(%rsp)                 # 8-byte Spill
	movabsq	$4294968273, %rcx               # imm = 0x1000003D1
	imulq	%rcx, %rbp
	addq	40(%rdx), %rbp
	movq	%rbp, %xmm0
	shrq	$52, %rbp
	addq	48(%rdx), %rbp
	movq	%rbp, %xmm1
	shrq	$52, %rbp
	addq	56(%rdx), %rbp
	punpcklqdq	%xmm1, %xmm0            # xmm0 = xmm0[0],xmm1[0]
	movq	%rbp, %xmm1
	shrq	$52, %rbp
	addq	64(%rdx), %rbp
	movq	-24(%rsp), %rax                 # 8-byte Reload
	leaq	40(%rax), %rsi
	leaq	256(%rsp), %rdi
	leaq	352(%rsp), %rbx
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 208(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 160(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -112(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	160(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-112(%rsp), %rsi
	movq	208(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	-32(%rsp), %rbx                 # 8-byte Reload
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 208(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 160(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -112(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	160(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-112(%rsp), %rsi
	movq	208(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	-56(%rsp), %rax                 # 8-byte Reload
	movups	80(%rax), %xmm2
	movups	96(%rax), %xmm3
	movaps	%xmm2, 208(%rsp)
	movdqa	.LCPI47_0(%rip), %xmm2          # xmm2 = [4503599627370495,4503599627370495]
	pand	%xmm2, %xmm0
	movaps	%xmm3, 224(%rsp)
	movq	112(%rax), %rax
	movaps	-16(%rsp), %xmm3
	movaps	%xmm3, -112(%rsp)
	movaps	(%rsp), %xmm3
	movaps	%xmm3, -96(%rsp)
	movq	16(%rsp), %rcx
	movq	%rcx, -80(%rsp)
	movdqa	-112(%rsp), %xmm3
	paddq	304(%rsp), %xmm3
	movdqa	%xmm3, -112(%rsp)
	movdqa	-96(%rsp), %xmm3
	paddq	320(%rsp), %xmm3
	movdqa	%xmm3, -96(%rsp)
	movq	%rbp, %xmm3
	shrq	$52, %rbp
	addq	-40(%rsp), %rbp                 # 8-byte Folded Reload
	movq	336(%rsp), %rcx
	addq	%rcx, -80(%rsp)
	punpcklqdq	%xmm3, %xmm1            # xmm1 = xmm1[0],xmm3[0]
	pand	%xmm2, %xmm1
	paddq	256(%rsp), %xmm0
	movq	%rax, 240(%rsp)
	movdqa	%xmm0, 160(%rsp)
	paddq	272(%rsp), %xmm1
	movdqa	%xmm1, 176(%rsp)
	addq	288(%rsp), %rbp
	movq	%rbp, 192(%rsp)
	leaq	120(%rsp), %rdi
	leaq	160(%rsp), %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 40(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -120(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 80(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-120(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	80(%rsp), %rsi
	movq	40(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	-112(%rsp), %rbp
	leaq	40(%rsp), %rdi
	movq	%rbp, %rbx
	leaq	120(%rsp), %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 80(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -64(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-64(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	80(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	120(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 80(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -64(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -120(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-64(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-120(%rsp), %rsi
	movq	80(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	80(%rsp), %rdi
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -120(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -48(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -64(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-48(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-64(%rsp), %rsi
	movq	-120(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	-16(%rsp), %rsi
	leaq	304(%rsp), %rbx
	movq	%rbp, %rdi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -120(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -48(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -64(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-48(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-64(%rsp), %rsi
	movq	-120(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movdqa	.LCPI47_1(%rip), %xmm0          # xmm0 = [18014381329608892,18014398509481980]
	psubq	-112(%rsp), %xmm0
	movdqa	%xmm0, -112(%rsp)
	movdqa	.LCPI47_2(%rip), %xmm1          # xmm1 = [18014398509481980,18014398509481980]
	psubq	-96(%rsp), %xmm1
	movdqa	%xmm1, -96(%rsp)
	movabsq	$1125899906842620, %rax         # imm = 0x3FFFFFFFFFFFC
	subq	-80(%rsp), %rax
	paddq	80(%rsp), %xmm0
	movq	%rax, -80(%rsp)
	movdqa	%xmm0, 80(%rsp)
	paddq	96(%rsp), %xmm1
	movdqa	%xmm1, 96(%rsp)
	addq	%rax, 112(%rsp)
	leaq	80(%rsp), %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, -120(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -48(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -64(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	-48(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-64(%rsp), %rsi
	movq	-120(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	32(%rsp), %rbp                  # 8-byte Reload
	leaq	80(%rbp), %rdi
	leaq	208(%rsp), %rbx
	leaq	160(%rsp), %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -120(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -48(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -64(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-48(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-64(%rsp), %rsi
	movq	-120(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	80(%rbp), %r10
	movq	88(%rbp), %r9
	movq	112(%rbp), %r8
	movq	%rbp, %r12
	movq	%r8, %rsi
	shrq	$48, %rsi
	movq	%r8, %rax
	movabsq	$281474976710655, %rcx          # imm = 0xFFFFFFFFFFFF
	andq	%rcx, %rax
	movabsq	$4294968273, %r15               # imm = 0x1000003D1
	imulq	%r15, %rsi
	addq	%r10, %rsi
	movq	%rsi, %rdi
	shrq	$52, %rdi
	addq	%r9, %rdi
	leaq	-1(%r15), %rbx
	xorq	%rsi, %rbx
	orq	%rdi, %rsi
	andq	%rdi, %rbx
	shrq	$52, %rdi
	movq	96(%rbp), %rcx
	addq	%rcx, %rdi
	orq	%rdi, %rsi
	andq	%rdi, %rbx
	shrq	$52, %rdi
	movq	104(%rbp), %rbp
	addq	%rbp, %rdi
	orq	%rdi, %rsi
	andq	%rdi, %rbx
	shrq	$52, %rdi
	addq	%rax, %rdi
	movabsq	$4222124650659840, %rax         # imm = 0xF000000000000
	xorq	%rdi, %rax
	andq	%rbx, %rax
	movq	-56(%rsp), %rdx                 # 8-byte Reload
	movslq	120(%rdx), %rbx
	movl	$1, %edx
	subq	%rbx, %rdx
	movabsq	$4503599627370495, %r14         # imm = 0xFFFFFFFFFFFFF
	andq	%r14, %rsi
	cmpq	%r14, %rax
	movl	$0, %eax
	cmovel	%edx, %eax
	orq	%rdi, %rsi
	cmovel	%edx, %eax
	movl	%eax, -32(%rsp)                 # 4-byte Spill
	addq	%rdx, %rdx
	imulq	%rdx, %r10
	movq	%r10, 80(%r12)
	imulq	%rdx, %r9
	movq	%r9, 88(%r12)
	imulq	%rdx, %rcx
	movq	%rcx, 96(%r12)
	imulq	%rdx, %rbp
	movq	%rbp, 104(%r12)
	imulq	%r8, %rdx
	movq	%rdx, 112(%r12)
	movq	-80(%rsp), %rbx
	movq	%rbx, 32(%r12)
	movaps	-112(%rsp), %xmm0
	movaps	-96(%rsp), %xmm1
	movups	%xmm1, 16(%r12)
	movups	%xmm0, (%r12)
	movabsq	$18014381329608892, %r8         # imm = 0x3FFFFBFFFFF0BC
	subq	40(%rsp), %r8
	movabsq	$18014398509481980, %r11        # imm = 0x3FFFFFFFFFFFFC
	movabsq	$1125899906842620, %rcx         # imm = 0x3FFFFFFFFFFFC
	subq	72(%rsp), %rcx
	movq	%rcx, -40(%rsp)                 # 8-byte Spill
	movq	(%r12), %rax
	addq	%r8, %rax
	addq	%rcx, %rbx
	movq	%rbx, %rdi
	shrq	$48, %rdi
	imulq	%r15, %rdi
	addq	%rax, %rdi
	movq	%r11, %r9
	subq	48(%rsp), %r9
	movq	8(%r12), %rax
	addq	%r9, %rax
	movq	%rdi, %rcx
	shrq	$52, %rcx
	addq	%rax, %rcx
	movq	%r11, %r10
	subq	56(%rsp), %r10
	movq	16(%r12), %rax
	addq	%r10, %rax
	movq	%rcx, %rdx
	shrq	$52, %rdx
	addq	%rax, %rdx
	subq	64(%rsp), %r11
	movq	24(%r12), %rax
	addq	%r11, %rax
	movq	%rdx, %rsi
	shrq	$52, %rsi
	addq	%rax, %rsi
	movabsq	$281474976710655, %r15          # imm = 0xFFFFFFFFFFFF
	andq	%r15, %rbx
	movq	%rsi, %rbp
	shrq	$52, %rbp
	addq	%rbx, %rbp
	movq	%r14, %r13
	andq	%r14, %rdi
	andq	%r14, %rdx
	movq	%rdx, %rbx
	andq	%rcx, %rbx
	andq	%r14, %rcx
	andq	%rsi, %rbx
	andq	%r14, %rsi
	movq	%rbp, %r14
	shrq	$48, %r14
	movq	%rbp, %rax
	xorq	%r15, %rax
	xorq	%r13, %rbx
	orq	%rax, %rbx
	sete	%al
	movabsq	$4503595332402222, %rbx         # imm = 0xFFFFEFFFFFC2E
	cmpq	%rbx, %rdi
	seta	%bl
	andb	%al, %bl
	movzbl	%bl, %ebx
	orq	%r14, %rbx
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	imulq	%rax, %rbx
	addq	%rdi, %rbx
	movq	%rbx, %rax
	shrq	$52, %rax
	addq	%rcx, %rax
	movq	%rax, %rcx
	shrq	$52, %rcx
	addq	%rdx, %rcx
	movq	%rcx, %rdx
	shrq	$52, %rdx
	addq	%rsi, %rdx
	movq	%rdx, %rsi
	shrq	$52, %rsi
	addq	%rbp, %rsi
	andq	%r13, %rbx
	movq	%rbx, (%r12)
	andq	%r13, %rax
	movq	%rax, 8(%r12)
	andq	%r13, %rcx
	movq	%rcx, 16(%r12)
	andq	%r13, %rdx
	movq	%rdx, 24(%r12)
	andq	%r15, %rsi
	movq	%rsi, 32(%r12)
	leaq	(%r8,%r8,2), %rax
	movq	%rax, 40(%rsp)
	movq	-112(%rsp), %rcx
	leaq	(%rax,%rcx,2), %rax
	movq	-104(%rsp), %rcx
	movq	-96(%rsp), %rdx
	movq	-88(%rsp), %rsi
	movq	-80(%rsp), %rdi
	movq	%rax, -112(%rsp)
	leaq	(%r9,%r9,2), %rax
	movq	%rax, 48(%rsp)
	leaq	(%rax,%rcx,2), %rax
	movq	%rax, -104(%rsp)
	leaq	(%r10,%r10,2), %rax
	movq	%rax, 56(%rsp)
	leaq	(%r11,%r11,2), %rcx
	movq	%rcx, 64(%rsp)
	movq	-40(%rsp), %rbp                 # 8-byte Reload
	leaq	(%rbp,%rbp,2), %rbp
	movq	%rbp, 72(%rsp)
	leaq	(%rax,%rdx,2), %rax
	movq	%rax, -96(%rsp)
	leaq	(%rcx,%rsi,2), %rax
	movq	%rax, -88(%rsp)
	leaq	(%rbp,%rdi,2), %rax
	movq	%rax, -80(%rsp)
	leaq	80(%rsp), %rbx
	leaq	-112(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, -120(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, -48(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, -64(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	-48(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	-64(%rsp), %rsi
	movq	-120(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	-112(%rsp), %r8
	movq	-104(%rsp), %rcx
	addq	120(%rsp), %r8
	addq	128(%rsp), %rcx
	movq	-96(%rsp), %rsi
	addq	136(%rsp), %rsi
	movq	-88(%rsp), %rdi
	addq	144(%rsp), %rdi
	movq	-80(%rsp), %rbp
	addq	152(%rsp), %rbp
	movabsq	$27021597764222970, %rax        # imm = 0x5FFFFFFFFFFFFA
	movq	%rax, %rbx
	subq	%rcx, %rbx
	movq	%rax, %rdx
	subq	%rsi, %rdx
	subq	%rdi, %rax
	movabsq	$1688849860263930, %rcx         # imm = 0x5FFFFFFFFFFFA
	subq	%rbp, %rcx
	movq	%rcx, %rsi
	shrq	$48, %rsi
	movabsq	$281474976710655, %rdi          # imm = 0xFFFFFFFFFFFF
	andq	%rdi, %rcx
	movabsq	$4294968273, %rdi               # imm = 0x1000003D1
	imulq	%rdi, %rsi
	subq	%r8, %rsi
	movabsq	$27021571994413338, %r10        # imm = 0x5FFFF9FFFFE91A
	addq	%rsi, %r10
	movq	%r10, %r13
	shrq	$52, %r13
	addq	%rbx, %r13
	movabsq	$4503599627370495, %rsi         # imm = 0xFFFFFFFFFFFFF
	andq	%rsi, %r10
	movq	%r13, %r12
	shrq	$52, %r12
	addq	%rdx, %r12
	andq	%rsi, %r13
	movq	%r12, %r15
	shrq	$52, %r15
	addq	%rax, %r15
	andq	%rsi, %r12
	movq	%r15, %r8
	shrq	$52, %r8
	addq	%rcx, %r8
	andq	%rsi, %r15
	movq	-56(%rsp), %rax                 # 8-byte Reload
	movslq	120(%rax), %rax
	leal	(,%rax,4), %ecx
	movl	$4, %ebp
	subl	%ecx, %ebp
	movslq	%ebp, %rcx
	movq	32(%rsp), %rsi                  # 8-byte Reload
	movq	(%rsi), %rbx
	imulq	%rcx, %rbx
	movq	%rbx, (%rsi)
	movq	8(%rsi), %rbp
	imulq	%rcx, %rbp
	movq	%rbp, 8(%rsi)
	movq	16(%rsi), %r14
	imulq	%rcx, %r14
	movq	%r14, 16(%rsi)
	movq	24(%rsi), %r11
	imulq	%rcx, %r11
	movq	%r11, 24(%rsi)
	movq	32(%rsi), %r9
	imulq	%rcx, %r9
	movq	%r9, 32(%rsi)
	imulq	%rcx, %r10
	movq	%r10, 40(%rsi)
	imulq	%rcx, %r13
	movq	%r13, 48(%rsi)
	imulq	%rcx, %r12
	movq	%r12, 56(%rsi)
	imulq	%rcx, %r15
	movq	%r15, 64(%rsi)
	imulq	%rcx, %r8
	movq	%r8, 72(%rsi)
	leaq	-1(%rax), %rcx
	negq	%rax
	andq	%rcx, %rbx
	movq	-24(%rsp), %rdi                 # 8-byte Reload
	movq	(%rdi), %rdx
	andq	%rax, %rdx
	orq	%rbx, %rdx
	movq	%rdx, (%rsi)
	andq	%rcx, %rbp
	movq	8(%rdi), %rdx
	andq	%rax, %rdx
	orq	%rbp, %rdx
	movq	%rdx, 8(%rsi)
	andq	%rcx, %r14
	movq	16(%rdi), %rdx
	andq	%rax, %rdx
	orq	%r14, %rdx
	movq	%rdx, 16(%rsi)
	andq	%rcx, %r11
	movq	24(%rdi), %rdx
	andq	%rax, %rdx
	orq	%r11, %rdx
	movq	%rdx, 24(%rsi)
	andq	%rcx, %r9
	movq	32(%rdi), %rdx
	andq	%rax, %rdx
	orq	%r9, %rdx
	movq	%rdx, 32(%rsi)
	andq	%rcx, %r10
	movq	40(%rdi), %rdx
	andq	%rax, %rdx
	orq	%r10, %rdx
	movq	%rdx, 40(%rsi)
	andq	%rcx, %r13
	movq	48(%rdi), %rdx
	andq	%rax, %rdx
	orq	%r13, %rdx
	movq	%rdx, 48(%rsi)
	andq	%rcx, %r12
	movq	56(%rdi), %rdx
	andq	%rax, %rdx
	orq	%r12, %rdx
	movq	%rdx, 56(%rsi)
	andq	%rcx, %r15
	movq	64(%rdi), %rdx
	andq	%rax, %rdx
	orq	%r15, %rdx
	movq	%rdx, 64(%rsi)
	andq	%rcx, %r8
	movq	72(%rdi), %rdx
	andq	%rax, %rdx
	orq	%r8, %rdx
	movq	%rdx, 72(%rsi)
	movq	%rcx, %xmm0
	andq	80(%rsi), %rcx
	andl	$1, %eax
	orq	%rcx, %rax
	movq	%rax, 80(%rsi)
	movdqu	88(%rsi), %xmm1
	pshufd	$68, %xmm0, %xmm0               # xmm0 = xmm0[0,1,0,1]
	pand	%xmm0, %xmm1
	movdqu	%xmm1, 88(%rsi)
	movdqu	104(%rsi), %xmm1
	pand	%xmm0, %xmm1
	movdqu	%xmm1, 104(%rsi)
	movl	-32(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 120(%rsi)
	addq	$392, %rsp                      # imm = 0x188
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end47:
	.size	secp256k1_gej_add_ge, .Lfunc_end47-secp256k1_gej_add_ge
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function init_worker
.LCPI48_0:
	.long	1779033703                      # 0x6a09e667
	.long	3144134277                      # 0xbb67ae85
	.long	1013904242                      # 0x3c6ef372
	.long	2773480762                      # 0xa54ff53a
.LCPI48_1:
	.long	1359893119                      # 0x510e527f
	.long	2600822924                      # 0x9b05688c
	.long	528734635                       # 0x1f83d9ab
	.long	1541459225                      # 0x5be0cd19
	.text
	.p2align	4, 0x90
	.type	init_worker,@function
init_worker:                            # @init_worker
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$360, %rsp                      # imm = 0x168
	.cfi_def_cfa_offset 416
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rdi, %rbx
	movabsq	$-4994812053365940165, %r12     # imm = 0xBAAEDCE6AF48A03B
	callq	make_seed
	movq	%rax, %r13
	xorl	%eax, %eax
	movq	%rbx, 56(%rsp)                  # 8-byte Spill
	shlq	$5, %rbx
	movq	%rbx, 40(%rsp)                  # 8-byte Spill
	movq	%r13, 48(%rsp)                  # 8-byte Spill
	.p2align	4, 0x90
.LBB48_1:                               # %.preheader
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB48_2 Depth 2
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	leaq	(%rax,%rax,8), %r14
	shlq	$5, %r14
	leaq	priv_offsets(%rip), %rax
	addq	%rax, %r14
	movq	40(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r14), %r15
	addq	$8, %r15
	addq	%rax, %r14
	.p2align	4, 0x90
.LBB48_2:                               #   Parent Loop BB48_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	addq	$1, 32(%r13)
	adcq	$0, 40(%r13)
	movaps	.LCPI48_0(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	%xmm0, 112(%rsp)
	movaps	.LCPI48_1(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movaps	%xmm0, 128(%rsp)
	movq	$0, 304(%rsp)
	movl	$48, %edx
	leaq	112(%rsp), %rbx
	movq	%rbx, %rdi
	movq	%r13, %rsi
	callq	secp256k1_sha256_write
	movq	304(%rsp), %rax
	movq	%rax, %rcx
	shrq	$29, %rcx
	shll	$24, %ecx
	movq	%rax, %rdx
	shrq	$21, %rdx
	andl	$16711680, %edx                 # imm = 0xFF0000
	orl	%ecx, %edx
	movq	%rax, %rcx
	shrq	$37, %rcx
	andl	$65280, %ecx                    # imm = 0xFF00
	orl	%edx, %ecx
	movq	%rax, %rdx
	shrq	$53, %rdx
	movzbl	%dl, %edx
	orl	%ecx, %edx
	movl	%edx, 72(%rsp)
	movl	%eax, %ecx
	shll	$27, %ecx
	movl	%eax, %edx
	shll	$11, %edx
	andl	$16711680, %edx                 # imm = 0xFF0000
	orl	%ecx, %edx
	movl	%eax, %ecx
	shrl	$5, %ecx
	andl	$65280, %ecx                    # imm = 0xFF00
	orl	%edx, %ecx
	movl	$55, %edx
	subl	%eax, %edx
	shrl	$21, %eax
	movzbl	%al, %eax
	orl	%ecx, %eax
	movl	%eax, 76(%rsp)
	andl	$63, %edx
	addq	$1, %rdx
	movq	%rbx, %rdi
	leaq	secp256k1_sha256_finalize.pad(%rip), %rsi
	callq	secp256k1_sha256_write
	movl	$8, %edx
	movq	%rbx, %rdi
	leaq	72(%rsp), %rsi
	callq	secp256k1_sha256_write
	movl	116(%rsp), %ecx
	bswapl	%ecx
	movl	124(%rsp), %esi
	bswapl	%esi
	movl	132(%rsp), %edi
	bswapl	%edi
	movl	140(%rsp), %eax
	bswapl	%eax
	movl	%eax, %edx
	shrl	$24, %edx
	movl	%eax, %ebx
	movl	%eax, %ebp
	shll	$24, %eax
	orl	%edx, %eax
	movl	136(%rsp), %edx
	bswapl	%edx
	shrl	$8, %ebx
	andl	$65280, %ebx                    # imm = 0xFF00
	orl	%ebx, %eax
	movl	%edi, %r9d
	shrl	$24, %r9d
	shll	$8, %ebp
	andl	$16711680, %ebp                 # imm = 0xFF0000
	orl	%ebp, %eax
	movl	%edx, %ebp
	shrl	$24, %ebp
	shlq	$32, %rbp
	orq	%rax, %rbp
	movl	%edx, %eax
	shrl	$16, %eax
	movzbl	%al, %eax
	shlq	$40, %rax
	orq	%rbp, %rax
	movzbl	%dh, %ebx
	shlq	$48, %rbx
	orq	%rax, %rbx
	shlq	$56, %rdx
	orq	%rbx, %rdx
	movq	%rdx, %r8
	movl	%edi, %eax
	movl	%edi, %ebp
	shll	$24, %edi
	orl	%r9d, %edi
	movl	128(%rsp), %edx
	bswapl	%edx
	shrl	$8, %eax
	andl	$65280, %eax                    # imm = 0xFF00
	orl	%eax, %edi
	movl	%esi, %eax
	shrl	$24, %eax
	shll	$8, %ebp
	andl	$16711680, %ebp                 # imm = 0xFF0000
	orl	%ebp, %edi
	movl	%edx, %ebp
	shrl	$24, %ebp
	shlq	$32, %rbp
	orq	%rdi, %rbp
	movl	%edx, %edi
	shrl	$16, %edi
	movzbl	%dil, %edi
	shlq	$40, %rdi
	orq	%rbp, %rdi
	movzbl	%dh, %ebx
	shlq	$48, %rbx
	orq	%rdi, %rbx
	shlq	$56, %rdx
	orq	%rbx, %rdx
	movq	%rdx, %r9
	movl	%esi, %edi
	movl	%esi, %ebp
	shll	$24, %esi
	orl	%eax, %esi
	movl	120(%rsp), %ebx
	bswapl	%ebx
	shrl	$8, %edi
	andl	$65280, %edi                    # imm = 0xFF00
	orl	%edi, %esi
	movl	%ecx, %eax
	shrl	$24, %eax
	shll	$8, %ebp
	andl	$16711680, %ebp                 # imm = 0xFF0000
	orl	%ebp, %esi
	movl	%ebx, %edi
	shrl	$24, %edi
	shlq	$32, %rdi
	orq	%rsi, %rdi
	movl	%ebx, %esi
	shrl	$16, %esi
	movzbl	%sil, %esi
	shlq	$40, %rsi
	orq	%rdi, %rsi
	movzbl	%bh, %edi
	shlq	$48, %rdi
	orq	%rsi, %rdi
	shlq	$56, %rbx
	orq	%rdi, %rbx
	movl	%ecx, %esi
	movl	%ecx, %edi
	shll	$24, %ecx
	orl	%eax, %ecx
	movl	112(%rsp), %eax
	bswapl	%eax
	shrl	$8, %esi
	andl	$65280, %esi                    # imm = 0xFF00
	orl	%esi, %ecx
	movl	%eax, %esi
	shrl	$16, %esi
	shll	$8, %edi
	andl	$16711680, %edi                 # imm = 0xFF0000
	orl	%edi, %ecx
	movl	%eax, %edi
	shrl	$24, %edi
	shlq	$32, %rdi
	orq	%rcx, %rdi
	movzbl	%sil, %ecx
	shlq	$40, %rcx
	orq	%rdi, %rcx
	movzbl	%ah, %esi
	shlq	$48, %rsi
	orq	%rcx, %rsi
	shlq	$56, %rax
	orq	%rsi, %rax
	cmpq	$-1, %rax
	setne	%cl
	cmpq	$-2, %rbx
	setb	%dl
	orb	%cl, %dl
	movzbl	%dl, %ecx
	xorl	%edx, %edx
	cmpq	$-1, %rbx
	sete	%dl
	movl	%ecx, %esi
	notl	%esi
	andl	%edx, %esi
	xorl	%edx, %edx
	cmpq	%r12, %r9
	setb	%dil
	seta	%dl
	orb	%dil, %cl
	movzbl	%cl, %ecx
	notl	%ecx
	andl	%ecx, %edx
	orl	%esi, %edx
	xorl	%esi, %esi
	movabsq	$-4624529908474429120, %rdi     # imm = 0xBFD25E8CD0364140
	cmpq	%rdi, %r8
	seta	%sil
	andl	%ecx, %esi
	orl	%edx, %esi
	movq	%rsi, %rcx
	movabsq	$4624529908474429119, %rdx      # imm = 0x402DA1732FC9BEBF
	imulq	%rdx, %rcx
	movq	%rsi, %rdx
	movabsq	$4994812053365940164, %rdi      # imm = 0x4551231950B75FC4
	imulq	%rdi, %rdx
	addq	%r8, %rcx
	adcq	%r9, %rdx
	movq	%rcx, (%r14)
	movq	%rdx, (%r15)
	adcq	%rsi, %rbx
	movq	%rbx, 8(%r15)
	adcq	$0, %rax
	movq	%rax, 16(%r15)
	testl	%esi, %esi
	jne	.LBB48_2
# %bb.3:                                #   in Loop: Header=BB48_1 Depth=1
	movq	cxt(%rip), %rdi
	addq	$8, %rdi
	leaq	112(%rsp), %rsi
	movq	%r14, %rdx
	callq	secp256k1_ecmult_gen
	imulq	$792, 24(%rsp), %rbp            # 8-byte Folded Reload
                                        # imm = 0x318
	leaq	offsets(%rip), %rax
	addq	%rax, %rbp
	imulq	$88, 56(%rsp), %rcx             # 8-byte Folded Reload
	movq	%rcx, 64(%rsp)                  # 8-byte Spill
	movl	232(%rsp), %eax
	movl	%eax, 80(%rcx,%rbp)
	leaq	192(%rsp), %rbx
	movq	%rbx, %rdi
	movq	%rbx, %rsi
	callq	secp256k1_fe_inv_var
	leaq	72(%rsp), %rdi
	movq	%rbx, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 320(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 8(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 16(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	8(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rsp), %rsi
	movq	320(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	72(%rsp), %rbx
	leaq	320(%rsp), %rdi
	leaq	192(%rsp), %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 16(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 32(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 8(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	32(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rsp), %rsi
	movq	16(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	72(%rsp), %rbx
	leaq	112(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 16(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 32(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 8(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	32(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rsp), %rsi
	movq	16(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	320(%rsp), %rbx
	leaq	152(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 16(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 32(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 8(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	32(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rsp), %rsi
	movq	16(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	48(%rsp), %r13                  # 8-byte Reload
	movabsq	$-4994812053365940165, %r12     # imm = 0xBAAEDCE6AF48A03B
	movq	$1, 192(%rsp)
	xorps	%xmm0, %xmm0
	leaq	192(%rsp), %rax
	movups	%xmm0, 24(%rax)
	movups	%xmm0, 8(%rax)
	movq	144(%rsp), %rax
	movq	64(%rsp), %rcx                  # 8-byte Reload
	movq	%rax, 32(%rcx,%rbp)
	movups	112(%rsp), %xmm0
	movups	128(%rsp), %xmm1
	movups	%xmm1, 16(%rcx,%rbp)
	movups	%xmm0, (%rcx,%rbp)
	movq	32(%rdi), %rax
	movq	%rax, 72(%rcx,%rbp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movups	(%rdi), %xmm0
	movups	16(%rdi), %xmm1
	movups	%xmm0, 40(%rcx,%rbp)
	movups	%xmm1, 56(%rcx,%rbp)
	addq	$1, %rax
	cmpq	$65535, %rax                    # imm = 0xFFFF
	jne	.LBB48_1
# %bb.4:
	movq	%r13, %rdi
	callq	free@PLT
	movq	stdout@GOTPCREL(%rip), %rbx
	movq	(%rbx), %rsi
	movl	$46, %edi
	callq	putc@PLT
	movq	(%rbx), %rdi
	callq	fflush@PLT
	xorl	%eax, %eax
	addq	$360, %rsp                      # imm = 0x168
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end48:
	.size	init_worker, .Lfunc_end48-init_worker
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function add_work
	.type	add_work,@function
add_work:                               # @add_work
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$104, %rsp
	.cfi_def_cfa_offset 160
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rdx, 32(%rsp)                  # 8-byte Spill
	movq	%rsi, %r14
	movq	%rdi, 40(%rsp)                  # 8-byte Spill
	movl	$48, %edi
	callq	malloc@PLT
	testq	%rax, %rax
	je	.LBB49_36
# %bb.1:
	leaq	184(%rsp), %r10
	leaq	160(%rsp), %rdx
	movl	16(%rdx), %ecx
	movl	%ecx, 16(%rax)
	movups	(%rdx), %xmm0
	movups	%xmm0, (%rax)
	movq	%rax, %r11
	addq	$20, %r11
	movups	(%r10), %xmm0
	movups	%xmm0, 20(%rax)
	movl	16(%r10), %ecx
	movl	%ecx, 36(%rax)
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movq	$0, 40(%rax)
	leaq	-1(%r14), %r15
	shrq	$4, %r15
	leaq	1(%r15), %r13
	movl	$16, %r9d
	subl	%r14d, %r9d
	xorl	%edx, %edx
	movl	$65535, %r8d                    # imm = 0xFFFF
	movq	%r14, %rdi
	xorl	%ebp, %ebp
	xorl	%ebx, %ebx
	jmp	.LBB49_2
	.p2align	4, 0x90
.LBB49_8:                               #   in Loop: Header=BB49_2 Depth=1
	movzwl	(%r10,%rbp,2), %eax
	rolw	$8, %ax
	leal	(%r9,%rdx), %ecx
	movl	$65535, %esi                    # imm = 0xFFFF
                                        # kill: def $cl killed $cl killed $ecx
	shll	%cl, %esi
	cmpq	$16, %rdi
	cmovael	%r8d, %esi
	andl	%eax, %esi
.LBB49_9:                               # %get_bits.exit
                                        #   in Loop: Header=BB49_2 Depth=1
	movzwl	%si, %eax
	movl	%ebx, %r12d
	addl	%eax, %r12d
	addq	$1, %rbp
	movzwl	%r12w, %ebx
	addq	$16, %rdx
	addq	$-16, %rdi
	cmpq	%rbp, %r13
	je	.LBB49_4
.LBB49_2:                               # =>This Inner Loop Header: Depth=1
	cmpq	%r14, %rdx
	jbe	.LBB49_8
# %bb.3:                                #   in Loop: Header=BB49_2 Depth=1
	xorl	%esi, %esi
	jmp	.LBB49_9
.LBB49_4:
	movq	%r11, %rbx
	leaq	table_lock(%rip), %rdi
	callq	pthread_mutex_lock@PLT
	testl	%eax, %eax
	jne	.LBB49_37
# %bb.5:                                # %mutex_lock.exit
	cmpb	$0, collision(%rip)
	je	.LBB49_10
# %bb.6:
	leaq	table_lock(%rip), %rdi
	callq	pthread_mutex_unlock@PLT
	testl	%eax, %eax
	jne	.LBB49_7
# %bb.34:                               # %mutex_unlock.exit
	movq	24(%rsp), %rdi                  # 8-byte Reload
	callq	free@PLT
	movl	$2, %eax
	jmp	.LBB49_35
.LBB49_10:
	andl	$8191, %r12d                    # imm = 0x1FFF
	leaq	table(%rip), %rcx
	movq	(%rcx,%r12,8), %rax
	testq	%rax, %rax
	je	.LBB49_11
# %bb.12:                               # %.preheader
	movl	$16, %esi
	subl	%r14d, %esi
	movl	$65535, %r10d                   # imm = 0xFFFF
	movq	24(%rsp), %r12                  # 8-byte Reload
	movq	%rbx, %r11
	.p2align	4, 0x90
.LBB49_13:                              # =>This Loop Header: Depth=1
                                        #     Child Loop BB49_14 Depth 2
	movq	%rax, %r9
	leaq	20(%rax), %r8
	movl	36(%rax), %eax
	movl	%eax, 96(%rsp)
	movups	20(%r9), %xmm0
	movaps	%xmm0, 80(%rsp)
	movl	16(%r11), %eax
	movl	%eax, 64(%rsp)
	movups	(%r11), %xmm0
	movaps	%xmm0, 48(%rsp)
	xorl	%ebp, %ebp
	movq	%r14, %rbx
	xorl	%edx, %edx
	xorl	%eax, %eax
	jmp	.LBB49_14
	.p2align	4, 0x90
.LBB49_16:                              # %get_bits.exit22.thread.i
                                        #   in Loop: Header=BB49_14 Depth=2
	cmpq	%r15, %rax
	leaq	1(%rax), %rcx
	setae	%dl
	addq	$16, %rbp
	addq	$-16, %rbx
	movq	%rcx, %rax
	cmpq	%rcx, %r13
	je	.LBB49_17
.LBB49_14:                              #   Parent Loop BB49_13 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	%r14, %rbp
	ja	.LBB49_16
# %bb.15:                               # %get_bits.exit22.i
                                        #   in Loop: Header=BB49_14 Depth=2
	leal	(%rsi,%rbp), %ecx
	movl	$65535, %edi                    # imm = 0xFFFF
                                        # kill: def $cl killed $cl killed $ecx
	shll	%cl, %edi
	cmpq	$16, %rbx
	cmovael	%r10d, %edi
	movzwl	48(%rsp,%rax,2), %ecx
	xorw	80(%rsp,%rax,2), %cx
	rolw	$8, %cx
	testw	%di, %cx
	je	.LBB49_16
.LBB49_17:                              # %is_equal.exit
                                        #   in Loop: Header=BB49_13 Depth=1
	testb	$1, %dl
	jne	.LBB49_18
# %bb.29:                               #   in Loop: Header=BB49_13 Depth=1
	movq	40(%r9), %rax
	testq	%rax, %rax
	jne	.LBB49_13
# %bb.30:                               # %._crit_edge
	addq	$40, %r9
	movq	%r12, (%r9)
	movq	40(%rsp), %rbx                  # 8-byte Reload
	testq	%rbx, %rbx
	jne	.LBB49_32
	jmp	.LBB49_33
.LBB49_11:
	leaq	(%rcx,%r12,8), %r9
	movq	24(%rsp), %r12                  # 8-byte Reload
	movq	%r12, (%r9)
	movq	40(%rsp), %rbx                  # 8-byte Reload
	testq	%rbx, %rbx
	je	.LBB49_33
.LBB49_32:
	leaq	.L.str.92(%rip), %rsi
	movq	%rbx, %rdi
	movl	%r14d, %edx
	movq	32(%rsp), %rcx                  # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	xorl	%eax, %eax
	callq	fprintf@PLT
	leaq	184(%rsp), %rcx
	movl	16(%rcx), %eax
	movl	%eax, 16(%rsp)
	movups	(%rcx), %xmm0
	movups	%xmm0, (%rsp)
	movq	%rbx, %rdi
	callq	write_hash160
	movl	$32, %edi
	movq	%rbx, %rsi
	callq	putc@PLT
	leaq	160(%rsp), %rcx
	movl	16(%rcx), %eax
	movl	%eax, 16(%rsp)
	movups	(%rcx), %xmm0
	movups	%xmm0, (%rsp)
	movq	%rbx, %rdi
	callq	write_hash160
	movl	$10, %edi
	movq	%rbx, %rsi
	callq	putc@PLT
	movq	%rbx, %rdi
	callq	fflush@PLT
.LBB49_33:                              # %write_work.exit68
	leaq	table_lock(%rip), %rdi
	callq	pthread_mutex_unlock@PLT
	movl	%eax, %ecx
	movl	$1, %eax
	testl	%ecx, %ecx
	jne	.LBB49_7
.LBB49_35:                              # %mutex_unlock.exit69
	addq	$104, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB49_18:
	.cfi_def_cfa_offset 160
	movl	16(%r12), %eax
	movl	%eax, 96(%rsp)
	movups	(%r12), %xmm0
	movaps	%xmm0, 80(%rsp)
	movups	(%r9), %xmm0
	movaps	%xmm0, 48(%rsp)
	movl	16(%r9), %eax
	movl	%eax, 64(%rsp)
	movl	$16, %esi
	subl	%r14d, %esi
	xorl	%edi, %edi
	movl	$65535, %r10d                   # imm = 0xFFFF
	movq	%r14, %rbx
	xorl	%edx, %edx
	xorl	%eax, %eax
	jmp	.LBB49_19
	.p2align	4, 0x90
.LBB49_21:                              # %get_bits.exit22.thread.i63
                                        #   in Loop: Header=BB49_19 Depth=1
	cmpq	%r15, %rax
	leaq	1(%rax), %rcx
	setae	%dl
	addq	$16, %rdi
	addq	$-16, %rbx
	movq	%rcx, %rax
	cmpq	%rcx, %r13
	je	.LBB49_22
.LBB49_19:                              # =>This Inner Loop Header: Depth=1
	cmpq	%r14, %rdi
	ja	.LBB49_21
# %bb.20:                               # %get_bits.exit22.i60
                                        #   in Loop: Header=BB49_19 Depth=1
	leal	(%rsi,%rdi), %ecx
	movl	$65535, %ebp                    # imm = 0xFFFF
                                        # kill: def $cl killed $cl killed $ecx
	shll	%cl, %ebp
	cmpq	$16, %rbx
	cmovael	%r10d, %ebp
	movzwl	80(%rsp,%rax,2), %ecx
	xorw	48(%rsp,%rax,2), %cx
	rolw	$8, %cx
	testw	%bp, %cx
	je	.LBB49_21
.LBB49_22:                              # %is_equal.exit65
	testb	$1, %dl
	je	.LBB49_25
# %bb.23:
	leaq	table_lock(%rip), %rdi
	callq	pthread_mutex_unlock@PLT
	testl	%eax, %eax
	jne	.LBB49_7
# %bb.24:                               # %mutex_unlock.exit66
	movq	%r12, %rdi
	callq	free@PLT
	movl	$-1, %eax
	jmp	.LBB49_35
.LBB49_25:
	movb	$1, collision(%rip)
	movl	16(%r9), %eax
	movl	%eax, c1+16(%rip)
	movups	(%r9), %xmm0
	movups	%xmm0, c1(%rip)
	movups	(%r12), %xmm0
	movups	%xmm0, c2(%rip)
	movl	16(%r12), %eax
	movl	%eax, c2+16(%rip)
	movups	(%r8), %xmm0
	movups	%xmm0, c3(%rip)
	movl	16(%r8), %eax
	movl	%eax, c3+16(%rip)
	movq	40(%rsp), %rbx                  # 8-byte Reload
	testq	%rbx, %rbx
	je	.LBB49_27
# %bb.26:
	leaq	.L.str.91(%rip), %rdi
	movl	$2, %esi
	movl	$1, %edx
	movq	%rbx, %rcx
	callq	fwrite@PLT
	leaq	.L.str.92(%rip), %rsi
	movq	%rbx, %rdi
	movl	%r14d, %edx
	movq	32(%rsp), %rcx                  # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	xorl	%eax, %eax
	callq	fprintf@PLT
	leaq	184(%rsp), %rcx
	movl	16(%rcx), %eax
	movl	%eax, 16(%rsp)
	movups	(%rcx), %xmm0
	movups	%xmm0, (%rsp)
	movq	%rbx, %rdi
	callq	write_hash160
	movl	$32, %edi
	movq	%rbx, %rsi
	callq	putc@PLT
	leaq	160(%rsp), %rcx
	movl	16(%rcx), %eax
	movl	%eax, 16(%rsp)
	movups	(%rcx), %xmm0
	movups	%xmm0, (%rsp)
	movq	%rbx, %rdi
	callq	write_hash160
	movl	$10, %edi
	movq	%rbx, %rsi
	callq	putc@PLT
	movq	%rbx, %rdi
	callq	fflush@PLT
.LBB49_27:                              # %write_work.exit
	leaq	table_lock(%rip), %rdi
	callq	pthread_mutex_unlock@PLT
	testl	%eax, %eax
	jne	.LBB49_7
# %bb.28:                               # %mutex_unlock.exit67
	movq	%r12, %rdi
	callq	free@PLT
	xorl	%eax, %eax
	jmp	.LBB49_35
.LBB49_36:
	movq	stderr@GOTPCREL(%rip), %rax
	movq	(%rax), %rdi
	leaq	.L.str.90(%rip), %rsi
	movl	$48, %edx
	xorl	%eax, %eax
	callq	fprintf@PLT
	movl	$1, %edi
	callq	exit@PLT
.LBB49_37:
	leaq	.L.str.80(%rip), %rdi
	leaq	.L.str.23(%rip), %rsi
	leaq	.L__PRETTY_FUNCTION__.mutex_lock(%rip), %rcx
	movl	$62, %edx
	callq	__assert_fail@PLT
.LBB49_7:
	leaq	.L.str.80(%rip), %rdi
	leaq	.L.str.23(%rip), %rsi
	leaq	.L__PRETTY_FUNCTION__.mutex_unlock(%rip), %rcx
	movl	$68, %edx
	callq	__assert_fail@PLT
.Lfunc_end49:
	.size	add_work, .Lfunc_end49-add_work
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function worker
.LCPI50_0:
	.long	1779033703                      # 0x6a09e667
	.long	3144134277                      # 0xbb67ae85
	.long	1013904242                      # 0x3c6ef372
	.long	2773480762                      # 0xa54ff53a
.LCPI50_1:
	.long	1359893119                      # 0x510e527f
	.long	2600822924                      # 0x9b05688c
	.long	528734635                       # 0x1f83d9ab
	.long	1541459225                      # 0x5be0cd19
	.text
	.p2align	4, 0x90
	.type	worker,@function
worker:                                 # @worker
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$600, %rsp                      # imm = 0x258
	.cfi_def_cfa_offset 656
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movl	16(%rdi), %eax
	movl	%eax, 112(%rsp)
	movups	(%rdi), %xmm0
	movaps	%xmm0, 96(%rsp)
	movq	24(%rdi), %r13
	movq	32(%rdi), %r14
	movq	48(%rdi), %rbx
	movl	72(%rdi), %eax
	movl	%eax, 272(%rsp)
	movups	56(%rdi), %xmm0
	movaps	%xmm0, 256(%rsp)
	movq	40(%rdi), %rax
	movq	%rax, 344(%rsp)                 # 8-byte Spill
	callq	free@PLT
	movb	$16, %cl
	subb	%r13b, %cl
	movl	$65535, %eax                    # imm = 0xFFFF
	movl	$65535, %edx                    # imm = 0xFFFF
	shll	%cl, %edx
	cmpq	$16, %r13
	cmovael	%eax, %edx
	movl	%edx, 132(%rsp)                 # 4-byte Spill
	leaq	-1(%r13), %rax
	movq	%rax, 240(%rsp)                 # 8-byte Spill
	shrq	$4, %rax
	movq	%rax, 136(%rsp)                 # 8-byte Spill
	leaq	-1(%r14), %r12
	shrq	$4, %r12
	movq	%r14, %rax
	shrq	$3, %rax
	cmpq	$2, %rax
	movl	$1, %edx
	movq	%rax, %rcx
	movq	%rax, 296(%rsp)                 # 8-byte Spill
	cmovaeq	%rax, %rdx
	movq	%rdx, 288(%rsp)                 # 8-byte Spill
	movl	%r14d, %ecx
	andb	$7, %cl
	movb	$-1, %al
	shrb	%cl, %al
	movb	%al, 55(%rsp)                   # 1-byte Spill
	leaq	-1(%rbx), %r8
	shrq	$4, %r8
	movl	$32, %eax
	subl	%r13d, %eax
	movl	%eax, 128(%rsp)                 # 4-byte Spill
	movl	$16, %r9d
	movl	$16, %eax
	subl	%r14d, %eax
	movq	%rax, 336(%rsp)                 # 8-byte Spill
	movq	%rbx, 216(%rsp)                 # 8-byte Spill
	subl	%ebx, %r9d
	leaq	-16(%r13), %rax
	movq	%rax, 224(%rsp)                 # 8-byte Spill
	leaq	1(%r12), %rax
	movq	%rax, 328(%rsp)                 # 8-byte Spill
	leaq	1(%r8), %r10
	movq	%r13, 184(%rsp)                 # 8-byte Spill
	movq	%r14, 248(%rsp)                 # 8-byte Spill
	movq	%r12, 232(%rsp)                 # 8-byte Spill
	movq	%r8, 320(%rsp)                  # 8-byte Spill
	movq	%r9, 312(%rsp)                  # 8-byte Spill
	movq	%r10, 304(%rsp)                 # 8-byte Spill
	jmp	.LBB50_1
	.p2align	4, 0x90
.LBB50_28:                              #   in Loop: Header=BB50_1 Depth=1
	movq	stdout@GOTPCREL(%rip), %rbx
	movq	(%rbx), %rsi
	movl	$85, %edi
	callq	putc@PLT
	movq	(%rbx), %rdi
	callq	fflush@PLT
	movups	72(%rsp), %xmm0
	movaps	%xmm0, 96(%rsp)
	movl	88(%rsp), %eax
	movl	%eax, 112(%rsp)
.LBB50_1:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB50_3 Depth 2
                                        #     Child Loop BB50_10 Depth 2
                                        #       Child Loop BB50_16 Depth 3
                                        #     Child Loop BB50_24 Depth 2
                                        #     Child Loop BB50_38 Depth 2
	movaps	96(%rsp), %xmm0
	movaps	%xmm0, 192(%rsp)
	movzwl	192(%rsp), %eax
	rolw	$8, %ax
	andl	132(%rsp), %eax                 # 4-byte Folded Reload
	movzwl	%ax, %eax
	shlq	$7, %rax
	movl	112(%rsp), %ecx
	movl	%ecx, 208(%rsp)
	leaq	bases(%rip), %rcx
	movups	112(%rax,%rcx), %xmm0
	movaps	%xmm0, 464(%rsp)
	movups	96(%rax,%rcx), %xmm0
	movaps	%xmm0, 448(%rsp)
	movups	80(%rax,%rcx), %xmm0
	movaps	%xmm0, 432(%rsp)
	movups	64(%rax,%rcx), %xmm0
	movaps	%xmm0, 416(%rsp)
	movups	(%rax,%rcx), %xmm0
	movdqu	16(%rax,%rcx), %xmm1
	movdqu	32(%rax,%rcx), %xmm2
	movups	48(%rax,%rcx), %xmm3
	movaps	%xmm3, 400(%rsp)
	movdqa	%xmm2, 384(%rsp)
	movdqa	%xmm1, 368(%rsp)
	movaps	%xmm0, 352(%rsp)
	cmpq	$16, 240(%rsp)                  # 8-byte Folded Reload
	jae	.LBB50_2
.LBB50_7:                               # %gen_pub_key.exit
                                        #   in Loop: Header=BB50_1 Depth=1
	leaq	432(%rsp), %rbp
	movq	%rbp, %rdi
	movq	%rbp, %rsi
	callq	secp256k1_fe_inv_var
	leaq	144(%rsp), %rdi
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 560(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 56(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 64(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	56(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	64(%rsp), %rsi
	movq	560(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	144(%rsp), %rbx
	leaq	560(%rsp), %rdi
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 64(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 120(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 56(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	120(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	56(%rsp), %rsi
	movq	64(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	144(%rsp), %rbx
	leaq	352(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 64(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 120(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 56(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	120(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	56(%rsp), %rsi
	movq	64(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	560(%rsp), %rbx
	leaq	392(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 64(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 120(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 56(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	120(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	56(%rsp), %rsi
	movq	64(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	360(%rsp), %rax
	movq	368(%rsp), %rdx
	movq	376(%rsp), %rbp
	movq	384(%rsp), %rdi
	movb	392(%rsp), %bl
	movq	%rax, %rcx
	shlq	$52, %rcx
	orq	352(%rsp), %rcx
	andb	$1, %bl
	orb	$2, %bl
	movb	%bl, 144(%rsp)
	shlq	$16, %rdi
	movq	%rbp, %rsi
	shrq	$36, %rsi
	orq	%rdi, %rsi
	bswapq	%rsi
	movq	%rsi, 145(%rsp)
	shlq	$28, %rbp
	movq	%rdx, %rsi
	shrq	$24, %rsi
	orq	%rbp, %rsi
	bswapq	%rsi
	movq	%rsi, 153(%rsp)
	shlq	$40, %rdx
	shrq	$12, %rax
	orq	%rdx, %rax
	bswapq	%rax
	movq	%rax, 161(%rsp)
	bswapq	%rcx
	movq	%rcx, 169(%rsp)
	leaq	72(%rsp), %rdi
	leaq	144(%rsp), %rsi
	callq	hash160
	cmpb	$0, stop(%rip)
	movq	248(%rsp), %r14                 # 8-byte Reload
	movq	232(%rsp), %r12                 # 8-byte Reload
	je	.LBB50_8
.LBB50_21:                              # %.critedge
                                        #   in Loop: Header=BB50_1 Depth=1
	cmpb	$0, stop(%rip)
	movq	184(%rsp), %r13                 # 8-byte Reload
	movq	216(%rsp), %r15                 # 8-byte Reload
	jne	.LBB50_51
# %bb.22:                               #   in Loop: Header=BB50_1 Depth=1
	testq	%r14, %r14
	je	.LBB50_30
# %bb.23:                               # %.preheader.i34.preheader
                                        #   in Loop: Header=BB50_1 Depth=1
	xorl	%eax, %eax
	movq	%r14, %rdx
	xorl	%esi, %esi
	xorl	%edi, %edi
	movq	336(%rsp), %r8                  # 8-byte Reload
	movq	328(%rsp), %r9                  # 8-byte Reload
	jmp	.LBB50_24
	.p2align	4, 0x90
.LBB50_26:                              # %get_bits.exit.thread.i42
                                        #   in Loop: Header=BB50_24 Depth=2
	cmpq	%r12, %rdi
	leaq	1(%rdi), %rcx
	setae	%sil
	addq	$16, %rax
	addq	$-16, %rdx
	movq	%rcx, %rdi
	cmpq	%rcx, %r9
	je	.LBB50_27
.LBB50_24:                              # %.preheader.i34
                                        #   Parent Loop BB50_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	%rax, %r14
	jb	.LBB50_26
# %bb.25:                               # %get_bits.exit.i39
                                        #   in Loop: Header=BB50_24 Depth=2
	movzwl	96(%rsp,%rdi,2), %ebp
	rolw	$8, %bp
	leal	(%r8,%rax), %ecx
	movl	$65535, %ebx                    # imm = 0xFFFF
                                        # kill: def $cl killed $cl killed $ecx
	shll	%cl, %ebx
	cmpq	$16, %rdx
	movl	$65535, %ecx                    # imm = 0xFFFF
	cmovael	%ecx, %ebx
	testw	%bx, %bp
	je	.LBB50_26
.LBB50_27:                              # %is_distinguished.exit44
                                        #   in Loop: Header=BB50_1 Depth=1
	testb	$1, %sil
	je	.LBB50_28
	jmp	.LBB50_30
	.p2align	4, 0x90
.LBB50_2:                               # %.lr.ph.i60.preheader
                                        #   in Loop: Header=BB50_1 Depth=1
	movl	$16, %r12d
	leaq	offsets(%rip), %rbx
	movq	224(%rsp), %r15                 # 8-byte Reload
	movl	128(%rsp), %r14d                # 4-byte Reload
	xorl	%ebp, %ebp
	jmp	.LBB50_3
	.p2align	4, 0x90
.LBB50_5:                               #   in Loop: Header=BB50_3 Depth=2
	movzwl	194(%rsp,%rbp,2), %edx
	rolw	$8, %dx
	movl	$65535, %eax                    # imm = 0xFFFF
	movl	%r14d, %ecx
	shll	%cl, %eax
	cmpq	$16, %r15
	movl	$65535, %ecx                    # imm = 0xFFFF
	cmovael	%ecx, %eax
	andl	%edx, %eax
.LBB50_6:                               # %get_bits.exit.i62
                                        #   in Loop: Header=BB50_3 Depth=2
	movzwl	%ax, %eax
	imulq	$792, %rax, %rdx                # imm = 0x318
	addq	%rbx, %rdx
	leaq	352(%rsp), %rdi
	movq	%rdi, %rsi
	callq	secp256k1_gej_add_ge
	addq	$1, %rbp
	addl	$16, %r14d
	addq	$-16, %r15
	addq	$16, %r12
	addq	$88, %rbx
	cmpq	%rbp, 136(%rsp)                 # 8-byte Folded Reload
	je	.LBB50_7
.LBB50_3:                               # %.lr.ph.i60
                                        #   Parent Loop BB50_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	%r12, %r13
	jae	.LBB50_5
# %bb.4:                                #   in Loop: Header=BB50_3 Depth=2
	xorl	%eax, %eax
	jmp	.LBB50_6
	.p2align	4, 0x90
.LBB50_8:                               # %.lr.ph
                                        #   in Loop: Header=BB50_1 Depth=1
	testq	%r14, %r14
	je	.LBB50_29
# %bb.9:                                # %.preheader.i.preheader
                                        #   in Loop: Header=BB50_1 Depth=1
	xorl	%eax, %eax
	xorl	%edx, %edx
	jmp	.LBB50_10
	.p2align	4, 0x90
.LBB50_20:                              # %gen_pub_key.exit82
                                        #   in Loop: Header=BB50_10 Depth=2
	leaq	432(%rsp), %rbp
	movq	%rbp, %rdi
	movq	%rbp, %rsi
	callq	secp256k1_fe_inv_var
	leaq	144(%rsp), %rdi
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movabsq	$4503599627370495, %r15         # imm = 0xFFFFFFFFFFFFF
	leaq	(%r10,%r10), %rax
	mulq	%r13
	movq	%rax, %rbx
	movq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r14, %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	shrdq	$52, %r9, %r8
	movq	%rbx, %rsi
	andq	%r15, %rsi
	movq	%rsi, 560(%rsp)
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	addq	%r14, %r14
	movq	%r10, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r11,%r11), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r12, %rax
	mulq	%r12
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 56(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 64(%rsp)
	movq	%r10, %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	%r11, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	leaq	(%r12,%r12), %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rsi
	andq	%r15, %rsi
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	shlq	$4, %rsi
	movq	56(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r10
	movq	%r10, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r12, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%r13, %rax
	mulq	%r13
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	xorq	%rcx, %rcx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	%r10, %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	64(%rsp), %rsi
	movq	560(%rsp), %r10
	movq	%r11, %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r13, %rax
	mulq	%r14
	addq	%rax, %rbx
	adcq	%rdx, %rcx
	movq	%rbx, %rax
	andq	%r15, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %rcx, %rbx
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rbx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	andq	%r15, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	144(%rsp), %rbx
	leaq	560(%rsp), %rdi
	movq	%rbp, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 64(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 120(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 56(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	120(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	56(%rsp), %rsi
	movq	64(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	144(%rsp), %rbx
	leaq	352(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 64(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 120(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 56(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	120(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	56(%rsp), %rsi
	movq	64(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	leaq	560(%rsp), %rbx
	leaq	392(%rsp), %rdi
	movq	%rdi, %rsi
	#APP
	movq	(%rsi), %r10
	movq	8(%rsi), %r11
	movq	16(%rsi), %r12
	movq	24(%rsi), %r13
	movq	32(%rsi), %r14
	movq	(%rbx), %rax
	mulq	%r13
	movq	%rax, %rcx
	movq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r14
	movq	%rax, %r8
	movq	%rdx, %r9
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	shrdq	$52, %r9, %r8
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	movq	%rsi, 64(%rsp)
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	8(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r10
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%r8, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%rsi, %rax
	shrq	$48, %rax
	movq	%rax, 120(%rsp)
	movabsq	$281474976710655, %rax          # imm = 0xFFFFFFFFFFFF
	andq	%rax, %rsi
	movq	%rsi, 56(%rsp)
	movq	(%rbx), %rax
	mulq	%r10
	movq	%rax, %r8
	movq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	16(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r11
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rsi
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rsi
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	shlq	$4, %rsi
	movq	120(%rsp), %rax
	orq	%rax, %rsi
	movabsq	$4294968273, %rax               # imm = 0x1000003D1
	mulq	%rsi
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, (%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	24(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r12
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	xorq	%r15, %r15
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 8(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	movq	(%rbx), %rax
	mulq	%r12
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	8(%rbx), %rax
	mulq	%r11
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	16(%rbx), %rax
	mulq	%r10
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	56(%rsp), %rsi
	movq	64(%rsp), %r10
	movq	24(%rbx), %rax
	mulq	%r14
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	32(%rbx), %rax
	mulq	%r13
	addq	%rax, %rcx
	adcq	%rdx, %r15
	movq	%rcx, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	shrdq	$52, %r15, %rcx
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 16(%rdi)
	shrdq	$52, %r9, %r8
	xorq	%r9, %r9
	addq	%r10, %r8
	movq	%rcx, %rax
	movabsq	$68719492368, %rdx              # imm = 0x1000003D10
	mulq	%rdx
	addq	%rax, %r8
	adcq	%rdx, %r9
	movq	%r8, %rax
	movabsq	$4503599627370495, %rdx         # imm = 0xFFFFFFFFFFFFF
	andq	%rdx, %rax
	movq	%rax, 24(%rdi)
	shrdq	$52, %r9, %r8
	addq	%rsi, %r8
	movq	%r8, 32(%rdi)

	#NO_APP
	movq	360(%rsp), %rax
	movq	368(%rsp), %rdx
	movq	376(%rsp), %rbp
	movq	384(%rsp), %rdi
	movb	392(%rsp), %bl
	movq	%rax, %rcx
	shlq	$52, %rcx
	orq	352(%rsp), %rcx
	andb	$1, %bl
	orb	$2, %bl
	movb	%bl, 144(%rsp)
	shlq	$16, %rdi
	movq	%rbp, %rsi
	shrq	$36, %rsi
	orq	%rdi, %rsi
	bswapq	%rsi
	movq	%rsi, 145(%rsp)
	shlq	$28, %rbp
	movq	%rdx, %rsi
	shrq	$24, %rsi
	orq	%rbp, %rsi
	bswapq	%rsi
	movq	%rsi, 153(%rsp)
	shlq	$40, %rdx
	shrq	$12, %rax
	orq	%rdx, %rax
	bswapq	%rax
	movq	%rax, 161(%rsp)
	bswapq	%rcx
	movq	%rcx, 169(%rsp)
	leaq	72(%rsp), %rdi
	leaq	144(%rsp), %rsi
	callq	hash160
	xorl	%eax, %eax
	cmpb	$0, stop(%rip)
	movl	$0, %edx
	movq	248(%rsp), %r14                 # 8-byte Reload
	movq	232(%rsp), %r12                 # 8-byte Reload
	jne	.LBB50_21
.LBB50_10:                              # %.preheader.i
                                        #   Parent Loop BB50_1 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB50_16 Depth 3
	movq	%rdx, %rcx
	shlq	$4, %rcx
	movq	%r14, %rsi
	subq	%rcx, %rsi
	jb	.LBB50_12
# %bb.11:                               # %get_bits.exit.i
                                        #   in Loop: Header=BB50_10 Depth=2
	movzwl	72(%rsp,%rdx,2), %edi
	rolw	$8, %di
	movb	$16, %cl
	subb	%sil, %cl
	movl	$65535, %ebp                    # imm = 0xFFFF
	shll	%cl, %ebp
	cmpq	$16, %rsi
	movl	$65535, %ecx                    # imm = 0xFFFF
	cmovael	%ecx, %ebp
	testw	%bp, %di
	jne	.LBB50_13
.LBB50_12:                              # %get_bits.exit.thread.i
                                        #   in Loop: Header=BB50_10 Depth=2
	leaq	1(%rdx), %rcx
	cmpq	%r12, %rdx
	setae	%al
	movq	%rcx, %rdx
	jne	.LBB50_10
.LBB50_13:                              # %is_distinguished.exit
                                        #   in Loop: Header=BB50_10 Depth=2
	testb	$1, %al
	jne	.LBB50_21
# %bb.14:                               #   in Loop: Header=BB50_10 Depth=2
	movups	72(%rsp), %xmm0
	movaps	%xmm0, 192(%rsp)
	movzwl	192(%rsp), %eax
	rolw	$8, %ax
	andl	132(%rsp), %eax                 # 4-byte Folded Reload
	movzwl	%ax, %eax
	shlq	$7, %rax
	movl	88(%rsp), %ecx
	movl	%ecx, 208(%rsp)
	leaq	bases(%rip), %rcx
	movups	112(%rax,%rcx), %xmm0
	movaps	%xmm0, 464(%rsp)
	movups	96(%rax,%rcx), %xmm0
	movaps	%xmm0, 448(%rsp)
	movups	80(%rax,%rcx), %xmm0
	movaps	%xmm0, 432(%rsp)
	movups	64(%rax,%rcx), %xmm0
	movaps	%xmm0, 416(%rsp)
	movups	(%rax,%rcx), %xmm0
	movdqu	16(%rax,%rcx), %xmm1
	movdqu	32(%rax,%rcx), %xmm2
	movups	48(%rax,%rcx), %xmm3
	movaps	%xmm3, 400(%rsp)
	movdqa	%xmm2, 384(%rsp)
	movdqa	%xmm1, 368(%rsp)
	movaps	%xmm0, 352(%rsp)
	cmpq	$16, 240(%rsp)                  # 8-byte Folded Reload
	movq	184(%rsp), %r13                 # 8-byte Reload
	jb	.LBB50_20
# %bb.15:                               # %.lr.ph.i77.preheader
                                        #   in Loop: Header=BB50_10 Depth=2
	movl	$16, %r12d
	leaq	offsets(%rip), %rbx
	movq	224(%rsp), %r15                 # 8-byte Reload
	movl	128(%rsp), %r14d                # 4-byte Reload
	xorl	%ebp, %ebp
	jmp	.LBB50_16
	.p2align	4, 0x90
.LBB50_18:                              #   in Loop: Header=BB50_16 Depth=3
	movzwl	194(%rsp,%rbp,2), %edx
	rolw	$8, %dx
	movl	$65535, %eax                    # imm = 0xFFFF
	movl	%r14d, %ecx
	shll	%cl, %eax
	cmpq	$16, %r15
	movl	$65535, %ecx                    # imm = 0xFFFF
	cmovael	%ecx, %eax
	andl	%edx, %eax
.LBB50_19:                              # %get_bits.exit.i81
                                        #   in Loop: Header=BB50_16 Depth=3
	movzwl	%ax, %eax
	imulq	$792, %rax, %rdx                # imm = 0x318
	addq	%rbx, %rdx
	leaq	352(%rsp), %rdi
	movq	%rdi, %rsi
	callq	secp256k1_gej_add_ge
	addq	$1, %rbp
	addl	$16, %r14d
	addq	$-16, %r15
	addq	$16, %r12
	addq	$88, %rbx
	cmpq	%rbp, 136(%rsp)                 # 8-byte Folded Reload
	je	.LBB50_20
.LBB50_16:                              # %.lr.ph.i77
                                        #   Parent Loop BB50_1 Depth=1
                                        #     Parent Loop BB50_10 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	cmpq	%r12, %r13
	jae	.LBB50_18
# %bb.17:                               #   in Loop: Header=BB50_16 Depth=3
	xorl	%eax, %eax
	jmp	.LBB50_19
.LBB50_29:                              # %.critedge.thread
                                        #   in Loop: Header=BB50_1 Depth=1
	cmpb	$0, stop(%rip)
	movq	184(%rsp), %r13                 # 8-byte Reload
	movq	216(%rsp), %r15                 # 8-byte Reload
	jne	.LBB50_51
	.p2align	4, 0x90
.LBB50_30:                              # %is_distinguished.exit44.thread
                                        #   in Loop: Header=BB50_1 Depth=1
	movl	88(%rsp), %eax
	movl	%eax, 40(%rsp)
	movups	72(%rsp), %xmm0
	movups	%xmm0, 24(%rsp)
	movl	112(%rsp), %eax
	movl	%eax, 16(%rsp)
	movaps	96(%rsp), %xmm0
	movups	%xmm0, (%rsp)
	movq	344(%rsp), %rdi                 # 8-byte Reload
	movq	%r13, %rsi
	movq	%r14, %rdx
	callq	add_work
                                        # kill: def $eax killed $eax def $rax
	addl	$1, %eax
	cmpl	$3, %eax
	ja	.LBB50_36
# %bb.31:                               # %is_distinguished.exit44.thread
                                        #   in Loop: Header=BB50_1 Depth=1
	leaq	.LJTI50_0(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	jmpq	*%rax
.LBB50_32:                              #   in Loop: Header=BB50_1 Depth=1
	movq	stdout@GOTPCREL(%rip), %rbx
	movq	(%rbx), %rsi
	movl	$88, %edi
	callq	putc@PLT
	movq	(%rbx), %rdi
	callq	fflush@PLT
	movaps	.LCPI50_0(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	%xmm0, 352(%rsp)
	movaps	.LCPI50_1(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movaps	%xmm0, 368(%rsp)
	movq	$0, 544(%rsp)
	movl	$20, %edx
	leaq	352(%rsp), %rbx
	movq	%rbx, %rdi
	leaq	72(%rsp), %rsi
	callq	secp256k1_sha256_write
	movq	544(%rsp), %rax
	movq	%rax, %rcx
	shrq	$29, %rcx
	shll	$24, %ecx
	movq	%rax, %rdx
	shrq	$21, %rdx
	andl	$16711680, %edx                 # imm = 0xFF0000
	orl	%ecx, %edx
	movq	%rax, %rcx
	shrq	$37, %rcx
	andl	$65280, %ecx                    # imm = 0xFF00
	orl	%edx, %ecx
	movq	%rax, %rdx
	shrq	$53, %rdx
	movzbl	%dl, %edx
	orl	%ecx, %edx
	movl	%edx, 144(%rsp)
	movl	%eax, %ecx
	shll	$27, %ecx
	movl	%eax, %edx
	shll	$11, %edx
	andl	$16711680, %edx                 # imm = 0xFF0000
	orl	%ecx, %edx
	movl	%eax, %ecx
	shrl	$5, %ecx
	andl	$65280, %ecx                    # imm = 0xFF00
	orl	%edx, %ecx
	movl	$55, %edx
	subl	%eax, %edx
	shrl	$21, %eax
	movzbl	%al, %eax
	orl	%ecx, %eax
	movl	%eax, 148(%rsp)
	andl	$63, %edx
	addq	$1, %rdx
	movq	%rbx, %rdi
	leaq	secp256k1_sha256_finalize.pad(%rip), %rsi
	callq	secp256k1_sha256_write
	movl	$8, %edx
	movq	%rbx, %rdi
	leaq	144(%rsp), %rsi
	callq	secp256k1_sha256_write
	movdqa	352(%rsp), %xmm0
	movdqa	%xmm0, %xmm1
	pxor	%xmm2, %xmm2
	punpckhbw	%xmm2, %xmm1            # xmm1 = xmm1[8],xmm2[8],xmm1[9],xmm2[9],xmm1[10],xmm2[10],xmm1[11],xmm2[11],xmm1[12],xmm2[12],xmm1[13],xmm2[13],xmm1[14],xmm2[14],xmm1[15],xmm2[15]
	pshuflw	$27, %xmm1, %xmm1               # xmm1 = xmm1[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm1, %xmm1               # xmm1 = xmm1[0,1,2,3,7,6,5,4]
	punpcklbw	%xmm2, %xmm0            # xmm0 = xmm0[0],xmm2[0],xmm0[1],xmm2[1],xmm0[2],xmm2[2],xmm0[3],xmm2[3],xmm0[4],xmm2[4],xmm0[5],xmm2[5],xmm0[6],xmm2[6],xmm0[7],xmm2[7]
	pshuflw	$27, %xmm0, %xmm0               # xmm0 = xmm0[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm0, %xmm0               # xmm0 = xmm0[0,1,2,3,7,6,5,4]
	packuswb	%xmm1, %xmm0
	movl	368(%rsp), %eax
	bswapl	%eax
	movdqa	%xmm0, 96(%rsp)
	movl	%eax, 112(%rsp)
	movl	%eax, 368(%rsp)
	movdqa	%xmm0, 352(%rsp)
	cmpq	$8, %r14
	jb	.LBB50_34
# %bb.33:                               # %.lr.ph.i
                                        #   in Loop: Header=BB50_1 Depth=1
	leaq	352(%rsp), %rdi
	xorl	%esi, %esi
	movq	288(%rsp), %rdx                 # 8-byte Reload
	callq	memset@PLT
.LBB50_34:                              # %make_distinguished.exit
                                        #   in Loop: Header=BB50_1 Depth=1
	movq	296(%rsp), %rax                 # 8-byte Reload
	movb	55(%rsp), %cl                   # 1-byte Reload
	andb	%cl, 352(%rsp,%rax)
	movaps	352(%rsp), %xmm0
	movaps	%xmm0, 96(%rsp)
	movl	368(%rsp), %eax
	movl	%eax, 112(%rsp)
	jmp	.LBB50_1
	.p2align	4, 0x90
.LBB50_35:                              #   in Loop: Header=BB50_1 Depth=1
	movq	stdout@GOTPCREL(%rip), %rbx
	movq	(%rbx), %rsi
	movl	$46, %edi
	callq	putc@PLT
	movq	(%rbx), %rdi
	callq	fflush@PLT
.LBB50_36:                              #   in Loop: Header=BB50_1 Depth=1
	testq	%r15, %r15
	movq	320(%rsp), %r8                  # 8-byte Reload
	movq	312(%rsp), %r9                  # 8-byte Reload
	movq	304(%rsp), %r10                 # 8-byte Reload
	je	.LBB50_42
# %bb.37:                               # %.preheader.i46.preheader
                                        #   in Loop: Header=BB50_1 Depth=1
	xorl	%eax, %eax
	movq	%r15, %rdx
	xorl	%esi, %esi
	xorl	%edi, %edi
	jmp	.LBB50_38
	.p2align	4, 0x90
.LBB50_40:                              # %get_bits.exit.thread.i54
                                        #   in Loop: Header=BB50_38 Depth=2
	cmpq	%r8, %rdi
	leaq	1(%rdi), %rcx
	setae	%sil
	addq	$16, %rax
	addq	$-16, %rdx
	movq	%rcx, %rdi
	cmpq	%rcx, %r10
	je	.LBB50_41
.LBB50_38:                              # %.preheader.i46
                                        #   Parent Loop BB50_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	%rax, %r15
	jb	.LBB50_40
# %bb.39:                               # %get_bits.exit.i51
                                        #   in Loop: Header=BB50_38 Depth=2
	movzwl	72(%rsp,%rdi,2), %ebp
	rolw	$8, %bp
	leal	(%r9,%rax), %ecx
	movl	$65535, %ebx                    # imm = 0xFFFF
                                        # kill: def $cl killed $cl killed $ecx
	shll	%cl, %ebx
	cmpq	$16, %rdx
	movl	$65535, %ecx                    # imm = 0xFFFF
	cmovael	%ecx, %ebx
	testw	%bx, %bp
	je	.LBB50_40
.LBB50_41:                              # %is_distinguished.exit56
                                        #   in Loop: Header=BB50_1 Depth=1
	testb	$1, %sil
	jne	.LBB50_42
# %bb.48:                               #   in Loop: Header=BB50_1 Depth=1
	movl	88(%rsp), %eax
	movl	%eax, 112(%rsp)
	movups	72(%rsp), %xmm0
	movaps	%xmm0, 96(%rsp)
	jmp	.LBB50_1
.LBB50_42:                              # %is_distinguished.exit56.thread
	movq	136(%rsp), %rax                 # 8-byte Reload
	addq	$1, %rax
	movl	$16, %r9d
	subl	%r13d, %r9d
	xorl	%esi, %esi
	movl	$65535, %r8d                    # imm = 0xFFFF
	movq	%r13, %rbp
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	jmp	.LBB50_43
	.p2align	4, 0x90
.LBB50_45:                              # %get_bits.exit22.thread.i
                                        #   in Loop: Header=BB50_43 Depth=1
	cmpq	136(%rsp), %rdi                 # 8-byte Folded Reload
	leaq	1(%rdi), %rcx
	setae	%bl
	addq	$16, %rsi
	addq	$-16, %rbp
	movq	%rcx, %rdi
	cmpq	%rcx, %rax
	je	.LBB50_46
.LBB50_43:                              # =>This Inner Loop Header: Depth=1
	cmpq	%rsi, %r13
	jb	.LBB50_45
# %bb.44:                               # %get_bits.exit22.i
                                        #   in Loop: Header=BB50_43 Depth=1
	leal	(%r9,%rsi), %ecx
	movl	$65535, %edx                    # imm = 0xFFFF
                                        # kill: def $cl killed $cl killed $ecx
	shll	%cl, %edx
	cmpq	$16, %rbp
	cmovael	%r8d, %edx
	movzwl	256(%rsp,%rdi,2), %ecx
	xorw	72(%rsp,%rdi,2), %cx
	rolw	$8, %cx
	testw	%dx, %cx
	je	.LBB50_45
.LBB50_46:                              # %is_equal.exit
	movl	$83, %edi
	testb	$1, %bl
	jne	.LBB50_50
# %bb.47:
	movb	$1, stop(%rip)
	movq	stderr@GOTPCREL(%rip), %rbx
	movq	(%rbx), %rcx
	leaq	.L.str.99(%rip), %rdi
	movl	$77, %esi
	movl	$1, %edx
	callq	fwrite@PLT
	movq	(%rbx), %rcx
	leaq	.L.str.100(%rip), %rdi
	movl	$10, %esi
	movl	$1, %edx
	callq	fwrite@PLT
	movq	(%rbx), %rdi
	movl	272(%rsp), %eax
	movl	%eax, 16(%rsp)
	movaps	256(%rsp), %xmm0
	movups	%xmm0, (%rsp)
	callq	write_hash160
	movq	(%rbx), %rcx
	leaq	.L.str.101(%rip), %rdi
	movl	$11, %esi
	movl	$1, %edx
	callq	fwrite@PLT
	movq	(%rbx), %rdi
	movl	88(%rsp), %eax
	movl	%eax, 16(%rsp)
	movups	72(%rsp), %xmm0
	movups	%xmm0, (%rsp)
	callq	write_hash160
	movq	(%rbx), %rsi
	movl	$10, %edi
	callq	putc@PLT
	movl	$1, %edi
	callq	exit@PLT
.LBB50_49:
	movb	$1, stop(%rip)
	movl	$89, %edi
.LBB50_50:                              # %.loopexit.sink.split
	movq	stdout@GOTPCREL(%rip), %rbx
	movq	(%rbx), %rsi
	callq	putc@PLT
	movq	(%rbx), %rdi
	callq	fflush@PLT
.LBB50_51:                              # %.loopexit
	xorl	%eax, %eax
	addq	$600, %rsp                      # imm = 0x258
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end50:
	.size	worker, .Lfunc_end50-worker
	.cfi_endproc
	.section	.rodata,"a",@progbits
	.p2align	2
.LJTI50_0:
	.long	.LBB50_32-.LJTI50_0
	.long	.LBB50_49-.LJTI50_0
	.long	.LBB50_35-.LJTI50_0
	.long	.LBB50_51-.LJTI50_0
                                        # -- End function
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function sha256d
.LCPI51_0:
	.long	1779033703                      # 0x6a09e667
	.long	3144134277                      # 0xbb67ae85
	.long	1013904242                      # 0x3c6ef372
	.long	2773480762                      # 0xa54ff53a
.LCPI51_1:
	.long	1359893119                      # 0x510e527f
	.long	2600822924                      # 0x9b05688c
	.long	528734635                       # 0x1f83d9ab
	.long	1541459225                      # 0x5be0cd19
	.text
	.p2align	4, 0x90
	.type	sha256d,@function
sha256d:                                # @sha256d
	.cfi_startproc
# %bb.0:
	pushq	%r15
	.cfi_def_cfa_offset 16
	pushq	%r14
	.cfi_def_cfa_offset 24
	pushq	%r12
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	subq	$216, %rsp
	.cfi_def_cfa_offset 256
	.cfi_offset %rbx, -40
	.cfi_offset %r12, -32
	.cfi_offset %r14, -24
	.cfi_offset %r15, -16
	movq	%rdi, %r14
	movaps	.LCPI51_0(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	%xmm0, 16(%rsp)
	movaps	.LCPI51_1(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movaps	%xmm0, 32(%rsp)
	movq	$0, 208(%rsp)
	movslq	%edx, %rdx
	leaq	16(%rsp), %r12
	movq	%r12, %rdi
	callq	secp256k1_sha256_write
	movq	208(%rsp), %rax
	movq	%rax, %rcx
	movq	%rax, %rsi
	movq	%rax, %r8
	movl	%eax, %r9d
	movl	%eax, %edi
	movl	%eax, %ebx
	movl	%eax, %r10d
	movl	$55, %edx
	subl	%eax, %edx
	shrq	$29, %rax
	shll	$24, %eax
	shrq	$21, %rcx
	andl	$16711680, %ecx                 # imm = 0xFF0000
	orl	%eax, %ecx
	shrq	$37, %rsi
	andl	$65280, %esi                    # imm = 0xFF00
	orl	%ecx, %esi
	shrq	$53, %r8
	movzbl	%r8b, %eax
	orl	%esi, %eax
	movl	%eax, 8(%rsp)
	shll	$27, %r9d
	shll	$11, %edi
	andl	$16711680, %edi                 # imm = 0xFF0000
	orl	%r9d, %edi
	shrl	$5, %ebx
	andl	$65280, %ebx                    # imm = 0xFF00
	orl	%edi, %ebx
	shrl	$21, %r10d
	movzbl	%r10b, %eax
	orl	%ebx, %eax
	movl	%eax, 12(%rsp)
	movl	$55, %ebx
	andl	$63, %edx
	addq	$1, %rdx
	leaq	secp256k1_sha256_finalize.pad(%rip), %r15
	movq	%r12, %rdi
	movq	%r15, %rsi
	callq	secp256k1_sha256_write
	leaq	8(%rsp), %rsi
	movl	$8, %edx
	movq	%r12, %rdi
	callq	secp256k1_sha256_write
	movdqa	16(%rsp), %xmm0
	movdqa	32(%rsp), %xmm1
	pxor	%xmm3, %xmm3
	movdqa	%xmm0, %xmm2
	punpckhbw	%xmm3, %xmm2            # xmm2 = xmm2[8],xmm3[8],xmm2[9],xmm3[9],xmm2[10],xmm3[10],xmm2[11],xmm3[11],xmm2[12],xmm3[12],xmm2[13],xmm3[13],xmm2[14],xmm3[14],xmm2[15],xmm3[15]
	pshuflw	$27, %xmm2, %xmm2               # xmm2 = xmm2[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm2, %xmm2               # xmm2 = xmm2[0,1,2,3,7,6,5,4]
	punpcklbw	%xmm3, %xmm0            # xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1],xmm0[2],xmm3[2],xmm0[3],xmm3[3],xmm0[4],xmm3[4],xmm0[5],xmm3[5],xmm0[6],xmm3[6],xmm0[7],xmm3[7]
	pshuflw	$27, %xmm0, %xmm0               # xmm0 = xmm0[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm0, %xmm0               # xmm0 = xmm0[0,1,2,3,7,6,5,4]
	packuswb	%xmm2, %xmm0
	movdqa	%xmm1, %xmm2
	punpckhbw	%xmm3, %xmm2            # xmm2 = xmm2[8],xmm3[8],xmm2[9],xmm3[9],xmm2[10],xmm3[10],xmm2[11],xmm3[11],xmm2[12],xmm3[12],xmm2[13],xmm3[13],xmm2[14],xmm3[14],xmm2[15],xmm3[15]
	pshuflw	$27, %xmm2, %xmm2               # xmm2 = xmm2[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm2, %xmm2               # xmm2 = xmm2[0,1,2,3,7,6,5,4]
	punpcklbw	%xmm3, %xmm1            # xmm1 = xmm1[0],xmm3[0],xmm1[1],xmm3[1],xmm1[2],xmm3[2],xmm1[3],xmm3[3],xmm1[4],xmm3[4],xmm1[5],xmm3[5],xmm1[6],xmm3[6],xmm1[7],xmm3[7]
	pshuflw	$27, %xmm1, %xmm1               # xmm1 = xmm1[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm1, %xmm1               # xmm1 = xmm1[0,1,2,3,7,6,5,4]
	packuswb	%xmm2, %xmm1
	movdqu	%xmm0, (%r14)
	movdqu	%xmm1, 16(%r14)
	movaps	.LCPI51_0(%rip), %xmm0          # xmm0 = [1779033703,3144134277,1013904242,2773480762]
	movaps	%xmm0, 16(%rsp)
	movaps	.LCPI51_1(%rip), %xmm0          # xmm0 = [1359893119,2600822924,528734635,1541459225]
	movaps	%xmm0, 32(%rsp)
	movq	$0, 208(%rsp)
	leaq	16(%rsp), %r12
	movl	$32, %edx
	movq	%r12, %rdi
	movq	%r14, %rsi
	callq	secp256k1_sha256_write
	movq	208(%rsp), %rax
	movq	%rax, %rcx
	shrq	$29, %rcx
	shll	$24, %ecx
	movq	%rax, %rdx
	shrq	$21, %rdx
	andl	$16711680, %edx                 # imm = 0xFF0000
	orl	%ecx, %edx
	movq	%rax, %rcx
	shrq	$37, %rcx
	andl	$65280, %ecx                    # imm = 0xFF00
	orl	%edx, %ecx
	movq	%rax, %rdx
	shrq	$53, %rdx
	movzbl	%dl, %edx
	orl	%ecx, %edx
	movl	%edx, 8(%rsp)
	movl	%eax, %ecx
	shll	$27, %ecx
	movl	%eax, %edx
	shll	$11, %edx
	andl	$16711680, %edx                 # imm = 0xFF0000
	orl	%ecx, %edx
	movl	%eax, %ecx
	shrl	$5, %ecx
	andl	$65280, %ecx                    # imm = 0xFF00
	orl	%edx, %ecx
	subl	%eax, %ebx
	shrl	$21, %eax
	movzbl	%al, %eax
	orl	%ecx, %eax
	movl	%eax, 12(%rsp)
	andl	$63, %ebx
	addq	$1, %rbx
	movq	%r12, %rdi
	movq	%r15, %rsi
	movq	%rbx, %rdx
	callq	secp256k1_sha256_write
	leaq	8(%rsp), %rsi
	movl	$8, %edx
	movq	%r12, %rdi
	callq	secp256k1_sha256_write
	movdqa	16(%rsp), %xmm0
	movdqa	32(%rsp), %xmm1
	movdqa	%xmm0, %xmm2
	pxor	%xmm3, %xmm3
	punpckhbw	%xmm3, %xmm2            # xmm2 = xmm2[8],xmm3[8],xmm2[9],xmm3[9],xmm2[10],xmm3[10],xmm2[11],xmm3[11],xmm2[12],xmm3[12],xmm2[13],xmm3[13],xmm2[14],xmm3[14],xmm2[15],xmm3[15]
	pshuflw	$27, %xmm2, %xmm2               # xmm2 = xmm2[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm2, %xmm2               # xmm2 = xmm2[0,1,2,3,7,6,5,4]
	punpcklbw	%xmm3, %xmm0            # xmm0 = xmm0[0],xmm3[0],xmm0[1],xmm3[1],xmm0[2],xmm3[2],xmm0[3],xmm3[3],xmm0[4],xmm3[4],xmm0[5],xmm3[5],xmm0[6],xmm3[6],xmm0[7],xmm3[7]
	pshuflw	$27, %xmm0, %xmm0               # xmm0 = xmm0[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm0, %xmm0               # xmm0 = xmm0[0,1,2,3,7,6,5,4]
	packuswb	%xmm2, %xmm0
	movdqa	%xmm1, %xmm2
	punpckhbw	%xmm3, %xmm2            # xmm2 = xmm2[8],xmm3[8],xmm2[9],xmm3[9],xmm2[10],xmm3[10],xmm2[11],xmm3[11],xmm2[12],xmm3[12],xmm2[13],xmm3[13],xmm2[14],xmm3[14],xmm2[15],xmm3[15]
	pxor	%xmm3, %xmm3
	pshuflw	$27, %xmm2, %xmm2               # xmm2 = xmm2[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm2, %xmm2               # xmm2 = xmm2[0,1,2,3,7,6,5,4]
	punpcklbw	%xmm3, %xmm1            # xmm1 = xmm1[0],xmm3[0],xmm1[1],xmm3[1],xmm1[2],xmm3[2],xmm1[3],xmm3[3],xmm1[4],xmm3[4],xmm1[5],xmm3[5],xmm1[6],xmm3[6],xmm1[7],xmm3[7]
	pshuflw	$27, %xmm1, %xmm1               # xmm1 = xmm1[3,2,1,0,4,5,6,7]
	pshufhw	$27, %xmm1, %xmm1               # xmm1 = xmm1[0,1,2,3,7,6,5,4]
	packuswb	%xmm2, %xmm1
	movdqu	%xmm0, (%r14)
	movdqu	%xmm1, 16(%r14)
	addq	$216, %rsp
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r12
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end51:
	.size	sha256d, .Lfunc_end51-sha256d
	.cfi_endproc
                                        # -- End function
	.p2align	4, 0x90                         # -- Begin function base58_encode
	.type	base58_encode,@function
base58_encode:                          # @base58_encode
	.cfi_startproc
# %bb.0:
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset %rbp, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	movq	%rdx, %r14
	movq	%rsi, %r12
	movq	%rdi, -56(%rbp)                 # 8-byte Spill
	xorl	%r13d, %r13d
	testq	%rdx, %rdx
	je	.LBB52_4
	.p2align	4, 0x90
.LBB52_1:                               # %.lr.ph
                                        # =>This Inner Loop Header: Depth=1
	cmpb	$0, (%r12,%r13)
	jne	.LBB52_4
# %bb.2:                                #   in Loop: Header=BB52_1 Depth=1
	addq	$1, %r13
	cmpq	%r13, %r14
	jne	.LBB52_1
# %bb.3:
	movq	%r14, %r13
.LBB52_4:                               # %.critedge
	movq	%r14, %rax
	subq	%r13, %rax
	imulq	$138, %rax, %rax
	shrq	$2, %rax
	movabsq	$2951479051793528259, %rcx      # imm = 0x28F5C28F5C28F5C3
	mulq	%rcx
	shrq	$2, %rdx
	leaq	1(%rdx), %r15
	movq	%rdx, -48(%rbp)                 # 8-byte Spill
	leaq	16(%rdx), %rax
	andq	$-16, %rax
	movq	%rsp, %rbx
	subq	%rax, %rbx
	movq	%rbx, %rsp
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movq	%r15, %rdx
	callq	memset@PLT
	cmpq	%r13, %r14
	jbe	.LBB52_11
# %bb.5:
	movq	%r13, %r8
	.p2align	4, 0x90
.LBB52_7:                               # %.lr.ph8
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB52_8 Depth 2
	movzbl	(%r12,%r8), %edx
	movq	%r15, %rcx
	.p2align	4, 0x90
.LBB52_8:                               #   Parent Loop BB52_7 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movsbl	-1(%rbx,%rcx), %esi
	shll	$8, %esi
	addl	%edx, %esi
	movslq	%esi, %rdi
	imulq	$-1925330167, %rdi, %rdx        # imm = 0x8D3DCB09
	shrq	$32, %rdx
	addl	%esi, %edx
	movl	%edx, %eax
	shrl	$31, %eax
	sarl	$5, %edx
	addl	%eax, %edx
	imull	$58, %edx, %eax
	subl	%eax, %edi
	movb	%dil, -1(%rbx,%rcx)
	addq	$-1, %rcx
	jg	.LBB52_8
# %bb.9:                                #   in Loop: Header=BB52_7 Depth=1
	addl	$57, %esi
	cmpl	$115, %esi
	jae	.LBB52_10
# %bb.6:                                #   in Loop: Header=BB52_7 Depth=1
	addq	$1, %r8
	cmpq	%r14, %r8
	jne	.LBB52_7
.LBB52_11:                              # %.preheader.preheader
	xorl	%r14d, %r14d
	movq	-56(%rbp), %r12                 # 8-byte Reload
	.p2align	4, 0x90
.LBB52_12:                              # %.preheader
                                        # =>This Inner Loop Header: Depth=1
	cmpb	$0, (%rbx,%r14)
	jne	.LBB52_15
# %bb.13:                               #   in Loop: Header=BB52_12 Depth=1
	addq	$1, %r14
	cmpq	%r14, %r15
	jne	.LBB52_12
# %bb.14:
	movq	%r15, %r14
.LBB52_15:                              # %.critedge1
	addq	%r13, %r15
	subq	%r14, %r15
	addq	$-100, %r15
	cmpq	$-101, %r15
	jb	.LBB52_25
# %bb.16:
	movq	%r12, %rdi
	movl	$49, %esi
	movq	%r13, %rdx
	callq	memset@PLT
	movq	-48(%rbp), %r8                  # 8-byte Reload
	movq	%r8, %rdi
	subq	%r14, %rdi
	jb	.LBB52_24
# %bb.17:                               # %.lr.ph13.preheader
	movl	%r8d, %ecx
	subl	%r14d, %ecx
	addl	$1, %ecx
	andq	$3, %rcx
	je	.LBB52_20
# %bb.18:                               # %.lr.ph13.prol.preheader
	negq	%rcx
	leaq	.L.str.106(%rip), %rdx
	.p2align	4, 0x90
.LBB52_19:                              # %.lr.ph13.prol
                                        # =>This Inner Loop Header: Depth=1
	movsbq	(%rbx,%r14), %rsi
	movzbl	(%rsi,%rdx), %eax
	movb	%al, (%r12,%r13)
	addq	$1, %r14
	addq	$1, %r13
	incq	%rcx
	jne	.LBB52_19
.LBB52_20:                              # %.lr.ph13.prol.loopexit
	cmpq	$3, %rdi
	jb	.LBB52_24
# %bb.21:                               # %.lr.ph13.preheader27
	leaq	(%r12,%r13), %rax
	addq	$3, %rax
	negq	%r13
	subq	%r14, %r8
	leaq	(%r14,%rbx), %rcx
	addq	$3, %rcx
	movq	$-1, %rdx
	leaq	.L.str.106(%rip), %rsi
	.p2align	4, 0x90
.LBB52_22:                              # %.lr.ph13
                                        # =>This Inner Loop Header: Depth=1
	movsbq	-2(%rcx,%rdx), %rdi
	movzbl	(%rdi,%rsi), %ebx
	movb	%bl, -2(%rax,%rdx)
	movsbq	-1(%rcx,%rdx), %rdi
	movzbl	(%rdi,%rsi), %ebx
	movb	%bl, -1(%rax,%rdx)
	movsbq	(%rcx,%rdx), %rdi
	movzbl	(%rdi,%rsi), %ebx
	movb	%bl, (%rax,%rdx)
	movsbq	1(%rcx,%rdx), %rdi
	movzbl	(%rdi,%rsi), %ebx
	movb	%bl, 1(%rax,%rdx)
	addq	$-4, %r13
	addq	$4, %rdx
	cmpq	%rdx, %r8
	jne	.LBB52_22
# %bb.23:                               # %._crit_edge.loopexit
	negq	%r13
.LBB52_24:                              # %._crit_edge
	movb	$0, (%r12,%r13)
.LBB52_25:
	cmpq	$-101, %r15
	setae	%al
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa %rsp, 8
	retq
.LBB52_10:
	.cfi_def_cfa %rbp, 16
	leaq	.L.str.105(%rip), %rdi
	leaq	.L.str.23(%rip), %rsi
	leaq	.L__PRETTY_FUNCTION__.base58_encode(%rip), %rcx
	movl	$305, %edx                      # imm = 0x131
	callq	__assert_fail@PLT
.Lfunc_end52:
	.size	base58_encode, .Lfunc_end52-base58_encode
	.cfi_endproc
                                        # -- End function
	.type	.L.str,@object                  # @.str
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str:
	.asciz	"%s:%d: %s\n"
	.size	.L.str, 11

	.type	.L.str.1,@object                # @.str.1
.L.str.1:
	.asciz	"secp256k1/src/secp256k1.c"
	.size	.L.str.1, 26

	.type	.L.str.2,@object                # @.str.2
.L.str.2:
	.asciz	"test condition failed: ctx != NULL"
	.size	.L.str.2, 35

	.type	.L.str.3,@object                # @.str.3
.L.str.3:
	.asciz	"test condition failed: secp256k1_ecmult_context_is_built(&ctx->ecmult_ctx)"
	.size	.L.str.3, 75

	.type	.L.str.4,@object                # @.str.4
.L.str.4:
	.asciz	"test condition failed: msg32 != NULL"
	.size	.L.str.4, 37

	.type	.L.str.5,@object                # @.str.5
.L.str.5:
	.asciz	"test condition failed: sig != NULL"
	.size	.L.str.5, 35

	.type	.L.str.6,@object                # @.str.6
.L.str.6:
	.asciz	"test condition failed: pubkey != NULL"
	.size	.L.str.6, 38

	.type	secp256k1_nonce_function_rfc6979,@object # @secp256k1_nonce_function_rfc6979
	.section	.data.rel.ro,"aw",@progbits
	.globl	secp256k1_nonce_function_rfc6979
	.p2align	3
secp256k1_nonce_function_rfc6979:
	.quad	nonce_function_rfc6979
	.size	secp256k1_nonce_function_rfc6979, 8

	.type	secp256k1_nonce_function_default,@object # @secp256k1_nonce_function_default
	.globl	secp256k1_nonce_function_default
	.p2align	3
secp256k1_nonce_function_default:
	.quad	nonce_function_rfc6979
	.size	secp256k1_nonce_function_default, 8

	.type	.L.str.7,@object                # @.str.7
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.7:
	.asciz	"test condition failed: secp256k1_ecmult_gen_context_is_built(&ctx->ecmult_gen_ctx)"
	.size	.L.str.7, 83

	.type	.L.str.8,@object                # @.str.8
.L.str.8:
	.asciz	"test condition failed: signature != NULL"
	.size	.L.str.8, 41

	.type	.L.str.9,@object                # @.str.9
.L.str.9:
	.asciz	"test condition failed: signaturelen != NULL"
	.size	.L.str.9, 44

	.type	.L.str.10,@object               # @.str.10
.L.str.10:
	.asciz	"test condition failed: seckey != NULL"
	.size	.L.str.10, 38

	.type	.L.str.11,@object               # @.str.11
.L.str.11:
	.asciz	"test condition failed: sig64 != NULL"
	.size	.L.str.11, 37

	.type	.L.str.12,@object               # @.str.12
.L.str.12:
	.asciz	"test condition failed: pubkeylen != NULL"
	.size	.L.str.12, 41

	.type	.L.str.13,@object               # @.str.13
.L.str.13:
	.asciz	"test condition failed: recid >= 0 && recid <= 3"
	.size	.L.str.13, 48

	.type	.L.str.14,@object               # @.str.14
.L.str.14:
	.asciz	"test condition failed: tweak != NULL"
	.size	.L.str.14, 37

	.type	.L.str.15,@object               # @.str.15
.L.str.15:
	.asciz	"test condition failed: privkey != NULL"
	.size	.L.str.15, 39

	.type	.L.str.16,@object               # @.str.16
.L.str.16:
	.asciz	"test condition failed: privkeylen != NULL"
	.size	.L.str.16, 42

	.type	main.long_options,@object       # @main.long_options
	.data
	.p2align	4
main.long_options:
	.quad	.L.str.17
	.long	1                               # 0x1
	.zero	4
	.quad	0
	.long	1                               # 0x1
	.zero	4
	.quad	.L.str.18
	.long	0                               # 0x0
	.zero	4
	.quad	0
	.long	2                               # 0x2
	.zero	4
	.quad	.L.str.19
	.long	1                               # 0x1
	.zero	4
	.quad	0
	.long	3                               # 0x3
	.zero	4
	.zero	32
	.size	main.long_options, 128

	.type	.L.str.17,@object               # @.str.17
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.17:
	.asciz	"job"
	.size	.L.str.17, 4

	.type	.L.str.18,@object               # @.str.18
.L.str.18:
	.asciz	"help"
	.size	.L.str.18, 5

	.type	.L.str.19,@object               # @.str.19
.L.str.19:
	.asciz	"message"
	.size	.L.str.19, 8

	.type	.L.str.20,@object               # @.str.20
.L.str.20:
	.asciz	"This is a real Bitcoin address."
	.size	.L.str.20, 32

	.type	.L.str.21,@object               # @.str.21
	.section	.rodata,"a",@progbits
.L.str.21:
	.zero	1
	.size	.L.str.21, 1

	.type	.L.str.22,@object               # @.str.22
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.22:
	.asciz	"job != NULL"
	.size	.L.str.22, 12

	.type	.L.str.23,@object               # @.str.23
.L.str.23:
	.asciz	"qianshiBTC.c"
	.size	.L.str.23, 13

	.type	.L__PRETTY_FUNCTION__.main,@object # @__PRETTY_FUNCTION__.main
.L__PRETTY_FUNCTION__.main:
	.asciz	"int main(int, char **)"
	.size	.L__PRETTY_FUNCTION__.main, 23

	.type	.L.str.24,@object               # @.str.24
.L.str.24:
	.asciz	"error: message length too long (max = 100).\n"
	.size	.L.str.24, 45

	.type	.L.str.25,@object               # @.str.25
.L.str.25:
	.asciz	"message != NULL"
	.size	.L.str.25, 16

	.type	.L.str.26,@object               # @.str.26
.L.str.26:
	.asciz	"usage: %s [--job=JOB] [--message=MESSAGE] bits [distinguished-bits]\n\n"
	.size	.L.str.26, 70

	.type	.L.str.27,@object               # @.str.27
.L.str.27:
	.asciz	"WHERE:\n"
	.size	.L.str.27, 8

	.type	.L.str.28,@object               # @.str.28
.L.str.28:
	.asciz	"\t--job=JOB\n"
	.size	.L.str.28, 12

	.type	.L.str.29,@object               # @.str.29
.L.str.29:
	.asciz	"\t\tSave/restore state under JOB\n"
	.size	.L.str.29, 32

	.type	.L.str.30,@object               # @.str.30
.L.str.30:
	.asciz	"\t--message=MESSAGE\n"
	.size	.L.str.30, 20

	.type	.L.str.31,@object               # @.str.31
.L.str.31:
	.asciz	"\t\tGenerate signatures using MESSAGE.\n"
	.size	.L.str.31, 38

	.type	.L.str.32,@object               # @.str.32
.L.str.32:
	.asciz	"error: number-of-bits must be within the range 32..160, got %lu\n"
	.size	.L.str.32, 65

	.type	.L.str.33,@object               # @.str.33
.L.str.33:
	.asciz	"error: number-of-distinguished-bits must be within the range 0..%lu, got %lu\n"
	.size	.L.str.33, 78

	.type	.L.str.34,@object               # @.str.34
.L.str.34:
	.asciz	"n = %lubits\n"
	.size	.L.str.34, 13

	.type	.L.str.35,@object               # @.str.35
.L.str.35:
	.asciz	"difficulty = %.30g\n"
	.size	.L.str.35, 20

	.type	.L.str.36,@object               # @.str.36
.L.str.36:
	.asciz	"distinguished = %lubits\n"
	.size	.L.str.36, 25

	.type	bases,@object                   # @bases
	.local	bases
	.comm	bases,8388480,16
	.type	offsets,@object                 # @offsets
	.local	offsets
	.comm	offsets,51903720,16
	.type	priv_bases,@object              # @priv_bases
	.local	priv_bases
	.comm	priv_bases,2097120,16
	.type	priv_offsets,@object            # @priv_offsets
	.local	priv_offsets
	.comm	priv_offsets,18874080,16
	.type	cxt,@object                     # @cxt
	.local	cxt
	.comm	cxt,8,8
	.type	table_lock,@object              # @table_lock
	.local	table_lock
	.comm	table_lock,40,8
	.type	.L.str.37,@object               # @.str.37
.L.str.37:
	.asciz	"threads = %lu\n"
	.size	.L.str.37, 15

	.type	.L.str.38,@object               # @.str.38
.L.str.38:
	.asciz	"init |"
	.size	.L.str.38, 7

	.type	.L.str.39,@object               # @.str.39
.L.str.39:
	.asciz	".public"
	.size	.L.str.39, 8

	.type	.L.str.40,@object               # @.str.40
.L.str.40:
	.asciz	".secret"
	.size	.L.str.40, 8

	.type	.L.str.42,@object               # @.str.42
.L.str.42:
	.asciz	"find_Y |"
	.size	.L.str.42, 9

	.type	.L.str.43,@object               # @.str.43
.L.str.43:
	.asciz	".work"
	.size	.L.str.43, 6

	.type	stop,@object                    # @stop
	.local	stop
	.comm	stop,1,1
	.type	.L.str.44,@object               # @.str.44
.L.str.44:
	.asciz	"r"
	.size	.L.str.44, 2

	.type	.L.str.45,@object               # @.str.45
.L.str.45:
	.asciz	"a"
	.size	.L.str.45, 2

	.type	.L.str.46,@object               # @.str.46
.L.str.46:
	.asciz	"error: failed to open file \"%s\"\n"
	.size	.L.str.46, 33

	.type	.L.str.47,@object               # @.str.47
.L.str.47:
	.asciz	"refine_Y[%lu] |"
	.size	.L.str.47, 16

	.type	.L.str.48,@object               # @.str.48
.L.str.48:
	.asciz	"Y[0] = "
	.size	.L.str.48, 8

	.type	.L.str.49,@object               # @.str.49
.L.str.49:
	.asciz	"\nY[1] = "
	.size	.L.str.49, 9

	.type	.L.str.50,@object               # @.str.50
.L.str.50:
	.asciz	"\ntime = %lums\n"
	.size	.L.str.50, 15

	.type	.L.str.52,@object               # @.str.52
.L.str.52:
	.asciz	"warning: failed to read \"%s\"\n"
	.size	.L.str.52, 30

	.type	.L.str.53,@object               # @.str.53
.L.str.53:
	.asciz	"priv_key[1] = "
	.size	.L.str.53, 15

	.type	.L.str.54,@object               # @.str.54
.L.str.54:
	.asciz	"%.2X"
	.size	.L.str.54, 5

	.type	.L.str.55,@object               # @.str.55
.L.str.55:
	.asciz	"UNKNOWN"
	.size	.L.str.55, 8

	.type	.L.str.56,@object               # @.str.56
.L.str.56:
	.asciz	"\npriv_key[2] = "
	.size	.L.str.56, 16

	.type	.L.str.57,@object               # @.str.57
.L.str.57:
	.asciz	"\nWIF[1] = %s\n"
	.size	.L.str.57, 14

	.type	.L.str.58,@object               # @.str.58
.L.str.58:
	.asciz	"WIF[2] = %s\n\n"
	.size	.L.str.58, 14

	.type	.L.str.60,@object               # @.str.60
.L.str.60:
	.asciz	"message = \"%s\"\n"
	.size	.L.str.60, 16

	.type	.L.str.61,@object               # @.str.61
.L.str.61:
	.asciz	"sig[1] = %s\n"
	.size	.L.str.61, 13

	.type	.L.str.62,@object               # @.str.62
.L.str.62:
	.asciz	"sig[2] = %s\n\n"
	.size	.L.str.62, 14

	.type	.L.str.64,@object               # @.str.64
.L.str.64:
	.asciz	"pub_key[1] = "
	.size	.L.str.64, 14

	.type	.L.str.65,@object               # @.str.65
.L.str.65:
	.asciz	"\npub_key[2] = "
	.size	.L.str.65, 15

	.type	.L.str.66,@object               # @.str.66
.L.str.66:
	.asciz	"\n\nbonus = %lubits\n"
	.size	.L.str.66, 19

	.type	.L.str.67,@object               # @.str.67
.L.str.67:
	.asciz	"shared = %luchars\n"
	.size	.L.str.67, 19

	.type	.L.str.68,@object               # @.str.68
.L.str.68:
	.asciz	"hash160[1] = "
	.size	.L.str.68, 14

	.type	.L.str.69,@object               # @.str.69
.L.str.69:
	.asciz	"%.1x"
	.size	.L.str.69, 5

	.type	.L.str.70,@object               # @.str.70
.L.str.70:
	.asciz	"hash160[2] = "
	.size	.L.str.70, 14

	.type	.L.str.71,@object               # @.str.71
.L.str.71:
	.asciz	"addr[1] = "
	.size	.L.str.71, 11

	.type	.L.str.73,@object               # @.str.73
.L.str.73:
	.asciz	"addr[2] = "
	.size	.L.str.73, 11

	.type	.L.str.74,@object               # @.str.74
.L.str.74:
	.asciz	"%s\n\n"
	.size	.L.str.74, 5

	.type	.L.str.76,@object               # @.str.76
.L.str.76:
	.asciz	"secp256k1/src/util.h"
	.size	.L.str.76, 21

	.type	.L.str.77,@object               # @.str.77
.L.str.77:
	.asciz	"test condition failed: ret != NULL"
	.size	.L.str.77, 35

	.type	secp256k1_ge_const_g,@object    # @secp256k1_ge_const_g
	.section	.rodata,"a",@progbits
	.p2align	3
secp256k1_ge_const_g:
	.quad	705178180786072                 # 0x2815b16f81798
	.quad	3855836460717471                # 0xdb2dce28d959f
	.quad	4089131105950716                # 0xe870b07029bfc
	.quad	3301581525494108                # 0xbbac55a06295c
	.quad	133858670344668                 # 0x79be667ef9dc
	.quad	2199641648059576                # 0x7d08ffb10d4b8
	.quad	1278080618437060                # 0x48a68554199c4
	.quad	3959378566518708                # 0xe1108a8fd17b4
	.quad	3455034269351872                # 0xc4655da4fbfc0
	.quad	79417610544803                  # 0x483ada7726a3
	.long	0                               # 0x0
	.zero	4
	.size	secp256k1_ge_const_g, 88

	.type	secp256k1_fe_inv_var.prime,@object # @secp256k1_fe_inv_var.prime
	.p2align	4
secp256k1_fe_inv_var.prime:
	.ascii	"\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\376\377\377\374/"
	.size	secp256k1_fe_inv_var.prime, 32

	.type	secp256k1_rfc6979_hmac_sha256_initialize.one,@object # @secp256k1_rfc6979_hmac_sha256_initialize.one
secp256k1_rfc6979_hmac_sha256_initialize.one:
	.byte	1
	.size	secp256k1_rfc6979_hmac_sha256_initialize.one, 1

	.type	secp256k1_sha256_finalize.pad,@object # @secp256k1_sha256_finalize.pad
	.p2align	4
secp256k1_sha256_finalize.pad:
	.byte	128                             # 0x80
	.zero	63
	.size	secp256k1_sha256_finalize.pad, 64

	.type	secp256k1_rfc6979_hmac_sha256_generate.zero,@object # @secp256k1_rfc6979_hmac_sha256_generate.zero
secp256k1_rfc6979_hmac_sha256_generate.zero:
	.zero	1
	.size	secp256k1_rfc6979_hmac_sha256_generate.zero, 1

	.type	secp256k1_eckey_privkey_serialize.middle,@object # @secp256k1_eckey_privkey_serialize.middle
	.p2align	4
secp256k1_eckey_privkey_serialize.middle:
	.asciz	"\240\201\2050\201\202\002\001\0010,\006\007*\206H\316=\001\001\002!\000\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\376\377\377\374/0\006\004\001\000\004\001\007\004!\002y\276f~\371\334\273\254U\240b\225\316\207\013\007\002\233\374\333-\316(\331Y\362\201[\026\370\027\230\002!\000\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\376\272\256\334\346\257H\240;\277\322^\214\3206AA\002\001\001\241$\003\""
	.size	secp256k1_eckey_privkey_serialize.middle, 141

	.type	secp256k1_eckey_privkey_serialize.begin.78,@object # @secp256k1_eckey_privkey_serialize.begin.78
secp256k1_eckey_privkey_serialize.begin.78:
	.ascii	"0\202\001\023\002\001\001\004 "
	.size	secp256k1_eckey_privkey_serialize.begin.78, 9

	.type	secp256k1_eckey_privkey_serialize.middle.79,@object # @secp256k1_eckey_privkey_serialize.middle.79
	.p2align	4
secp256k1_eckey_privkey_serialize.middle.79:
	.asciz	"\240\201\2450\201\242\002\001\0010,\006\007*\206H\316=\001\001\002!\000\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\376\377\377\374/0\006\004\001\000\004\001\007\004A\004y\276f~\371\334\273\254U\240b\225\316\207\013\007\002\233\374\333-\316(\331Y\362\201[\026\370\027\230H:\332w&\243\304e]\244\373\374\016\021\b\250\375\027\264H\246\205T\031\234G\320\217\373\020\324\270\002!\000\377\377\377\377\377\377\377\377\377\377\377\377\377\377\377\376\272\256\334\346\257H\240;\277\322^\214\3206AA\002\001\001\241D\003B"
	.size	secp256k1_eckey_privkey_serialize.middle.79, 173

	.type	.L.str.80,@object               # @.str.80
	.section	.rodata.str1.1,"aMS",@progbits,1
.L.str.80:
	.asciz	"res == 0"
	.size	.L.str.80, 9

	.type	.L__PRETTY_FUNCTION__.mutex_init,@object # @__PRETTY_FUNCTION__.mutex_init
.L__PRETTY_FUNCTION__.mutex_init:
	.asciz	"void mutex_init(mutex *)"
	.size	.L__PRETTY_FUNCTION__.mutex_init, 25

	.type	.L.str.81,@object               # @.str.81
.L.str.81:
	.asciz	"name != NULL"
	.size	.L.str.81, 13

	.type	.L__PRETTY_FUNCTION__.gen_filename,@object # @__PRETTY_FUNCTION__.gen_filename
.L__PRETTY_FUNCTION__.gen_filename:
	.asciz	"char *gen_filename(const char *, const char *)"
	.size	.L__PRETTY_FUNCTION__.gen_filename, 47

	.type	.L.str.82,@object               # @.str.82
.L.str.82:
	.asciz	"error: failed to read \"%s\"\n"
	.size	.L.str.82, 28

	.type	.L.str.83,@object               # @.str.83
.L.str.83:
	.asciz	"RRRRRRRRRR"
	.size	.L.str.83, 11

	.type	.L.str.84,@object               # @.str.84
.L.str.84:
	.asciz	"error: failed to spawn init worker thread\n"
	.size	.L.str.84, 43

	.type	.L.str.85,@object               # @.str.85
.L.str.85:
	.asciz	"w"
	.size	.L.str.85, 2

	.type	.L.str.86,@object               # @.str.86
.L.str.86:
	.asciz	"error: failed to write \"%s\"\n"
	.size	.L.str.86, 29

	.type	.L.str.87,@object               # @.str.87
.L.str.87:
	.asciz	"table != NULL"
	.size	.L.str.87, 14

	.type	.L__PRETTY_FUNCTION__.read_pub_table,@object # @__PRETTY_FUNCTION__.read_pub_table
.L__PRETTY_FUNCTION__.read_pub_table:
	.asciz	"_Bool read_pub_table(FILE *)"
	.size	.L__PRETTY_FUNCTION__.read_pub_table, 29

	.type	.L__PRETTY_FUNCTION__.write_pub_table,@object # @__PRETTY_FUNCTION__.write_pub_table
.L__PRETTY_FUNCTION__.write_pub_table:
	.asciz	"_Bool write_pub_table(FILE *)"
	.size	.L__PRETTY_FUNCTION__.write_pub_table, 30

	.type	.L.str.88,@object               # @.str.88
.L.str.88:
	.asciz	"ptr - table == PUB_TABLE_SIZE"
	.size	.L.str.88, 30

	.type	.L__PRETTY_FUNCTION__.write_priv_table,@object # @__PRETTY_FUNCTION__.write_priv_table
.L__PRETTY_FUNCTION__.write_priv_table:
	.asciz	"_Bool write_priv_table(FILE *)"
	.size	.L__PRETTY_FUNCTION__.write_priv_table, 31

	.type	.L.str.89,@object               # @.str.89
.L.str.89:
	.asciz	"ptr - table == PRIV_TABLE_SIZE"
	.size	.L.str.89, 31

	.type	.L.str.90,@object               # @.str.90
.L.str.90:
	.asciz	"error: failed to allocate %lu bytes for entry"
	.size	.L.str.90, 46

	.type	collision,@object               # @collision
	.local	collision
	.comm	collision,1,1
	.type	table,@object                   # @table
	.local	table
	.comm	table,65536,16
	.type	c1,@object                      # @c1
	.local	c1
	.comm	c1,20,8
	.type	c2,@object                      # @c2
	.local	c2
	.comm	c2,20,8
	.type	c3,@object                      # @c3
	.local	c3
	.comm	c3,20,4
	.type	.L__PRETTY_FUNCTION__.mutex_lock,@object # @__PRETTY_FUNCTION__.mutex_lock
.L__PRETTY_FUNCTION__.mutex_lock:
	.asciz	"void mutex_lock(mutex *)"
	.size	.L__PRETTY_FUNCTION__.mutex_lock, 25

	.type	.L__PRETTY_FUNCTION__.mutex_unlock,@object # @__PRETTY_FUNCTION__.mutex_unlock
.L__PRETTY_FUNCTION__.mutex_unlock:
	.asciz	"void mutex_unlock(mutex *)"
	.size	.L__PRETTY_FUNCTION__.mutex_unlock, 27

	.type	.L.str.91,@object               # @.str.91
.L.str.91:
	.asciz	"# "
	.size	.L.str.91, 3

	.type	.L.str.92,@object               # @.str.92
.L.str.92:
	.asciz	"W%.3u%.3u "
	.size	.L.str.92, 11

	.type	.L.str.93,@object               # @.str.93
.L.str.93:
	.asciz	"seed != NULL"
	.size	.L.str.93, 13

	.type	.L__PRETTY_FUNCTION__.make_seed,@object # @__PRETTY_FUNCTION__.make_seed
.L__PRETTY_FUNCTION__.make_seed:
	.asciz	"struct seed *make_seed(void)"
	.size	.L__PRETTY_FUNCTION__.make_seed, 29

	.type	.L.str.94,@object               # @.str.94
.L.str.94:
	.asciz	"error: failed to init random seed\n"
	.size	.L.str.94, 35

	.type	.L.str.95,@object               # @.str.95
.L.str.95:
	.asciz	"error: random seed initialization failed\n"
	.size	.L.str.95, 42

	.type	.L.str.96,@object               # @.str.96
.L.str.96:
	.asciz	"/dev/urandom"
	.size	.L.str.96, 13

	.type	.L.str.97,@object               # @.str.97
.L.str.97:
	.asciz	"info != NULL"
	.size	.L.str.97, 13

	.type	.L__PRETTY_FUNCTION__.spawn_worker,@object # @__PRETTY_FUNCTION__.spawn_worker
.L__PRETTY_FUNCTION__.spawn_worker:
	.asciz	"thread spawn_worker(FILE *, size_t, size_t, uint160_t, size_t, uint160_t)"
	.size	.L__PRETTY_FUNCTION__.spawn_worker, 74

	.type	.L.str.98,@object               # @.str.98
.L.str.98:
	.asciz	"error: failed to spawn thread"
	.size	.L.str.98, 30

	.type	.L.str.99,@object               # @.str.99
.L.str.99:
	.asciz	"error: something went wrong; collision is not valid (bug or hardware error?)\n"
	.size	.L.str.99, 78

	.type	.L.str.100,@object              # @.str.100
.L.str.100:
	.asciz	"expected: "
	.size	.L.str.100, 11

	.type	.L.str.101,@object              # @.str.101
.L.str.101:
	.asciz	"\ngot     : "
	.size	.L.str.101, 12

	.type	.L.str.102,@object              # @.str.102
.L.str.102:
	.asciz	"collision"
	.size	.L.str.102, 10

	.type	.L__PRETTY_FUNCTION__.get_collision,@object # @__PRETTY_FUNCTION__.get_collision
.L__PRETTY_FUNCTION__.get_collision:
	.asciz	"void get_collision(uint160_t *, uint160_t *, uint160_t *)"
	.size	.L__PRETTY_FUNCTION__.get_collision, 58

	.type	.L.str.103,@object              # @.str.103
.L.str.103:
	.asciz	"%.2x"
	.size	.L.str.103, 5

	.type	.L__PRETTY_FUNCTION__.read_priv_table,@object # @__PRETTY_FUNCTION__.read_priv_table
.L__PRETTY_FUNCTION__.read_priv_table:
	.asciz	"_Bool read_priv_table(FILE *)"
	.size	.L__PRETTY_FUNCTION__.read_priv_table, 30

	.type	.L.str.104,@object              # @.str.104
.L.str.104:
	.asciz	"error: failed to create WIF\n"
	.size	.L.str.104, 29

	.type	.L.str.105,@object              # @.str.105
.L.str.105:
	.asciz	"carry == 0"
	.size	.L.str.105, 11

	.type	.L__PRETTY_FUNCTION__.base58_encode,@object # @__PRETTY_FUNCTION__.base58_encode
.L__PRETTY_FUNCTION__.base58_encode:
	.asciz	"_Bool base58_encode(char *, size_t, const uint8_t *, size_t)"
	.size	.L__PRETTY_FUNCTION__.base58_encode, 61

	.type	.L.str.106,@object              # @.str.106
.L.str.106:
	.asciz	"123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz"
	.size	.L.str.106, 59

	.type	.L.str.107,@object              # @.str.107
.L.str.107:
	.asciz	"error: failed to generate address\n"
	.size	.L.str.107, 35

	.type	.L.str.108,@object              # @.str.108
.L.str.108:
	.asciz	"Bitcoin Signed Message:\n"
	.size	.L.str.108, 25

	.type	.L.str.109,@object              # @.str.109
.L.str.109:
	.asciz	"error: failed to generate signature\n"
	.size	.L.str.109, 37

	.type	.L.str.110,@object              # @.str.110
.L.str.110:
	.asciz	"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/"
	.size	.L.str.110, 65

	.type	.L.str.111,@object              # @.str.111
.L.str.111:
	.asciz	"\033[1m"
	.size	.L.str.111, 5

	.type	.L.str.112,@object              # @.str.112
.L.str.112:
	.asciz	"\033[0m"
	.size	.L.str.112, 5

	.type	.Lstr.2,@object                 # @str.2
.Lstr.2:
	.asciz	"\n--RESULT-(SECRET)---------------------------------------------------------------\n"
	.size	.Lstr.2, 83

	.type	.Lstr.3,@object                 # @str.3
.Lstr.3:
	.asciz	"--RESULT-(SIGNATURES)-----------------------------------------------------------\n"
	.size	.Lstr.3, 82

	.type	.Lstr.4,@object                 # @str.4
.Lstr.4:
	.asciz	"--RESULT-(PUBLIC)---------------------------------------------------------------\n"
	.size	.Lstr.4, 82

	.type	.Lstr.5,@object                 # @str.5
.Lstr.5:
	.asciz	"warning: verify the keys/addresses before use!\n"
	.size	.Lstr.5, 48

	.type	.Lstr.6,@object                 # @str.6
.Lstr.6:
	.asciz	"|"
	.size	.Lstr.6, 2

	.ident	"Ubuntu clang version 14.0.0-1ubuntu1.1"
	.section	".note.GNU-stack","",@progbits
	.addrsig
	.addrsig_sym nonce_function_rfc6979
	.addrsig_sym init_worker
	.addrsig_sym worker
	.addrsig_sym main.long_options
	.addrsig_sym .L.str.21
	.addrsig_sym bases
	.addrsig_sym offsets
	.addrsig_sym priv_bases
	.addrsig_sym priv_offsets
	.addrsig_sym table_lock
	.addrsig_sym stop
	.addrsig_sym secp256k1_ge_const_g
	.addrsig_sym secp256k1_fe_inv_var.prime
	.addrsig_sym secp256k1_rfc6979_hmac_sha256_initialize.one
	.addrsig_sym secp256k1_sha256_finalize.pad
	.addrsig_sym secp256k1_rfc6979_hmac_sha256_generate.zero
